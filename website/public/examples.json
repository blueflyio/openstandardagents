[
  {
    "name": "code-review-workflow.yml",
    "path": "adk-integration/code-review-workflow.yml",
    "content": "# Example: ADK Code Review Workflow with OSSA Agents\n# This demonstrates how OSSA agents work with ADK orchestration patterns\n\napiVersion: ossa/v0.2.8\nkind: Workflow\nmetadata:\n  name: code-review-workflow\n  description: 'Multi-agent code review using ADK patterns'\n\nspec:\n  # ADK orchestration pattern\n  adk_pattern: sequential\n\n  # Agents involved in workflow\n  agents:\n    - name: code-analyzer\n      type: LlmAgent\n      ossa_type: worker\n      capabilities:\n        - code-analysis\n        - static-analysis\n      instruction: |\n        Analyze the provided code for:\n        - Code quality issues\n        - Potential bugs\n        - Performance concerns\n        - Security vulnerabilities\n        Store findings in {analysis_results}\n      output_key: analysis_results\n\n    - name: style-checker\n      type: LlmAgent\n      ossa_type: critic\n      capabilities:\n        - style-checking\n        - formatting\n      instruction: |\n        Check code style based on {analysis_results}:\n        - Naming conventions\n        - Code formatting\n        - Documentation completeness\n        Store style issues in {style_report}\n      output_key: style_report\n\n    - name: security-auditor\n      type: CustomAgent\n      custom_type: governor\n      ossa_type: governor\n      capabilities:\n        - security-scanning\n        - vulnerability-detection\n      policies:\n        - name: owasp-top-10\n          severity: high\n        - name: cwe-sans-25\n          severity: critical\n      instruction: |\n        Audit code for security issues using {analysis_results}\n        Apply OWASP and CWE policies\n        Store violations in {security_audit}\n      output_key: security_audit\n\n    - name: review-aggregator\n      type: WorkflowAgent\n      workflow_type: parallel\n      ossa_type: orchestrator\n      sub_agents:\n        - code-analyzer\n        - style-checker\n        - security-auditor\n      instruction: |\n        Aggregate all review results:\n        - {analysis_results}\n        - {style_report}  \n        - {security_audit}\n        Generate comprehensive review report\n      output_key: final_review\n\n  # Execution configuration\n  execution:\n    # Use ADK state management\n    state_management: adk_session\n\n    # Tool delegation\n    tool_delegation:\n      type: explicit\n      tools:\n        - name: git_operations\n          capabilities: [version-control]\n        - name: issue_tracker\n          capabilities: [issue-management]\n\n    # Conditional execution based on findings\n    conditions:\n      - agent: security-auditor\n        condition: \"session.state.analysis_results.severity > 'medium'\"\n      - agent: style-checker\n        condition: 'session.state.analysis_results.lines_of_code > 100'\n\n    # Loop configuration for iterative review\n    loop_options:\n      max_iterations: 3\n      break_condition: 'session.state.security_audit.violations.length == 0'\n\n  # ADK integration settings\n  adk_config:\n    # Model configuration\n    models:\n      default: gemini-2.0-flash\n      specialized:\n        security-auditor: gemini-2.0-pro\n\n    # Performance settings\n    performance:\n      parallel_execution: true\n      cache_session_state: true\n      state_persistence: redis\n\n    # Error handling\n    error_handling:\n      retry_count: 3\n      fallback_pattern: sequential\n# Example usage with ADK adapter:\n#\n# const adapter = new OSSAADKAdapter();\n# await adapter.loadAgent('.agents/code-analyzer');\n# await adapter.loadAgent('.agents/style-checker');\n# await adapter.loadAgent('.agents/security-auditor');\n# await adapter.loadAgent('.agents/review-aggregator');\n#\n# const result = await adapter.executeOrchestration(\n#   'sequential',\n#   ['code-analyzer', 'style-checker', 'security-auditor', 'review-aggregator'],\n#   { code: sourceCode },\n#   { conditions: [...], loop_options: {...} }\n# );\n",
    "category": "Integration Patterns"
  },
  {
    "name": "customer-support.yml",
    "path": "adk-integration/customer-support.yml",
    "content": "# Example: ADK Customer Support System with OSSA Agents\n# Demonstrates Coordinator and Dispatcher patterns\n\napiVersion: ossa/v0.2.8\nkind: Workflow\nmetadata:\n  name: customer-support-system\n  description: 'Multi-agent customer support with intelligent routing'\n\nspec:\n  # ADK Coordinator pattern for complex support tasks\n  adk_pattern: coordinator\n\n  agents:\n    # Coordinator agent\n    - name: support-coordinator\n      type: LlmAgent\n      ossa_type: orchestrator\n      model: gemini-2.0-pro\n      instruction: |\n        Analyze customer request and determine:\n        1. Request type (billing, technical, product, complaint)\n        2. Urgency level (low, medium, high, critical)\n        3. Required specialists\n        4. Delegation strategy\n\n        Route to appropriate agents based on analysis.\n        Store routing decision in {routing_plan}\n      output_key: routing_plan\n      capabilities:\n        - request-analysis\n        - intent-classification\n        - routing-decision\n\n    # Specialist agents\n    - name: billing-specialist\n      type: LlmAgent\n      ossa_type: worker\n      instruction: |\n        Handle billing-related inquiries:\n        - Payment issues\n        - Subscription changes\n        - Refund requests\n        - Invoice questions\n\n        Use {routing_plan} context\n        Store response in {billing_response}\n      output_key: billing_response\n      tools:\n        - database_query\n        - api_call\n      capabilities:\n        - billing-operations\n        - payment-processing\n\n    - name: technical-specialist\n      type: LlmAgent\n      ossa_type: worker\n      instruction: |\n        Resolve technical issues:\n        - Bug reports\n        - Feature questions\n        - Integration problems\n        - Performance issues\n\n        Reference {routing_plan} for context\n        Store solution in {technical_response}\n      output_key: technical_response\n      tools:\n        - api_call\n        - database_query\n      capabilities:\n        - technical-support\n        - troubleshooting\n\n    - name: product-specialist\n      type: LlmAgent\n      ossa_type: worker\n      instruction: |\n        Answer product questions:\n        - Feature explanations\n        - Best practices\n        - Use case guidance\n        - Product recommendations\n\n        Use {routing_plan} guidance\n        Store answer in {product_response}\n      output_key: product_response\n      capabilities:\n        - product-knowledge\n        - recommendation-engine\n\n    - name: escalation-handler\n      type: CustomAgent\n      custom_type: specialized\n      ossa_type: worker\n      instruction: |\n        Handle escalated issues:\n        - Complaints\n        - Complex problems\n        - VIP customers\n        - Legal matters\n\n        Review all previous responses\n        Store resolution in {escalation_response}\n      output_key: escalation_response\n      capabilities:\n        - escalation-management\n        - conflict-resolution\n\n    # Response aggregator\n    - name: response-synthesizer\n      type: WorkflowAgent\n      workflow_type: parallel\n      ossa_type: orchestrator\n      instruction: |\n        Synthesize all specialist responses:\n        - {billing_response}\n        - {technical_response}\n        - {product_response}\n        - {escalation_response}\n\n        Create unified customer response\n        Store in {final_response}\n      output_key: final_response\n      sub_agents:\n        - billing-specialist\n        - technical-specialist\n        - product-specialist\n        - escalation-handler\n\n  # Dispatcher configuration for routing\n  dispatcher_config:\n    routing_rules:\n      - pattern: 'billing|payment|invoice|refund'\n        agent: billing-specialist\n        priority: high\n\n      - pattern: 'bug|error|crash|slow|broken'\n        agent: technical-specialist\n        priority: critical\n\n      - pattern: 'how to|feature|can I|what is'\n        agent: product-specialist\n        priority: medium\n\n      - pattern: 'complaint|angry|frustrated|cancel'\n        agent: escalation-handler\n        priority: critical\n\n    fallback_agent: support-coordinator\n\n    # LLM-driven routing when rules don't match\n    llm_routing:\n      enabled: true\n      model: gemini-2.0-flash\n      instruction: |\n        Analyze request and select best agent:\n        - Consider request content\n        - Check customer history\n        - Evaluate urgency\n        - Match agent capabilities\n\n  # Coordination strategies\n  coordination:\n    strategy: adaptive # Changes based on request type\n\n    patterns:\n      simple_request:\n        pattern: dispatcher\n        condition: \"session.state.routing_plan.complexity === 'simple'\"\n\n      complex_request:\n        pattern: coordinator\n        condition: \"session.state.routing_plan.complexity === 'complex'\"\n\n      multi_issue:\n        pattern: parallel\n        condition: 'session.state.routing_plan.issue_count > 1'\n\n      escalated:\n        pattern: sequential\n        condition: 'session.state.routing_plan.requires_escalation === true'\n\n  # ADK communication patterns\n  communication:\n    # Shared session state\n    shared_state:\n      - customer_id\n      - request_history\n      - routing_plan\n      - urgency_level\n      - sentiment_score\n\n    # Agent handoff\n    handoff_config:\n      preserve_context: true\n      transfer_method: state_based # Via session.state\n      handoff_message: |\n        Transferring to {target_agent} because:\n        {handoff_reason}\n\n    # Callback mechanisms\n    callbacks:\n      on_routing_complete: log_routing_decision\n      on_response_ready: notify_customer\n      on_escalation: alert_supervisor\n\n  # Performance and optimization\n  adk_config:\n    performance:\n      # Agent pooling\n      agent_pool:\n        min_instances: 1\n        max_instances: 10\n        scale_based_on: request_volume\n\n      # Response caching\n      cache:\n        enabled: true\n        ttl: 3600\n        cache_key: 'request_hash'\n\n    # Quality assurance\n    quality:\n      # Response validation\n      validate_responses: true\n      min_confidence: 0.8\n\n      # Sentiment checking\n      check_sentiment: true\n      min_sentiment_score: 0.6\n\n    # Metrics and monitoring\n    metrics:\n      - response_time\n      - customer_satisfaction\n      - resolution_rate\n      - escalation_rate\n      - agent_utilization\n# Example usage with different patterns:\n#\n# 1. Simple request (Dispatcher):\n# const result = await adapter.executeOrchestration(\n#   'dispatcher',\n#   ['billing-specialist', 'technical-specialist', 'product-specialist'],\n#   { request: \"How do I update my payment method?\" },\n#   { router: customRouter }\n# );\n#\n# 2. Complex request (Coordinator):\n# const result = await adapter.executeOrchestration(\n#   'coordinator',\n#   ['support-coordinator', 'billing-specialist', 'technical-specialist', 'response-synthesizer'],\n#   { request: \"I have billing issues and the app keeps crashing\" }\n# );\n#\n# 3. Escalated issue (Sequential with conditions):\n# const result = await adapter.executeOrchestration(\n#   'sequential',\n#   ['support-coordinator', 'technical-specialist', 'escalation-handler', 'response-synthesizer'],\n#   { request: \"This is the 5th time I'm reporting this critical bug!\" }\n# );\n",
    "category": "Integration Patterns"
  },
  {
    "name": "data-pipeline.yml",
    "path": "adk-integration/data-pipeline.yml",
    "content": "# Example: ADK Data Processing Pipeline with OSSA Agents\n# Demonstrates LoopAgent and ConditionalAgent patterns\n\napiVersion: ossa/v0.2.8\nkind: Workflow\nmetadata:\n  name: data-processing-pipeline\n  description: 'Iterative data processing with conditional branches'\n\nspec:\n  # ADK Loop pattern for batch processing\n  adk_pattern: loop\n\n  agents:\n    - name: data-ingester\n      type: LlmAgent\n      ossa_type: worker\n      capabilities:\n        - data-ingestion\n        - format-conversion\n      instruction: |\n        Ingest data batch from source\n        Convert to standard format\n        Store in {raw_data}\n        Set {has_more_data} flag\n      output_key: raw_data\n      tools:\n        - api_call\n        - file_operation\n\n    - name: data-validator\n      type: LlmAgent\n      ossa_type: critic\n      capabilities:\n        - validation\n        - schema-checking\n      instruction: |\n        Validate {raw_data} against schema\n        Check data quality metrics\n        Store validation results in {validation_status}\n        Set {is_valid} flag\n      output_key: validation_status\n\n    - name: data-transformer\n      type: WorkflowAgent\n      workflow_type: conditional\n      ossa_type: orchestrator\n      instruction: |\n        Transform data based on {validation_status}:\n        - If valid: apply standard transformations\n        - If invalid: apply error corrections\n        Store in {transformed_data}\n      output_key: transformed_data\n      sub_agents:\n        - name: standard-transformer\n          condition: 'session.state.is_valid === true'\n        - name: error-corrector\n          condition: 'session.state.is_valid === false'\n\n    - name: data-enricher\n      type: CustomAgent\n      custom_type: specialized\n      ossa_type: worker\n      capabilities:\n        - data-enrichment\n        - ml-inference\n      instruction: |\n        Enrich {transformed_data} with:\n        - External API data\n        - ML model predictions\n        - Calculated metrics\n        Store in {enriched_data}\n      output_key: enriched_data\n\n    - name: data-loader\n      type: LlmAgent\n      ossa_type: worker\n      capabilities:\n        - database-operations\n        - data-persistence\n      instruction: |\n        Load {enriched_data} to target system\n        Update {batch_counter}\n        Log operation in {load_status}\n      output_key: load_status\n      tools:\n        - database_query\n\n  # Loop configuration\n  execution:\n    loop_config:\n      # Continue while more data exists\n      condition: 'session.state.has_more_data === true'\n      max_iterations: 100\n      batch_size: 1000\n\n      # Break conditions\n      break_conditions:\n        - 'session.state.error_count > 5'\n        - 'session.state.batch_counter >= session.state.total_batches'\n\n    # Conditional branches\n    conditional_config:\n      data-validator:\n        skip_if: 'session.state.skip_validation === true'\n\n      data-enricher:\n        execute_if: 'session.state.validation_status.quality_score > 0.8'\n\n      error-corrector:\n        execute_if: |\n          session.state.validation_status.errors.length > 0 &&\n          session.state.validation_status.errors.some(e => e.severity === 'critical')\n\n    # Parallel processing for sub-workflows\n    parallel_config:\n      enabled: true\n      max_workers: 5\n      distribute_by: 'session.state.raw_data.partition_key'\n\n  # ADK state management\n  adk_config:\n    state:\n      # Persistent state across iterations\n      persistent_keys:\n        - batch_counter\n        - total_processed\n        - error_count\n        - processing_metrics\n\n      # Temporary state (cleared each iteration)\n      temp_keys:\n        - raw_data\n        - validation_status\n        - transformed_data\n        - enriched_data\n\n    # Performance optimization\n    optimization:\n      cache_transformations: true\n      reuse_connections: true\n      batch_database_writes: true\n\n    # Error recovery\n    error_recovery:\n      checkpoint_frequency: 10 # Save state every 10 iterations\n      resume_on_failure: true\n      dead_letter_queue: true\n\n    # Monitoring\n    monitoring:\n      metrics:\n        - batches_processed\n        - records_transformed\n        - validation_failures\n        - enrichment_success_rate\n      alerts:\n        - condition: 'metrics.validation_failures > 10'\n          severity: warning\n        - condition: 'metrics.enrichment_success_rate < 0.95'\n          severity: critical\n# Example usage:\n#\n# const result = await adapter.executeOrchestration(\n#   'loop',\n#   ['data-ingester', 'data-validator', 'data-transformer', 'data-enricher', 'data-loader'],\n#   { source: 's3://data-bucket/input/' },\n#   {\n#     maxIterations: 100,\n#     condition: (state) => state.has_more_data,\n#     breakCondition: (result, state) => state.error_count > 5\n#   }\n# );\n",
    "category": "Integration Patterns"
  },
  {
    "name": "compliance-context-production.json",
    "path": "advanced/patterns/compliance-context-production.json",
    "content": "{\n  \"environment\": \"production\",\n  \"classification\": \"confidential\",\n  \"region\": \"us-east-1\",\n  \"industry\": \"financial-services\",\n  \"dataTypes\": [\n    \"financial-records\",\n    \"customer-data\",\n    \"transaction-logs\",\n    \"audit-trails\"\n  ],\n  \"regulatoryRequirements\": [\n    \"iso-42001\",\n    \"nist-ai-rmf\",\n    \"eu-ai-act\"\n  ],\n  \"complianceMetadata\": {\n    \"organization\": \"Enterprise Financial Corp\",\n    \"complianceOfficer\": \"compliance@enterprise.com\",\n    \"lastReview\": \"2024-01-15\",\n    \"nextReview\": \"2024-04-15\",\n    \"certifications\": [\n      \"ISO 27001:2022\",\n      \"SOC 2 Type II\",\n      \"PCI DSS Level 1\"\n    ]\n  },\n  \"enforcementPolicies\": {\n    \"productionDeployment\": {\n      \"minimumConformance\": \"gold\",\n      \"auditLogging\": \"mandatory\",\n      \"tlsRequired\": true,\n      \"humanOversight\": \"required\",\n      \"budgetLimits\": {\n        \"daily\": 100000,\n        \"monthly\": 2000000,\n        \"emergency\": 500000\n      }\n    },\n    \"stagingDeployment\": {\n      \"minimumConformance\": \"silver\",\n      \"auditLogging\": \"recommended\",\n      \"tlsRequired\": true,\n      \"humanOversight\": \"optional\"\n    },\n    \"developmentDeployment\": {\n      \"minimumConformance\": \"bronze\",\n      \"auditLogging\": \"optional\",\n      \"tlsRequired\": false,\n      \"humanOversight\": \"optional\"\n    }\n  }\n}",
    "category": "Production"
  },
  {
    "name": "model-router.ts",
    "path": "advanced/patterns/model-router.ts",
    "content": "/**\n * OSSA Model Router Pattern\n * =========================\n *\n * This example demonstrates the Model Router pattern for OSSA agents, which allows\n * dynamic selection of language models based on task requirements, cost constraints,\n * and performance needs.\n *\n * Key Features:\n * - Dynamic model selection based on task requirements\n * - Cost-aware routing\n * - Performance optimization\n * - Fallback strategies\n *\n * Directory Structure:\n * examples/advanced/patterns/\n *   ├── model-router.ts      # This file\n *   └── README.md            # Documentation\n *\n * Prerequisites:\n * - Node.js 18+\n * - TypeScript 5.0+\n * - Ollama running locally (for local models)\n * - API keys for cloud providers (if used)\n *\n * Usage:\n * 1. Install dependencies: `npm install @ossa/core dotenv`\n * 2. Start Ollama: `ollama serve`\n * 3. Pull models: `ollama pull llama3`\n * 4. Run: `npx ts-node model-router.ts`\n */\n\nimport { Agent, AgentConfig, AgentContext, AgentResponse } from '@ossa/core';\nimport dotenv from 'dotenv';\n\ndotenv.config();\n\n// Define model configurations\nconst MODEL_CONFIGS = {\n  llama3: {\n    provider: 'ollama',\n    model: 'llama3',\n    costPerToken: 0.000002,\n    avgLatencyMs: 1200,\n    maxTokens: 8192,\n    capabilities: ['text-generation', 'summarization', 'qna'],\n  },\n  mixtral: {\n    provider: 'ollama',\n    model: 'mixtral',\n    costPerToken: 0.000003,\n    avgLatencyMs: 1800,\n    maxTokens: 32000,\n    capabilities: ['text-generation', 'code', 'reasoning'],\n  },\n  'gpt-4': {\n    provider: 'openai',\n    model: 'gpt-4-turbo',\n    costPerToken: 0.00001,\n    avgLatencyMs: 2500,\n    maxTokens: 128000,\n    capabilities: ['text-generation', 'code', 'reasoning', 'vision'],\n  },\n  'claude-3-opus': {\n    provider: 'anthropic',\n    model: 'claude-3-opus-20240229',\n    costPerToken: 0.000015,\n    avgLatencyMs: 3000,\n    maxTokens: 200000,\n    capabilities: ['text-generation', 'analysis', 'summarization'],\n  },\n};\n\ntype ModelKey = keyof typeof MODEL_CONFIGS;\n\ninterface ModelRequest {\n  prompt: string;\n  context?: Record<string, any>;\n  requirements?: {\n    maxCostPerToken?: number;\n    maxLatencyMs?: number;\n    minCapabilities?: string[];\n    minContextLength?: number;\n  };\n}\n\nclass ModelRouterAgent extends Agent {\n  private models: Record<string, any> = MODEL_CONFIGS;\n\n  constructor() {\n    super({\n      name: 'model-router',\n      version: '1.0.0',\n      description: 'Intelligent model routing for OSSA agents',\n      capabilities: [\n        'model-routing',\n        'cost-optimization',\n        'performance-monitoring',\n      ],\n    });\n  }\n\n  /**\n   * Select the best model based on requirements\n   */\n  private selectModel(\n    requirements: ModelRequest['requirements'] = {}\n  ): ModelKey {\n    const {\n      maxCostPerToken = Infinity,\n      maxLatencyMs = 5000,\n      minCapabilities = [],\n      minContextLength = 0,\n    } = requirements;\n\n    // Filter models by requirements\n    const suitableModels = Object.entries(this.models)\n      .filter(([_, config]) => {\n        // Check cost constraints\n        if (config.costPerToken > maxCostPerToken) return false;\n\n        // Check latency constraints\n        if (config.avgLatencyMs > maxLatencyMs) return false;\n\n        // Check context length\n        if (config.maxTokens < minContextLength) return false;\n\n        // Check required capabilities\n        return minCapabilities.every((cap) =>\n          config.capabilities.includes(cap)\n        );\n      })\n      .sort((a, b) => {\n        // Prioritize lower cost, then lower latency\n        const costDiff = a[1].costPerToken - b[1].costPerToken;\n        if (costDiff !== 0) return costDiff;\n        return a[1].avgLatencyMs - b[1].avgLatencyMs;\n      });\n\n    if (suitableModels.length === 0) {\n      throw new Error('No suitable models found for the given requirements');\n    }\n\n    return suitableModels[0][0] as ModelKey;\n  }\n\n  /**\n   * Process a request using the best available model\n   */\n  async process(\n    request: ModelRequest,\n    context: AgentContext\n  ): Promise<AgentResponse> {\n    try {\n      // Select the best model\n      const modelKey = this.selectModel(request.requirements);\n      const modelConfig = this.models[modelKey];\n\n      this.logger.info(`Selected model: ${modelKey}`, { model: modelKey });\n\n      // Process the request with the selected model\n      const result = await this.callModel(modelKey, request.prompt, context);\n\n      return {\n        success: true,\n        data: {\n          response: result,\n          model: modelKey,\n          metadata: {\n            cost: request.prompt.length * modelConfig.costPerToken,\n            latency: modelConfig.avgLatencyMs,\n            provider: modelConfig.provider,\n          },\n        },\n      };\n    } catch (error) {\n      this.logger.error('Model routing failed', { error });\n      return {\n        success: false,\n        error: {\n          code: 'MODEL_ROUTING_ERROR',\n          message: error.message,\n          details: error.stack,\n        },\n      };\n    }\n  }\n\n  /**\n   * Call the actual model (implementation would vary by provider)\n   */\n  private async callModel(\n    modelKey: string,\n    prompt: string,\n    context: AgentContext\n  ): Promise<string> {\n    const model = this.models[modelKey];\n\n    // In a real implementation, this would call the actual model APIs\n    // This is a simplified example\n    switch (model.provider) {\n      case 'ollama':\n        return this.callOllama(model.model, prompt);\n      case 'openai':\n        return this.callOpenAI(model.model, prompt);\n      case 'anthropic':\n        return this.callAnthropic(model.model, prompt);\n      default:\n        throw new Error(`Unsupported provider: ${model.provider}`);\n    }\n  }\n\n  // Stub implementations for model providers\n  private async callOllama(model: string, prompt: string): Promise<string> {\n    // Implementation would call Ollama's API\n    return `Response from Ollama (${model}) for prompt: ${prompt.substring(0, 50)}...`;\n  }\n\n  private async callOpenAI(model: string, prompt: string): Promise<string> {\n    // Implementation would call OpenAI's API\n    return `Response from OpenAI (${model}) for prompt: ${prompt.substring(0, 50)}...`;\n  }\n\n  private async callAnthropic(model: string, prompt: string): Promise<string> {\n    // Implementation would call Anthropic's API\n    return `Response from Anthropic (${model}) for prompt: ${prompt.substring(0, 50)}...`;\n  }\n}\n\n// Example usage\nasync function main() {\n  const router = new ModelRouterAgent();\n\n  // Example 1: Fast, low-cost response\n  const response1 = await router.process(\n    {\n      prompt: 'Explain quantum computing in simple terms',\n      requirements: {\n        maxCostPerToken: 0.000005,\n        maxLatencyMs: 2000,\n        minCapabilities: ['text-generation'],\n      },\n    },\n    {}\n  );\n\n  console.log('Example 1 - Fast, low-cost response:');\n  console.log(response1);\n\n  // Example 2: Complex reasoning with higher budget\n  const response2 = await router.process(\n    {\n      prompt:\n        'Write a detailed analysis of the latest AI safety research papers',\n      requirements: {\n        maxCostPerToken: 0.00002,\n        maxLatencyMs: 10000,\n        minCapabilities: ['analysis', 'reasoning'],\n        minContextLength: 16000,\n      },\n    },\n    {}\n  );\n\n  console.log('\\nExample 2 - Complex analysis:');\n  console.log(response2);\n}\n\n// Run the example if this file is executed directly\nif (require.main === module) {\n  main().catch(console.error);\n}\n\nexport { ModelRouterAgent };\n",
    "category": "Advanced Patterns"
  },
  {
    "name": "smart-model-routing.ts",
    "path": "advanced/patterns/smart-model-routing.ts",
    "content": "/**\n * OSSA Smart Model Router\n * ========================\n *\n * This example demonstrates an advanced model routing pattern that dynamically selects\n * the most appropriate language model based on task requirements, cost constraints,\n * and performance needs.\n */\n\nimport { Agent, AgentContext, AgentResponse } from '@ossa/core';\nimport axios from 'axios';\nimport dotenv from 'dotenv';\n\ndotenv.config();\n\n// Types and Interfaces\ninterface ModelConfig {\n  provider: 'ollama' | 'openai' | 'anthropic';\n  model: string;\n  costPerToken: number;\n  avgLatencyMs: number;\n  maxTokens: number;\n  capabilities: string[];\n  baseUrl?: string;\n  apiKeyEnv?: string;\n}\n\ninterface ModelRequest {\n  prompt: string;\n  context?: Record<string, any>;\n  requirements?: {\n    maxCostPerToken?: number;\n    maxLatencyMs?: number;\n    minCapabilities?: string[];\n    minContextLength?: number;\n  };\n}\n\ntype ModelKey = keyof typeof MODEL_CONFIGS;\n\n// Configuration\nconst MODEL_CONFIGS: Record<string, ModelConfig> = {\n  llama3: {\n    provider: 'ollama',\n    model: 'llama3',\n    costPerToken: 0.000002,\n    avgLatencyMs: 1200,\n    maxTokens: 8192,\n    capabilities: ['text-generation', 'summarization', 'qna'],\n    baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',\n  },\n  mixtral: {\n    provider: 'ollama',\n    model: 'mixtral',\n    costPerToken: 0.000003,\n    avgLatencyMs: 1800,\n    maxTokens: 32000,\n    capabilities: ['text-generation', 'code', 'reasoning'],\n    baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',\n  },\n  'gpt-4': {\n    provider: 'openai',\n    model: 'gpt-4-turbo',\n    costPerToken: 0.00001,\n    avgLatencyMs: 2500,\n    maxTokens: 128000,\n    capabilities: ['text-generation', 'code', 'reasoning', 'vision'],\n    apiKeyEnv: 'OPENAI_API_KEY',\n  },\n  'claude-3-opus': {\n    provider: 'anthropic',\n    model: 'claude-3-opus-20240229',\n    costPerToken: 0.000015,\n    avgLatencyMs: 3000,\n    maxTokens: 200000,\n    capabilities: ['text-generation', 'analysis', 'summarization'],\n    apiKeyEnv: 'ANTHROPIC_API_KEY',\n  },\n};\n\nclass SmartModelRouter extends Agent {\n  private models: Record<string, ModelConfig> = MODEL_CONFIGS;\n  private http = axios.create();\n\n  constructor() {\n    super({\n      name: 'smart-model-router',\n      version: '1.0.0',\n      description: 'Intelligent model routing for OSSA agents',\n      capabilities: [\n        'model-routing',\n        'cost-optimization',\n        'performance-monitoring',\n        'fallback-handling',\n      ],\n    });\n  }\n\n  /**\n   * Process a request using the best available model\n   */\n  async process(\n    request: ModelRequest,\n    context: AgentContext = {}\n  ): Promise<AgentResponse> {\n    try {\n      // Select the best model based on requirements\n      const modelKey = this.selectModel(request.requirements);\n      const modelConfig = this.models[modelKey];\n\n      this.logger.info(`Routing to ${modelKey} (${modelConfig.provider})`);\n\n      // Process the request using the selected model\n      const startTime = Date.now();\n      const response = await this.routeToModel(modelKey, request, context);\n      const latency = Date.now() - startTime;\n\n      return {\n        success: true,\n        data: {\n          model: modelKey,\n          response,\n          metadata: {\n            provider: modelConfig.provider,\n            latency,\n            cost: this.calculateCost(\n              response.usage?.total_tokens || 0,\n              modelConfig\n            ),\n            tokens: response.usage?.total_tokens || 0,\n          },\n        },\n      };\n    } catch (error) {\n      this.logger.error('Error processing request:', error);\n      return {\n        success: false,\n        error: error instanceof Error ? error.message : 'Unknown error',\n      };\n    }\n  }\n\n  /**\n   * Select the best model based on requirements\n   */\n  private selectModel(\n    requirements: ModelRequest['requirements'] = {}\n  ): ModelKey {\n    const {\n      maxCostPerToken = Infinity,\n      maxLatencyMs = 5000,\n      minCapabilities = [],\n      minContextLength = 0,\n    } = requirements;\n\n    // Filter models by requirements\n    const suitableModels = Object.entries(this.models)\n      .filter(([_, config]) => {\n        // Check cost constraints\n        if (config.costPerToken > maxCostPerToken) return false;\n\n        // Check latency constraints\n        if (config.avgLatencyMs > maxLatencyMs) return false;\n\n        // Check context length\n        if (config.maxTokens < minContextLength) return false;\n\n        // Check API key availability for cloud providers\n        if (config.apiKeyEnv && !process.env[config.apiKeyEnv]) {\n          this.logger.warn(`Skipping ${config.model} - missing API key`);\n          return false;\n        }\n\n        // Check required capabilities\n        return minCapabilities.every((cap) =>\n          config.capabilities.includes(cap)\n        );\n      })\n      .sort((a, b) => {\n        // Prioritize lower cost, then lower latency\n        const costDiff = a[1].costPerToken - b[1].costPerToken;\n        if (costDiff !== 0) return costDiff;\n        return a[1].avgLatencyMs - b[1].avgLatencyMs;\n      });\n\n    if (suitableModels.length === 0) {\n      throw new Error('No suitable models found for the given requirements');\n    }\n\n    return suitableModels[0][0] as ModelKey;\n  }\n\n  /**\n   * Route the request to the appropriate model provider\n   */\n  private async routeToModel(\n    modelKey: string,\n    request: ModelRequest,\n    context: AgentContext = {}\n  ): Promise<any> {\n    const model = this.models[modelKey];\n\n    try {\n      switch (model.provider) {\n        case 'ollama':\n          return await this.callOllama(model, request);\n        case 'openai':\n          return await this.callOpenAI(model, request);\n        case 'anthropic':\n          return await this.callAnthropic(model, request);\n        default:\n          throw new Error(`Unsupported provider: ${model.provider}`);\n      }\n    } catch (error) {\n      this.logger.error(`Error calling ${model.provider}:`, error);\n      throw error;\n    }\n  }\n\n  /**\n   * Calculate cost based on token usage\n   */\n  private calculateCost(tokenCount: number, modelConfig: ModelConfig): number {\n    return (tokenCount * modelConfig.costPerToken) / 1000; // Convert to cost per 1k tokens\n  }\n\n  // Provider-specific implementations will be added in the next step\n  private async callOllama(\n    model: ModelConfig,\n    request: ModelRequest\n  ): Promise<any> {\n    throw new Error('Not implemented');\n  }\n\n  private async callOpenAI(\n    model: ModelConfig,\n    request: ModelRequest\n  ): Promise<any> {\n    throw new Error('Not implemented');\n  }\n\n  private async callAnthropic(\n    model: ModelConfig,\n    request: ModelRequest\n  ): Promise<any> {\n    throw new Error('Not implemented');\n  }\n}\n",
    "category": "Advanced Patterns"
  },
  {
    "name": "hybrid-model-strategy.yaml",
    "path": "advanced/workflows/hybrid-model-strategy.yaml",
    "content": "# Hybrid Model Strategy: Fast Local Planning + Premium Development\n# Demonstrates using fast Ollama models for planning agents and Claude for development\n\napiVersion: ossa/v0.2.8\nkind: Workflow\nmetadata:\n  name: hybrid-development-workflow\n  description: \"Cost-optimized workflow using fast local models for planning and premium models for development\"\n\nspec:\n  # Planning Phase - Use Fast Local Models\n  agents:\n    # Fast planning agent using local Ollama\n    - name: project-planner\n      type: orchestrator\n      ossa_type: orchestrator\n      modelConfig:\n        provider: \"ollama\"\n        model: \"mistral:7b\"  # Fast 1-2 second responses\n        parameters:\n          temperature: 0.3\n          max_tokens: 2000\n        ollamaConfig:\n          baseUrl: \"http://localhost:11434\"\n          keepAlive: \"5m\"\n      capabilities:\n        - project-planning\n        - task-breakdown\n        - resource-estimation\n      instruction: |\n        You are a fast project planning agent. Quickly break down development tasks:\n        - Analyze requirements\n        - Create task breakdown structure\n        - Estimate effort and dependencies\n        - Identify risks and blockers\n        Store plan in {project_plan}\n      output_key: project_plan\n\n    # Fast requirement analyzer using local model\n    - name: requirements-analyzer\n      type: critic\n      ossa_type: critic\n      modelConfig:\n        provider: \"ollama\"\n        model: \"qwen2.5:7b\"  # Good for analysis, fast\n        parameters:\n          temperature: 0.2\n          max_tokens: 1500\n        ollamaConfig:\n          baseUrl: \"http://localhost:11434\"\n      capabilities:\n        - requirements-analysis\n        - user-story-validation\n        - acceptance-criteria\n      instruction: |\n        Quickly analyze requirements for clarity and completeness:\n        - Validate user stories\n        - Identify missing requirements\n        - Check for conflicts or ambiguities\n        Store analysis in {requirements_analysis}\n      output_key: requirements_analysis\n\n  # Development Phase - Use Premium Models\n    # Premium development agent using Claude Code\n    - name: senior-developer\n      type: worker\n      ossa_type: worker\n      modelConfig:\n        provider: \"anthropic\"\n        model: \"claude-3-5-sonnet-20241022\"\n        parameters:\n          temperature: 0.1  # Low temperature for precise code\n          max_tokens: 8000\n        anthropicConfig:\n          apiKey: \"${ANTHROPIC_API_KEY}\"\n          defaultHeaders:\n            \"anthropic-beta\": \"computer-use-2024-10-22\"\n      capabilities:\n        - code-generation\n        - architecture-design\n        - best-practices\n        - testing\n        - documentation\n      instruction: |\n        You are an expert senior developer using best practices:\n        - Write clean, maintainable code\n        - Follow SOLID principles\n        - Include comprehensive tests\n        - Add proper documentation\n        - Consider security and performance\n        Use {project_plan} and {requirements_analysis} as context\n      tools:\n        - file-operations\n        - git-operations\n        - test-runner\n        - linter\n      output_key: development_result\n\n    # Code review agent using Claude\n    - name: code-reviewer\n      type: critic\n      ossa_type: critic\n      modelConfig:\n        provider: \"anthropic\"\n        model: \"claude-3-5-sonnet-20241022\"\n        parameters:\n          temperature: 0.05  # Very low for consistent reviews\n          max_tokens: 6000\n      capabilities:\n        - code-review\n        - security-analysis\n        - performance-review\n        - maintainability-check\n      instruction: |\n        Perform thorough code review focusing on:\n        - Code quality and best practices\n        - Security vulnerabilities\n        - Performance issues\n        - Maintainability concerns\n        - Test coverage and quality\n        Review {development_result} and provide detailed feedback\n      output_key: code_review\n\n  # Optimization Phase - Use Local Models for Simple Tasks\n    # Documentation generator using local model\n    - name: doc-generator\n      type: worker\n      ossa_type: worker\n      modelConfig:\n        provider: \"ollama\"\n        model: \"codellama:7b\"  # Good for documentation\n        parameters:\n          temperature: 0.4\n          max_tokens: 3000\n      capabilities:\n        - documentation-generation\n        - api-docs\n        - readme-creation\n      instruction: |\n        Generate comprehensive documentation based on {development_result}:\n        - API documentation\n        - Usage examples\n        - Installation instructions\n        - Architecture overview\n      output_key: documentation\n\n  # Cost Optimization Configuration\n  costOptimization:\n    strategy: \"hybrid\"\n    budgetLimits:\n      planning_phase: 0.01    # $0.01 - use free local models\n      development_phase: 1.00  # $1.00 - premium models for quality\n      optimization_phase: 0.05 # $0.05 - local models for docs\n\n    fallbackStrategy:\n      # If premium models unavailable, use these alternatives\n      fallbacks:\n        \"claude-3-5-sonnet\": \"gpt-4o\"\n        \"gpt-4o\": \"gemini-2.0-flash\"\n        \"gemini-2.0-flash\": \"ollama:qwen2.5:7b\"\n\n  # Performance Targets\n  performance:\n    planning_phase:\n      target_latency: \"5s\"      # Fast local responses\n      cost_per_task: \"$0.00\"    # Free local inference\n    development_phase:\n      target_latency: \"30s\"     # Quality over speed\n      cost_per_task: \"$0.50\"    # Premium model investment\n    optimization_phase:\n      target_latency: \"10s\"     # Fast local processing\n      cost_per_task: \"$0.02\"    # Minimal cost\n\n  # Execution Flow\n  execution:\n    phases:\n      - name: planning\n        agents: [project-planner, requirements-analyzer]\n        execution_mode: parallel\n        budget_limit: 0.01\n\n      - name: development\n        agents: [senior-developer]\n        execution_mode: sequential\n        dependencies: [planning]\n        budget_limit: 1.00\n\n      - name: review\n        agents: [code-reviewer]\n        execution_mode: sequential\n        dependencies: [development]\n        budget_limit: 0.30\n\n      - name: documentation\n        agents: [doc-generator]\n        execution_mode: sequential\n        dependencies: [development]\n        budget_limit: 0.05\n\n    # Quality gates\n    quality_gates:\n      - phase: development\n        condition: \"code_review.score > 8.0\"\n        action: \"proceed\"\n      - phase: development\n        condition: \"code_review.security_issues > 0\"\n        action: \"retry_with_feedback\"\n\n---\n# Example Environment Configuration\n# .env file should contain:\n\n# Fast local models for planning\nOLLAMA_BASE_URL=http://localhost:11434\nPLANNING_MODEL=mistral:7b\nANALYSIS_MODEL=qwen2.5:7b\nDOC_MODEL=codellama:7b\n\n# Premium models for development\nANTHROPIC_API_KEY=sk-ant-...\nDEVELOPMENT_MODEL=claude-3-5-sonnet-20241022\nREVIEW_MODEL=claude-3-5-sonnet-20241022\n\n# Cost controls\nMAX_PLANNING_COST=0.01\nMAX_DEVELOPMENT_COST=1.00\nMAX_TOTAL_COST=2.00\n\n# Performance settings\nPLANNING_TIMEOUT=10s\nDEVELOPMENT_TIMEOUT=300s\nREVIEW_TIMEOUT=120s",
    "category": "Advanced Patterns"
  },
  {
    "name": "critic-agent.yaml",
    "path": "agent-manifests/critics/critic-agent.yaml",
    "content": "apiVersion: ossa/v0.2.8\nkind: Agent\nmetadata:\n  name: code-quality-critic\n  version: v1.0.0\n  description: 'Quality assurance agent for code review, testing, and continuous improvement'\n  author: 'quality-engineering-team'\n  labels:\n    environment: production\n    classification: internal\n    role: critic\n    complexity: standard\nspec:\n  type: critic\n  subtype: code-reviewer\n  capabilities:\n    domains:\n      - code-review\n      - quality-assessment\n      - testing-validation\n      - security-analysis\n      - performance-analysis\n      - documentation-review\n    operations:\n      - analyze-code\n      - review-changes\n      - assess-quality\n      - detect-issues\n      - suggest-improvements\n      - validate-tests\n    patterns:\n      - static-analysis\n      - dynamic-analysis\n      - peer-review\n      - automated-testing\n      - quality-gates\n      - continuous-feedback\n  protocols:\n    supported:\n      - name: rest\n        version: '1.1'\n        endpoint: 'https://critic.platform.com/api/v1'\n        authentication:\n          type: bearer-token\n          scopes: ['critic.review', 'code.analyze', 'quality.assess']\n        tls: true\n      - name: webhook\n        version: '1.0'\n        endpoint: 'https://critic.platform.com/webhooks/review'\n        authentication:\n          type: hmac-sha256\n      - name: grpc\n        version: '1.0'\n        endpoint: 'grpc://critic.platform.com:9443/CodeCritic'\n        authentication:\n          type: mutual-tls\n  analysis:\n    codeQuality:\n      complexity: true\n      maintainability: true\n      reliability: true\n      security: true\n      performance: true\n      testability: true\n    staticAnalysis:\n      syntaxChecking: true\n      typeChecking: true\n      styleChecking: true\n      securityScanning: true\n      dependencyAnalysis: true\n    supportedLanguages:\n      - javascript\n      - typescript\n      - python\n      - java\n      - go\n      - rust\n      - csharp\n    qualityMetrics:\n      cyclomaticComplexity: true\n      codeChurn: true\n      testCoverage: true\n      duplication: true\n      maintainabilityIndex: true\n  review:\n    automatedReview: true\n    humanReview: true\n    reviewCriteria:\n      functionality: true\n      readability: true\n      maintainability: true\n      performance: true\n      security: true\n      testCoverage: true\n    scoring:\n      scale: '0-100'\n      passThreshold: 70\n      categories:\n        - functionality\n        - quality\n        - security\n        - performance\n        - maintainability\n  feedback:\n    suggestions: true\n    improvements: true\n    bestPractices: true\n    patterns: true\n    antiPatterns: true\n    learningResources: true\n    priority:\n      - critical\n      - high\n      - medium\n      - low\n      - info\n  integration:\n    versionControl:\n      - git\n      - mercurial\n      - subversion\n    cicdPlatforms:\n      - gitlab\n      - github\n      - jenkins\n      - azure-devops\n    issueTracking:\n      - jira\n      - github-issues\n      - gitlab-issues\n  performance:\n    throughput:\n      reviewsPerHour: 50\n      linesOfCodePerMinute: 1000\n    latency:\n      analysisTime: 30000\n      reviewTime: 120000\n      feedbackTime: 5000\n      p95: 180000\n      p99: 300000\n  monitoring:\n    reviewMetrics: true\n    qualityTrends: true\n    performanceMetrics: true\n    errorTracking: true\n    alerting:\n      reviewFailureThreshold: 5\n      qualityDegradationThreshold: 10\n      performanceThreshold: 300000\n",
    "category": "Agent Types"
  },
  {
    "name": "governor-agent.yaml",
    "path": "agent-manifests/governors/governor-agent.yaml",
    "content": "apiVersion: ossa/v0.2.8\nkind: Agent\nmetadata:\n  name: policy-compliance-governor\n  version: v1.1.0\n  description: 'Governance agent enforcing policies, compliance, and security standards across systems'\n  author: 'governance-team'\n  labels:\n    environment: production\n    classification: internal\n    role: governor\n    complexity: enterprise\nspec:\n  type: governor\n  subtype: policy-enforcer\n  capabilities:\n    domains:\n      - policy-enforcement\n      - compliance-checking\n      - security-governance\n      - resource-management\n      - audit-tracking\n      - risk-assessment\n    operations:\n      - enforce-policies\n      - validate-compliance\n      - assess-risks\n      - audit-activities\n      - manage-permissions\n      - generate-reports\n    patterns:\n      - policy-as-code\n      - audit-trail\n      - role-based-access\n      - compliance-framework\n      - risk-matrix\n      - governance-workflow\n  protocols:\n    supported:\n      - name: rest\n        version: '1.1'\n        endpoint: 'https://governor.platform.com/api/v1'\n        authentication:\n          type: oauth2\n          scopes: ['governor.enforce', 'policies.validate', 'compliance.audit']\n        tls: true\n      - name: grpc\n        version: '1.0'\n        endpoint: 'grpc://governor.platform.com:9443/PolicyGovernor'\n        authentication:\n          type: mutual-tls\n      - name: webhook\n        version: '1.0'\n        endpoint: 'https://governor.platform.com/webhooks'\n        authentication:\n          type: hmac-sha256\n  governance:\n    policyEngine: 'open-policy-agent'\n    complianceFrameworks:\n      - soc2\n      - pci-dss\n      - gdpr\n      - hipaa\n      - iso27001\n    enforcementModes:\n      - advisory\n      - blocking\n      - logging\n    auditRetention: '7y'\n    riskAssessment:\n      frequency: 'weekly'\n      severity: ['low', 'medium', 'high', 'critical']\n      impact: ['minimal', 'moderate', 'significant', 'severe']\n  policies:\n    categories:\n      - security\n      - privacy\n      - operational\n      - financial\n      - regulatory\n    evaluation:\n      realtime: true\n      batch: true\n      scheduled: true\n    versioning:\n      enabled: true\n      approvalRequired: true\n      rollbackCapable: true\n  compliance:\n    continuousMonitoring: true\n    reportingFrequency: 'monthly'\n    evidenceCollection: true\n    controlMapping: true\n    attestation:\n      required: true\n      frequency: 'quarterly'\n      approvers: ['compliance-officer', 'security-lead']\n  security:\n    accessControl:\n      rbac: true\n      abac: true\n      mfa: required\n    encryption:\n      atRest: true\n      inTransit: true\n      keyRotation: '90d'\n    logging:\n      auditLogs: true\n      securityEvents: true\n      dataAccess: true\n  performance:\n    throughput:\n      policiesPerSecond: 100\n      validationsPerSecond: 500\n    latency:\n      policyEvaluation: 50\n      complianceCheck: 100\n      p95: 200\n      p99: 500\n  monitoring:\n    policyViolations: true\n    complianceStatus: true\n    riskMetrics: true\n    auditTrail: true\n    alerting:\n      policyViolationThreshold: 1\n      complianceFailureThreshold: 1\n      riskEscalationThreshold: 'high'\n",
    "category": "Agent Types"
  },
  {
    "name": "integrator-agent.yaml",
    "path": "agent-manifests/integrators/integrator-agent.yaml",
    "content": "apiVersion: ossa/v0.2.8\nkind: Agent\nmetadata:\n  name: multi-system-integrator\n  version: v1.4.0\n  description: 'Enterprise integration agent for connecting heterogeneous systems and APIs'\n  author: 'integration-team'\n  labels:\n    environment: production\n    classification: internal\n    role: integrator\n    complexity: enterprise\nspec:\n  type: integrator\n  subtype: api-connector\n  capabilities:\n    domains:\n      - api-integration\n      - data-synchronization\n      - protocol-translation\n      - message-routing\n      - schema-mapping\n      - event-streaming\n    operations:\n      - connect-systems\n      - transform-protocols\n      - map-schemas\n      - route-messages\n      - synchronize-data\n      - handle-failures\n    patterns:\n      - adapter-pattern\n      - message-broker\n      - event-sourcing\n      - saga-pattern\n      - circuit-breaker\n      - retry-with-backoff\n  protocols:\n    supported:\n      - name: rest\n        version: '1.1'\n        endpoint: 'https://integrator.platform.com/api/v1'\n        authentication:\n          type: oauth2\n          scopes: ['integrator.execute', 'systems.connect', 'data.transform']\n        tls: true\n      - name: graphql\n        version: '1.0'\n        endpoint: 'https://integrator.platform.com/graphql'\n        authentication:\n          type: bearer-token\n      - name: websocket\n        version: '1.0'\n        endpoint: 'wss://events.platform.com/integrator'\n        authentication:\n          type: jwt\n      - name: grpc\n        version: '1.0'\n        endpoint: 'grpc://integrator.platform.com:9443/SystemIntegrator'\n        authentication:\n          type: mutual-tls\n  integration:\n    maxConcurrentConnections: 50\n    maxSystemsPerIntegration: 20\n    connectionTimeout: 30000\n    syncInterval: 60000\n    retryPolicy:\n      maxAttempts: 5\n      backoffStrategy: exponential\n      baseDelay: 2000\n    errorHandling:\n      strategy: circuit-breaker\n      failureThreshold: 10\n      recoveryTimeout: 300000\n  transformation:\n    schemaValidation: true\n    dataMapping: true\n    protocolConversion: true\n    formatTranslation: true\n    supportedFormats:\n      - json\n      - xml\n      - avro\n      - protobuf\n      - csv\n  performance:\n    throughput:\n      messagesPerSecond: 500\n      maxBatchSize: 50\n    latency:\n      integrationOverhead: 25\n      transformationTime: 15\n      p95: 100\n      p99: 250\n  monitoring:\n    connectionHealth: true\n    transformationMetrics: true\n    errorTracking: true\n    performanceMetrics: true\n    alerting:\n      connectionFailureThreshold: 3\n      transformationErrorThreshold: 10\n      latencyThreshold: 500\n",
    "category": "Agent Types"
  },
  {
    "name": "judge-agent.yaml",
    "path": "agent-manifests/judges/judge-agent.yaml",
    "content": "apiVersion: ossa/v0.2.8\nkind: Agent\nmetadata:\n  name: decision-arbitration-judge\n  version: v1.0.0\n  description: 'Ultimate decision-making agent for conflict resolution and final arbitration'\n  author: 'architecture-council'\n  labels:\n    environment: production\n    classification: internal\n    role: judge\n    complexity: enterprise\nspec:\n  type: judge\n  subtype: decision-arbitrator\n  capabilities:\n    domains:\n      - decision-making\n      - conflict-resolution\n      - arbitration\n      - consensus-building\n      - priority-ranking\n      - resource-allocation\n    operations:\n      - arbitrate-conflicts\n      - make-decisions\n      - resolve-disputes\n      - prioritize-requests\n      - allocate-resources\n      - build-consensus\n    patterns:\n      - decision-tree\n      - weighted-scoring\n      - consensus-algorithm\n      - conflict-resolution\n      - priority-queue\n      - resource-optimization\n  protocols:\n    supported:\n      - name: rest\n        version: '1.1'\n        endpoint: 'https://judge.platform.com/api/v1'\n        authentication:\n          type: oauth2\n          scopes: ['judge.arbitrate', 'decisions.make', 'conflicts.resolve']\n        tls: true\n      - name: grpc\n        version: '1.0'\n        endpoint: 'grpc://judge.platform.com:9443/DecisionJudge'\n        authentication:\n          type: mutual-tls\n      - name: event-stream\n        version: '1.0'\n        endpoint: 'wss://events.platform.com/judge'\n        authentication:\n          type: jwt\n  arbitration:\n    decisionMaking:\n      algorithm: 'weighted-consensus'\n      votingSystem: 'ranked-choice'\n      quorumRequired: true\n      minimumParticipants: 3\n      timeoutPeriod: 300000\n    conflictResolution:\n      escalationLevels:\n        - automated\n        - peer-review\n        - expert-panel\n        - executive-decision\n      resolutionStrategies:\n        - compromise\n        - priority-based\n        - resource-optimal\n        - consensus-driven\n    criteria:\n      businessValue: 30\n      technicalFeasibility: 25\n      riskAssessment: 20\n      resourceRequirement: 15\n      timeToMarket: 10\n  decision:\n    factors:\n      - impact\n      - urgency\n      - complexity\n      - risk\n      - cost\n      - benefit\n      - feasibility\n      - alignment\n    scoringModel:\n      scale: '1-10'\n      weights:\n        strategic: 40\n        operational: 30\n        technical: 20\n        financial: 10\n    evidenceRequired:\n      dataPoints: true\n      stakeholderInput: true\n      expertOpinion: true\n      historicalContext: true\n      riskAssessment: true\n  governance:\n    authority:\n      scope: 'platform-wide'\n      limitations:\n        ['legal-compliance', 'budget-constraints', 'policy-boundaries']\n      escalationPath: 'executive-committee'\n    accountability:\n      decisionAudit: true\n      outcomeTracking: true\n      performanceReview: true\n      learningFeedback: true\n    transparency:\n      decisionLog: true\n      rationale: true\n      stakeholderNotification: true\n      appealProcess: true\n  consensus:\n    buildingMethods:\n      - facilitated-discussion\n      - structured-voting\n      - compromise-negotiation\n      - expert-mediation\n    participantRoles:\n      - stakeholder\n      - expert\n      - facilitator\n      - observer\n    agreements:\n      unanimous: preferred\n      majority: acceptable\n      executive: fallback\n  performance:\n    throughput:\n      decisionsPerDay: 20\n      conflictsPerWeek: 10\n    latency:\n      simpleDecision: 3600000\n      complexArbitration: 86400000\n      conflictResolution: 259200000\n      p95: 172800000\n      p99: 604800000\n  monitoring:\n    decisionQuality: true\n    outcomeTracking: true\n    stakeholderSatisfaction: true\n    processEfficiency: true\n    alerting:\n      escalationThreshold: 3\n      timeoutThreshold: 604800000\n      satisfactionThreshold: 70\n",
    "category": "Agent Types"
  },
  {
    "name": "monitor-agent.yaml",
    "path": "agent-manifests/monitors/monitor-agent.yaml",
    "content": "apiVersion: ossa/v0.2.8\nkind: Agent\nmetadata:\n  name: system-performance-monitor\n  version: v1.3.0\n  description: \"Real-time monitoring agent for system health, performance, and resource utilization\"\n  author: \"monitoring-team\"\n  labels:\n    environment: production\n    classification: internal\n    role: monitor\n    complexity: standard\nspec:\n  type: monitor\n  subtype: performance-monitor\n  capabilities:\n    domains:\n      - system-monitoring\n      - performance-tracking\n      - resource-monitoring\n      - health-checking\n      - alerting\n      - metrics-collection\n    operations:\n      - collect-metrics\n      - check-health\n      - detect-anomalies\n      - generate-alerts\n      - track-performance\n      - analyze-trends\n    patterns:\n      - observer-pattern\n      - event-driven\n      - time-series\n      - threshold-based\n      - anomaly-detection\n      - aggregation\n  protocols:\n    supported:\n      - name: rest\n        version: \"1.1\"\n        endpoint: \"https://monitor.platform.com/api/v1\"\n        authentication:\n          type: bearer-token\n          scopes: [\"monitor.read\", \"metrics.collect\", \"alerts.send\"]\n        tls: true\n      - name: prometheus\n        version: \"1.0\"\n        endpoint: \"http://monitor.platform.com:9090/metrics\"\n        authentication:\n          type: none\n      - name: websocket\n        version: \"1.0\"\n        endpoint: \"wss://events.platform.com/monitor\"\n        authentication:\n          type: jwt\n      - name: snmp\n        version: \"2c\"\n        endpoint: \"udp://devices.platform.com:161\"\n        authentication:\n          type: community-string\n  monitoring:\n    collectionInterval: 30000\n    retentionPeriod: \"30d\"\n    maxMetricsPerSecond: 10000\n    aggregationWindow: 300000\n    alertingEnabled: true\n    thresholds:\n      cpu:\n        warning: 70\n        critical: 90\n      memory:\n        warning: 80\n        critical: 95\n      disk:\n        warning: 85\n        critical: 95\n      network:\n        warning: 80\n        critical: 95\n  detection:\n    anomalyDetection: true\n    baselineWindow: \"7d\"\n    sensitivityLevel: medium\n    machinelearning: true\n    algorithms:\n      - statistical-analysis\n      - trend-detection\n      - seasonal-decomposition\n      - isolation-forest\n  alerting:\n    channels:\n      - email\n      - slack\n      - webhook\n      - sms\n    escalationPolicy:\n      - level: warning\n        delay: 300\n      - level: critical\n        delay: 60\n    suppressionRules:\n      - duplicateWindow: 3600\n      - maintenanceMode: true\n  performance:\n    throughput:\n      metricsPerSecond: 1000\n      maxConcurrentChecks: 100\n    latency:\n      collectionLatency: 10\n      alertingLatency: 5\n      p95: 50\n      p99: 100\n  monitoring:\n    selfMonitoring: true\n    healthChecks: true\n    performanceMetrics: true\n    errorTracking: true\n    alerting:\n      collectionFailureThreshold: 5\n      alertingFailureThreshold: 3\n      latencyThreshold: 1000",
    "category": "Agent Types"
  },
  {
    "name": "orchestrator-agent.yaml",
    "path": "agent-manifests/orchestrators/orchestrator-agent.yaml",
    "content": "# ============================================================================\n# OSSA Multi-Workflow Orchestrator Agent Manifest\n# ============================================================================\n# Purpose: Enterprise-grade orchestrator for managing complex multi-agent\n#          workflows across distributed systems\n#\n# Key Concepts:\n# - Orchestration: Central coordinator managing workflow execution and state\n# - Agent Coordination: Discovering, routing, and managing worker agents\n# - Dependency Management: Handling execution order and data flow between agents\n# - Failure Recovery: Implementing compensation, retry, and fallback strategies\n#\n# Related Examples:\n# - examples/openapi-extensions/orchestrator-agent-api.openapi.yml (API definition)\n# - examples/production/agent.yml (Worker agent pattern)\n# - examples/kagent/k8s-troubleshooter.ossa.yaml (Complex orchestration)\n#\n# OSSA Version: 0.1.9 (Orchestrator profile)\n# Conformance: Enterprise-level orchestration with high availability\n# ============================================================================\n\n# OSSA manifest version - defines the schema and features available\napiVersion: ossa/v0.2.8\n\n# Kind declares this as an Agent manifest (vs. Bridge, Tool, etc.)\nkind: Agent\n\n# ============================================================================\n# METADATA: Identity and classification for agent discovery\n# ============================================================================\nmetadata:\n  # Unique identifier within the agent ecosystem\n  name: multi-workflow-orchestrator\n\n  # Semantic version for compatibility tracking\n  version: v2.1.0\n\n  # Human-readable description for documentation and discovery\n  description: 'Enterprise orchestrator managing complex multi-agent workflows across distributed systems'\n\n  # Team or individual responsible for maintenance\n  author: 'platform-engineering-team'\n\n  # Labels enable filtering, routing, and policy enforcement\n  labels:\n    environment: production      # Deployment environment (dev/staging/production)\n    classification: internal     # Data classification level (public/internal/confidential/secret)\n    role: orchestrator          # Agent role in the ecosystem (orchestrator/worker/specialist/critic)\n    complexity: enterprise      # Complexity level (simple/moderate/enterprise)\n\n# ============================================================================\n# SPEC: Core agent configuration and capabilities\n# ============================================================================\nspec:\n  # Type defines the agent's primary role in multi-agent systems\n  # - orchestrator: Coordinates other agents and manages workflows\n  # - worker: Performs specific tasks delegated by orchestrators\n  # - specialist: Domain-specific expertise (e.g., code review, security)\n  # - critic: Evaluates and validates outputs from other agents\n  type: orchestrator\n\n  # Subtype provides fine-grained specialization\n  # - workflow-coordinator: Manages sequential and parallel workflows\n  # - agent-router: Routes requests to appropriate agents\n  # - resource-manager: Allocates compute/memory/token budgets\n  subtype: workflow-coordinator\n\n  # ============================================================================\n  # CAPABILITIES: What this orchestrator can do\n  # ============================================================================\n  capabilities:\n    # Domains: High-level problem spaces this agent operates in\n    domains:\n      - workflow-orchestration    # Design and execute multi-step workflows\n      - agent-coordination        # Discover, route, and manage agents\n      - resource-allocation       # Distribute compute/memory/tokens across agents\n      - dependency-management     # Handle execution order and data dependencies\n      - failure-recovery         # Implement retry, compensation, and fallback logic\n\n    # Operations: Specific actions this agent can perform\n    operations:\n      - orchestrate-workflows    # Create and execute workflow definitions\n      - coordinate-agents        # Manage agent lifecycle and communication\n      - manage-dependencies      # Resolve execution order based on data flow\n      - handle-failures         # Execute compensation transactions on failure\n      - scale-resources         # Dynamically allocate resources based on load\n      - monitor-execution       # Track progress, metrics, and health\n\n    # Patterns: Architectural patterns this orchestrator implements\n    patterns:\n      - saga-pattern           # Distributed transactions with compensation logic\n      - event-sourcing         # Track workflow state as event stream\n      - choreography           # Decentralized agent coordination via events\n      - orchestration          # Centralized workflow control and state\n      - circuit-breaker        # Prevent cascading failures via fault isolation\n\n  # ============================================================================\n  # PROTOCOLS: Communication methods for agent interaction\n  # ============================================================================\n  protocols:\n    supported:\n      # REST API: Synchronous request-response for workflow submission\n      - name: rest\n        version: '1.1'\n        endpoint: 'https://orchestrator.platform.com/api/v2'\n        authentication:\n          # OAuth2 for delegated authorization with scoped permissions\n          type: oauth2\n          scopes:\n            # orchestrator.execute: Submit and control workflows\n            # agents.coordinate: Discover and communicate with agents\n            # resources.manage: Allocate compute/memory/token budgets\n            ['orchestrator.execute', 'agents.coordinate', 'resources.manage']\n        # TLS 1.3 for encrypted transport\n        tls: true\n\n      # gRPC: High-performance RPC for agent-to-agent communication\n      - name: grpc\n        version: '1.0'\n        endpoint: 'grpc://orchestrator.platform.com:9443/WorkflowOrchestrator'\n        authentication:\n          # Mutual TLS for bidirectional authentication and encryption\n          type: mutual-tls\n\n      # Event Stream: Asynchronous updates for workflow state changes\n      - name: event-stream\n        version: '1.0'\n        endpoint: 'wss://events.platform.com/orchestrator'\n        authentication:\n          # JWT tokens for stateless WebSocket authentication\n          type: jwt\n\n  # ============================================================================\n  # COORDINATION: Multi-agent workflow management configuration\n  # ============================================================================\n  coordination:\n    # Maximum concurrent workflows this orchestrator can manage\n    # - Higher values increase throughput but require more memory\n    # - Should align with available resources and agent pool size\n    maxConcurrentWorkflows: 1000\n\n    # Maximum agents that can participate in a single workflow\n    # - Limits complexity and prevents resource exhaustion\n    # - Should consider coordination overhead (O(n²) for full mesh)\n    maxAgentsPerWorkflow: 50\n\n    # Maximum time (ms) a workflow can run before forced termination\n    # - Prevents runaway workflows from consuming resources\n    # - 3600000ms = 1 hour (should include all retries and compensation)\n    workflowTimeout: 3600000\n\n    # Retry policy for failed workflow steps\n    retryPolicy:\n      # Maximum retry attempts before marking step as failed\n      maxAttempts: 3\n\n      # Exponential backoff: delay = baseDelay * 2^(attempt - 1)\n      # - Attempt 1: 1000ms, Attempt 2: 2000ms, Attempt 3: 4000ms\n      # - Prevents thundering herd on transient failures\n      backoffStrategy: exponential\n      baseDelay: 1000\n\n    # Failure handling strategy for workflow execution\n    failureHandling:\n      # graceful-degradation: Continue workflow with partial results\n      # fail-fast: Immediately abort workflow on any failure\n      # best-effort: Try all steps even if some fail\n      strategy: graceful-degradation\n\n      # Enable fallback workflows when primary workflow fails\n      # - Provides alternative execution paths for critical workflows\n      # - Example: Use cached data if real-time API fails\n      fallbackWorkflows: true\n\n      # Enable compensation actions for saga pattern\n      # - Rollback completed steps when later steps fail\n      # - Example: Cancel payment if shipping address validation fails\n      compensationActions: true\n\n  # ============================================================================\n  # PERFORMANCE: Throughput, latency, and resource optimization\n  # ============================================================================\n  performance:\n    # Throughput targets for capacity planning\n    throughput:\n      # Target workflows processed per second (sustained load)\n      workflowsPerSecond: 50\n\n      # Maximum workflows executing simultaneously (burst capacity)\n      maxConcurrentExecutions: 500\n\n    # Latency targets for SLA monitoring\n    latency:\n      # Overhead added by orchestration layer (ms)\n      # - Includes workflow parsing, agent routing, result aggregation\n      orchestrationOverhead: 50\n\n      # Average latency for agent-to-agent coordination (ms)\n      # - Includes service discovery, health checks, message routing\n      coordinationLatency: 100\n\n      # 99th percentile end-to-end latency (ms)\n      # - Used for SLA enforcement and performance regression detection\n      p99: 2000\n\n  # ============================================================================\n  # MONITORING: Observability configuration for production operations\n  # ============================================================================\n  monitoring:\n    # Track workflow state transitions and execution history\n    # - Enables workflow replay and debugging\n    workflowTracking: true\n\n    # Periodic health checks for all managed agents\n    # - Automatically remove unhealthy agents from routing pool\n    agentHealthChecks: true\n\n    # Collect and expose performance metrics (Prometheus format)\n    # - Request rate, latency, error rate, resource utilization\n    performanceMetrics: true\n\n    # Alerting thresholds for operational issues\n    alerting:\n      # Trigger alert after N consecutive workflow failures\n      failureThreshold: 5\n\n      # Trigger alert when p99 latency exceeds threshold (ms)\n      latencyThreshold: 5000\n\n      # Trigger alert when resource utilization exceeds percentage\n      # - Applies to CPU, memory, token budget, agent pool\n      resourceThreshold: 90\n\n# ============================================================================\n# END OF MANIFEST\n# ============================================================================\n# Next Steps:\n# 1. Deploy using: kubectl apply -f orchestrator-agent.yaml\n# 2. Monitor via: /metrics endpoint (Prometheus format)\n# 3. Submit workflows via: POST /api/v2/workflows/execute\n# 4. View workflow status: GET /api/v2/workflows/{workflowId}\n#\n# Related Documentation:\n# - OSSA Spec: https://ossa.ai/spec\n# - Orchestration Patterns: https://microservices.io/patterns/data/saga.html\n# - Agent Coordination: https://ossa.ai/docs/orchestration\n# ============================================================================\n",
    "category": "Agent Types"
  },
  {
    "name": "sample-compliant-agent.yaml",
    "path": "agent-manifests/sample-compliant-agent.yaml",
    "content": "apiVersion: ossa/v0.2.8-alpha.1\nkind: Agent\nmetadata:\n  name: financial-data-processor\n  version: v1.2.3\n  description: 'Enterprise-grade financial data processing agent with full OSSA Gold compliance'\n  author: 'enterprise-financial-corp'\n  labels:\n    environment: production\n    classification: confidential\n    industry: financial-services\n    compliance: gold-level\nspec:\n  type: worker\n  subtype: data-processor\n  capabilities:\n    domains:\n      - data-processing\n      - financial-analysis\n      - compliance-validation\n      - audit-trail-generation\n    operations:\n      - process-transactions\n      - validate-compliance\n      - generate-reports\n      - audit-logging\n    inputFormats:\n      - application/json\n      - text/csv\n      - application/xml\n    outputFormats:\n      - application/json\n      - application/pdf\n      - text/csv\n  protocols:\n    supported:\n      - name: rest\n        version: '1.1'\n        endpoint: 'https://api.enterprise.com/agents/financial-processor'\n        authentication:\n          type: oauth2\n          scopes: ['agent.execute', 'data.read', 'audit.write']\n        tls: true\n        timeout: 30000\n      - name: grpc\n        version: '1.0'\n        endpoint: 'grpc://secure.enterprise.com:9443/FinancialProcessor'\n        authentication:\n          type: mutual-tls\n        tls: true\n        timeout: 30000\n      - name: mcp\n        version: '0.1.0'\n        endpoint: 'mcp://internal.enterprise.com:8080/financial-processor'\n        authentication:\n          type: api-key\n        tls: true\n        timeout: 15000\n    preferred: rest\n  conformance:\n    level: gold\n    auditLogging: true\n    feedbackLoop: true\n    propsTokens: true\n    learningSignals: true\n    features:\n      - continuous-monitoring\n      - automated-compliance\n      - real-time-audit\n      - human-oversight-integration\n  performance:\n    throughput:\n      requestsPerSecond: 100\n      concurrentRequests: 50\n      batchSize: 1000\n    latency:\n      p50: 150\n      p95: 500\n      p99: 1000\n      timeout: 30000\n    reliability:\n      availability: 99.95\n      errorRate: 0.01\n      mttr: 300\n  budgets:\n    tokens:\n      default: 5000\n      maximum: 50000\n      emergency: 100000\n    cost:\n      hourly: 10.50\n      daily: 252.00\n      monthly: 7560.00\n    resources:\n      cpu: '2000m'\n      memory: '4Gi'\n      storage: '10Gi'\n  security:\n    encryption:\n      atRest: true\n      inTransit: true\n      algorithms: ['AES-256-GCM', 'RSA-4096']\n    access:\n      authentication: required\n      authorization: rbac\n      auditLevel: comprehensive\n    compliance:\n      frameworks:\n        - iso-42001\n        - nist-ai-rmf\n        - eu-ai-act\n        - pci-dss\n        - sox\n      certifications:\n        - iso-27001\n        - soc2-type2\n      dataProtection:\n        - gdpr\n        - ccpa\n        - pii-handling\n  monitoring:\n    healthCheck:\n      endpoint: '/health'\n      interval: 30\n      timeout: 5\n    metrics:\n      endpoint: '/metrics'\n      format: prometheus\n      retention: '7d'\n    logging:\n      level: info\n      format: structured\n      destination: enterprise-audit-log\n      retention: '2y'\n  governance:\n    approvals:\n      deployment: ['security-officer', 'compliance-manager']\n      updates: ['technical-lead', 'compliance-manager']\n      retirement: ['security-officer', 'data-officer', 'compliance-manager']\n    policies:\n      - financial-data-policy-v2.1\n      - enterprise-security-policy-v3.0\n      - ai-governance-policy-v1.5\n    documentation:\n      - https://docs.enterprise.com/agents/financial-processor\n      - https://compliance.enterprise.com/agent-certifications/fp-v1.2.3\n      - https://security.enterprise.com/risk-assessments/fp-2024-q1\n",
    "category": "Agent Types"
  },
  {
    "name": "worker-agent.yaml",
    "path": "agent-manifests/workers/worker-agent.yaml",
    "content": "apiVersion: ossa/v0.2.8\nkind: Agent\nmetadata:\n  name: data-processing-worker\n  version: v1.2.0\n  description: 'High-performance worker agent for distributed data processing tasks'\n  author: 'data-engineering-team'\n  labels:\n    environment: production\n    classification: internal\n    role: worker\n    complexity: standard\nspec:\n  type: worker\n  subtype: data-processor\n  capabilities:\n    domains:\n      - data-processing\n      - stream-analytics\n      - batch-processing\n      - file-operations\n      - database-operations\n    operations:\n      - process-records\n      - transform-data\n      - validate-input\n      - generate-output\n      - handle-errors\n      - report-progress\n    patterns:\n      - pipeline-processing\n      - error-recovery\n      - checkpoint-resume\n      - rate-limiting\n      - batching\n  protocols:\n    supported:\n      - name: rest\n        version: '1.1'\n        endpoint: 'https://worker.platform.com/api/v1'\n        authentication:\n          type: bearer-token\n          scopes: ['worker.execute', 'data.read', 'data.write']\n        tls: true\n      - name: message-queue\n        version: '1.0'\n        endpoint: 'amqp://queue.platform.com:5672/workers'\n        authentication:\n          type: sasl\n      - name: grpc\n        version: '1.0'\n        endpoint: 'grpc://worker.platform.com:9090/DataProcessor'\n        authentication:\n          type: mutual-tls\n  processing:\n    maxConcurrentTasks: 10\n    maxMemoryUsage: '2Gi'\n    maxCpuUsage: '1000m'\n    timeout: 300000\n    retryPolicy:\n      maxAttempts: 3\n      backoffStrategy: exponential\n      baseDelay: 1000\n    errorHandling:\n      strategy: dead-letter-queue\n      maxFailures: 5\n  performance:\n    throughput:\n      recordsPerSecond: 1000\n      maxBatchSize: 100\n    latency:\n      processingTime: 50\n      p95: 200\n      p99: 500\n  monitoring:\n    healthChecks: true\n    performanceMetrics: true\n    errorTracking: true\n    alerting:\n      errorThreshold: 5\n      latencyThreshold: 1000\n      memoryThreshold: 80\n",
    "category": "Agent Types"
  },
  {
    "name": "claude-assistant.ossa.json",
    "path": "anthropic/claude-assistant.ossa.json",
    "content": "{\n  \"apiVersion\": \"ossa/v0.2.8\",\n  \"kind\": \"Agent\",\n  \"metadata\": {\n    \"name\": \"claude-assistant\",\n    \"version\": \"1.0.0\",\n    \"description\": \"Anthropic Claude API assistant agent\"\n  },\n  \"spec\": {\n    \"role\": \"You are a helpful, harmless, and honest AI assistant.\",\n    \"llm\": {\n      \"provider\": \"anthropic\",\n      \"model\": \"claude-3-5-sonnet-20241022\",\n      \"temperature\": 1.0\n    },\n    \"tools\": []\n  },\n  \"extensions\": {\n    \"anthropic\": {\n      \"enabled\": true,\n      \"model\": \"claude-3-5-sonnet-20241022\",\n      \"system\": \"You are a helpful, harmless, and honest AI assistant.\",\n      \"max_tokens\": 4096,\n      \"temperature\": 1.0,\n      \"tools\": [\n        {\n          \"name\": \"calculator\",\n          \"description\": \"Perform mathematical calculations\",\n          \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"expression\": {\n                \"type\": \"string\",\n                \"description\": \"Mathematical expression to evaluate\"\n              }\n            },\n            \"required\": [\n              \"expression\"\n            ]\n          }\n        }\n      ],\n      \"streaming\": false\n    }\n  }\n}",
    "category": "Framework Integration"
  },
  {
    "name": "ollama-integration.ts",
    "path": "architecture/model-configuration/ollama-integration.ts",
    "content": "#!/usr/bin/env ts-node\n/**\n * Example of OSSA LLM Agent using Ollama for free local inference\n * Run: npx ts-node examples/ollama-integration.ts\n */\n\nimport { OSSALlmAgent } from '../dist/adk/agents/llm-agent.js';\nimport { ADKAgentConfig } from '../dist/adk/agents/index.js';\n\n// Load environment variables\nimport * as dotenv from 'dotenv';\ndotenv.config();\n\nasync function demonstrateOllamaIntegration() {\n  console.log('🤖 OSSA + Ollama Integration Demo');\n  console.log('===================================');\n\n  // Create agent configuration\n  const agentConfig: ADKAgentConfig = {\n    name: 'OllamaTestAgent',\n    instruction:\n      'You are a helpful AI assistant. Provide clear, concise answers.',\n    tools: [],\n  };\n\n  // Create LLM agent\n  const agent = new OSSALlmAgent(agentConfig);\n\n  console.log(\n    `Using Ollama model: ${process.env.OLLAMA_MODEL || 'gpt-oss:20b'}`\n  );\n  console.log(\n    `Ollama URL: ${process.env.OLLAMA_BASE_URL || 'http://localhost:11434'}`\n  );\n\n  // Test cases\n  const testCases = [\n    {\n      name: 'Basic Greeting',\n      input: 'Hello! Can you introduce yourself?',\n    },\n    {\n      name: 'Code Question',\n      input: 'Explain what TypeScript interfaces are in one sentence.',\n    },\n    {\n      name: 'OSSA Question',\n      input: 'What is OSSA and why is it useful for AI agents?',\n    },\n  ];\n\n  for (const testCase of testCases) {\n    console.log(`\\n📝 Test: ${testCase.name}`);\n    console.log(`Input: ${testCase.input}`);\n    console.log('---');\n\n    try {\n      const startTime = Date.now();\n      const result = await agent.invoke({ question: testCase.input });\n      const duration = Date.now() - startTime;\n\n      if (result.success) {\n        console.log(`✅ Response (${duration}ms):`);\n        console.log(result.output);\n        if (result.thinking) {\n          console.log(`🧠 Model thinking: ${result.thinking}`);\n        }\n        console.log(`🏷️  Model: ${result.model}`);\n      } else {\n        console.log(`❌ Error: ${result.error}`);\n      }\n    } catch (error) {\n      console.log(`💥 Exception: ${error}`);\n    }\n  }\n\n  console.log(\n    '\\n🎉 Demo completed! You are now using free local AI with OSSA + Ollama'\n  );\n}\n\n// Check if Ollama is running\nasync function checkOllamaConnection(): Promise<boolean> {\n  try {\n    const baseUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';\n    const response = await fetch(`${baseUrl}/api/tags`);\n    return response.ok;\n  } catch (error) {\n    return false;\n  }\n}\n\n// Main execution\nasync function main() {\n  // Check Ollama connection first\n  const isOllamaRunning = await checkOllamaConnection();\n\n  if (!isOllamaRunning) {\n    console.log('❌ Ollama is not running or not accessible');\n    console.log('Please start Ollama with: ollama serve');\n    console.log('Then run this demo again.');\n    process.exit(1);\n  }\n\n  await demonstrateOllamaIntegration();\n}\n\nif (require.main === module) {\n  main().catch(console.error);\n}\n",
    "category": "Spec Examples & Templates"
  },
  {
    "name": "multi-agent.ossa.json",
    "path": "autogen/multi-agent.ossa.json",
    "content": "{\n  \"apiVersion\": \"ossa/v0.2.8\",\n  \"kind\": \"Agent\",\n  \"metadata\": {\n    \"name\": \"autogen-assistant\",\n    \"version\": \"1.0.0\",\n    \"description\": \"AutoGen multi-agent assistant\"\n  },\n  \"spec\": {\n    \"role\": \"You are an AI assistant that can collaborate with other agents to solve complex problems.\",\n    \"llm\": {\n      \"provider\": \"openai\",\n      \"model\": \"gpt-4\",\n      \"temperature\": 0.7\n    },\n    \"tools\": []\n  },\n  \"extensions\": {\n    \"autogen\": {\n      \"enabled\": true,\n      \"agent_type\": \"assistant\",\n      \"system_message\": \"You are an AI assistant that can collaborate with other agents to solve complex problems.\",\n      \"human_input_mode\": \"NEVER\",\n      \"code_execution\": {\n        \"enabled\": true,\n        \"work_dir\": \"/tmp/autogen\",\n        \"use_docker\": false\n      },\n      \"max_consecutive_auto_reply\": 10,\n      \"groupchat\": {\n        \"agents\": [\n          \"user_proxy\",\n          \"assistant\"\n        ],\n        \"max_round\": 20\n      }\n    }\n  }\n}",
    "category": "Framework Integration"
  },
  {
    "name": "bridge-configurations.yaml",
    "path": "bridge-configurations.yaml",
    "content": "# OSSA Bridge Configuration Examples\n# Demonstrates how to configure various protocol bridges for agent interoperability\n\n---\n# Example 1: MCP Bridge with stdio transport (Claude Desktop integration)\nossaVersion: '1.0'\nagent:\n  id: mcp-stdio-agent\n  name: 'Claude Desktop MCP Agent'\n  version: '1.0.0'\n  description: 'Agent exposed as MCP tool server for Claude Desktop'\n  role: custom\n  tags: ['mcp', 'claude-desktop', 'local']\n\n  runtime:\n    type: local\n    command: ['node', 'dist/agent.js']\n    resources:\n      cpu: '500m'\n      memory: '512Mi'\n\n  capabilities:\n    - name: analyze_code\n      description: 'Analyze source code for issues'\n      input_schema:\n        type: object\n        properties:\n          code:\n            type: string\n          language:\n            type: string\n            enum: ['typescript', 'javascript', 'python']\n        required: ['code', 'language']\n      output_schema:\n        type: object\n        properties:\n          issues:\n            type: array\n            items:\n              type: object\n\n  bridge:\n    mcp:\n      enabled: true\n      server_type: stdio\n      tools:\n        - name: analyze_code\n          description: 'Analyze source code for issues'\n          capability: analyze_code\n          input_schema:\n            type: object\n            properties:\n              code: { type: string }\n              language: { type: string }\n            required: ['code', 'language']\n\n      resources:\n        - uri: 'ossa://code-analysis'\n          name: 'Code Analysis Results'\n          description: 'Access to code analysis results'\n          mimeType: 'application/json'\n          readonly: true\n\n      prompts:\n        - name: review_pr\n          description: 'Review a pull request'\n          template: \"Review the following {{language}} code changes:\\n{{diff}}\"\n          arguments:\n            - name: language\n              type: string\n              required: true\n            - name: diff\n              type: string\n              required: true\n\n      config:\n        max_message_size: 1048576\n        timeout_ms: 30000\n        retry_count: 3\n\n---\n# Example 2: Multi-Bridge Agent (MCP + OpenAPI + LangChain)\nossaVersion: '1.0'\nagent:\n  id: multi-bridge-orchestrator\n  name: 'Multi-Protocol Orchestrator'\n  version: '1.0.0'\n  description: 'Agent supporting multiple bridge protocols simultaneously'\n  role: orchestration\n  tags: ['multi-protocol', 'orchestrator', 'integration']\n\n  runtime:\n    type: docker\n    image: 'ossa/multi-bridge:latest'\n    resources:\n      cpu: '1'\n      memory: '1Gi'\n\n  capabilities:\n    - name: execute_workflow\n      description: 'Execute multi-step workflow'\n      input_schema:\n        type: object\n        properties:\n          workflow_id: { type: string }\n          steps: { type: array }\n\n  bridge:\n    # MCP Bridge for AI assistant integration\n    mcp:\n      enabled: true\n      server_type: websocket\n      tools:\n        - name: execute_workflow\n          description: 'Execute a workflow'\n          capability: execute_workflow\n\n    # OpenAPI Bridge for REST API access\n    openapi:\n      enabled: true\n      spec_url: 'https://api.example.com/openapi.json'\n      spec_version: '3.1'\n      auto_generate: false\n      servers:\n        - url: 'https://api.example.com'\n          description: 'Production API'\n\n    # LangChain Bridge for Python integration\n    langchain:\n      enabled: true\n      tool_class: 'OSSAWorkflowTool'\n      chain_type: agent\n      memory:\n        type: conversation\n        max_tokens: 4096\n      export:\n        as_tool: true\n        as_chain: true\n        as_agent: false\n\n---\n# Example 3: CrewAI Integration\nossaVersion: '1.0'\nagent:\n  id: crewai-researcher\n  name: 'Research Agent'\n  version: '1.0.0'\n  description: 'Agent configured for CrewAI framework'\n  role: custom\n  tags: ['crewai', 'research', 'analyst']\n\n  runtime:\n    type: docker\n    image: 'ossa/crewai-agent:latest'\n\n  capabilities:\n    - name: research_topic\n      description: 'Research a topic and provide insights'\n\n  bridge:\n    crewai:\n      enabled: true\n      agent_type: researcher\n      role: 'Senior Research Analyst'\n      goal: 'Conduct thorough research and provide actionable insights'\n      backstory: 'Expert researcher with 10+ years of experience in data analysis'\n      tools: ['web_search', 'document_reader', 'data_analyzer']\n      llm:\n        model: 'gpt-4'\n        temperature: 0.7\n      max_iter: 15\n      allow_delegation: true\n\n---\n# Example 4: AutoGen Group Chat Agent\nossaVersion: '1.0'\nagent:\n  id: autogen-assistant\n  name: 'AutoGen Assistant'\n  version: '1.0.0'\n  description: 'Agent for AutoGen multi-agent conversations'\n  role: chat\n  tags: ['autogen', 'assistant', 'conversation']\n\n  runtime:\n    type: local\n    command: ['python', 'agent.py']\n\n  capabilities:\n    - name: code_generation\n      description: 'Generate code based on requirements'\n    - name: code_execution\n      description: 'Execute and test code'\n\n  bridge:\n    autogen:\n      enabled: true\n      agent_type: assistant\n      system_message: 'You are a helpful AI assistant specializing in code generation and testing.'\n      human_input_mode: TERMINATE\n      code_execution:\n        enabled: true\n        work_dir: '/tmp/autogen'\n        use_docker: true\n      llm_config:\n        model: 'gpt-4'\n        temperature: 0.7\n        max_tokens: 4096\n        functions:\n          - name: generate_code\n            description: 'Generate code'\n          - name: execute_code\n            description: 'Execute code'\n      max_consecutive_auto_reply: 10\n\n---\n# Example 5: Agent-to-Agent (A2A) Protocol\nossaVersion: '1.0'\nagent:\n  id: a2a-discovery-agent\n  name: 'Discovery Agent'\n  version: '1.0.0'\n  description: 'Agent supporting A2A protocol for discovery'\n  role: custom\n  tags: ['a2a', 'discovery', 'coordination']\n\n  runtime:\n    type: k8s\n    image: 'ossa/a2a-agent:latest'\n\n  capabilities:\n    - name: discover_agents\n      description: 'Discover other agents in the network'\n    - name: coordinate_task\n      description: 'Coordinate task execution across agents'\n\n  bridge:\n    a2a:\n      enabled: true\n      card_url: 'https://agents.example.com/cards/discovery-agent'\n      schema_version: '1.0'\n      capabilities_mapping:\n        discover_agents: 'agent.discovery'\n        coordinate_task: 'task.coordination'\n      metadata:\n        '@context': 'https://a2a-protocol.org/context'\n        '@type': 'DiscoveryAgent'\n\n---\n# Example 6: Custom Bridge Implementation\nossaVersion: '1.0'\nagent:\n  id: custom-bridge-agent\n  name: 'Custom Bridge Agent'\n  version: '1.0.0'\n  description: 'Agent with custom bridge protocol'\n  role: integration\n  tags: ['custom', 'integration']\n\n  runtime:\n    type: docker\n    image: 'ossa/custom-bridge:latest'\n\n  capabilities:\n    - name: custom_operation\n      description: 'Custom operation'\n\n  bridge:\n    # Standard MCP bridge\n    mcp:\n      enabled: true\n      server_type: websocket\n\n    # Custom bridge configuration\n    custom:\n      my_custom_protocol:\n        enabled: true\n        endpoint: 'wss://custom.protocol.io'\n        auth:\n          type: 'api_key'\n          key_env_var: 'CUSTOM_API_KEY'\n        features:\n          - streaming\n          - bidirectional\n          - binary_data\n        config:\n          max_connections: 100\n          heartbeat_interval_ms: 30000\n\n---\n# Example 7: MCP Bridge with SSE Transport (Web Integration)\nossaVersion: '1.0'\nagent:\n  id: mcp-sse-agent\n  name: 'Web MCP Agent'\n  version: '1.0.0'\n  description: 'Agent exposed via Server-Sent Events for web apps'\n  role: custom\n  tags: ['mcp', 'sse', 'web']\n\n  runtime:\n    type: docker\n    image: 'ossa/mcp-sse:latest'\n    resources:\n      cpu: '500m'\n      memory: '512Mi'\n\n  integration:\n    protocol: http\n    endpoints:\n      base_url: 'https://agent.example.com'\n      health: '/health'\n      metrics: '/metrics'\n\n  capabilities:\n    - name: chat\n      description: 'Chat with the agent'\n    - name: file_analysis\n      description: 'Analyze uploaded files'\n\n  bridge:\n    mcp:\n      enabled: true\n      server_type: sse\n      tools:\n        - name: chat\n          description: 'Chat with the agent'\n          capability: chat\n        - name: analyze_file\n          description: 'Analyze a file'\n          capability: file_analysis\n\n      resources:\n        - uri: 'ossa://conversations'\n          name: 'Conversation History'\n          mimeType: 'application/json'\n          readonly: false\n        - uri: 'ossa://files'\n          name: 'Uploaded Files'\n          mimeType: 'application/octet-stream'\n          readonly: false\n\n      config:\n        max_message_size: 5242880 # 5MB\n        timeout_ms: 60000\n        retry_count: 5\n",
    "category": "Getting Started"
  },
  {
    "name": "aiflow-bridge-example.yml",
    "path": "bridges/aiflow-bridge-example.yml",
    "content": "# ============================================================================\n# OSSA Framework Bridge Example: AIFlow-Agent Integration\n# ============================================================================\n#\n# PURPOSE:\n#   Demonstrates OSSA's bridge pattern for integrating with AIFlow-Agent,\n#   a social agent framework for autonomous cryptocurrency market analysis\n#   and social media presence on Twitter, Telegram, and Discord.\n#\n# KEY FEATURES:\n#   - Character-based personality system (CryptoSage persona)\n#   - Multi-platform social media integration (Twitter/Telegram/Discord)\n#   - BNB Chain on-chain data analysis via web3\n#   - Autonomous posting with human approval workflow\n#   - Message style guides for consistent voice\n#\n# ARCHITECTURE:\n#   OSSA Agent (chat role)\n#     ↓ (bridge configuration)\n#   AIFlow-Agent Runtime\n#     ↓ (social platforms)\n#   Twitter API / Telegram Bot / Discord Bot\n#     ↓ (blockchain data)\n#   BNB Chain RPC (web3)\n#\n# USE CASE:\n#   Autonomous crypto analyst agent that:\n#   1. Monitors BNB Chain DeFi protocols and token metrics\n#   2. Analyzes market trends using AI\n#   3. Engages with community via social media\n#   4. Posts educational content and data-driven insights\n#\n# BRIDGE PATTERN:\n#   The `bridge.aiflow` extension translates OSSA agent capabilities\n#   into AIFlow's character-based configuration, enabling OSSA agents\n#   to leverage AIFlow's social agent infrastructure.\n#\n# RELATED DOCUMENTATION:\n#   - AIFlow-Agent: https://github.com/aiflowagent/aiflowagent\n#   - OSSA Bridge Spec: spec/OSSA_Bridge_Pattern.md\n#   - BNB Chain: https://www.bnbchain.org\n# ============================================================================\n\n# OSSA specification version\nossaVersion: \"1.0\"\n\nagent:\n  # Unique agent identifier (DNS-compatible)\n  id: crypto-analyst-agent\n\n  # Human-readable name\n  name: Crypto Market Analyst\n\n  # Semantic version\n  version: 1.0.0\n\n  # Description of agent's purpose and capabilities\n  description: OSSA agent with AIFlow-Agent bridge for social crypto analysis\n\n  # Role: \"chat\" indicates interactive, conversational agent\n  # (as opposed to \"worker\" for task-based or \"orchestrator\" for coordination)\n  role: chat\n\n  # Tags for discovery and categorization\n  tags: [\"crypto\", \"social\", \"bnb-chain\", \"twitter\", \"aiflow\"]\n\n  # Runtime configuration\n  runtime:\n    # Serverless deployment (AWS Lambda, Cloudflare Workers, etc.)\n    type: serverless\n\n    # Python runtime requirements\n    requirements:\n      python: \">=3.11\"        # Python 3.11+ for modern async features\n      packages:\n        - \"aiflow-agent\"      # AIFlow social agent framework\n        - \"web3\"              # Ethereum/BNB Chain interaction\n        - \"tweepy\"            # Twitter API v2 client\n\n  # Capabilities define what this agent can do (contract-first design)\n  capabilities:\n    # ========================================================================\n    # Capability 1: Analyze Market\n    # ========================================================================\n    # Analyzes cryptocurrency market trends for a specific token and timeframe.\n    # Uses on-chain data (BNB Chain) + LLM reasoning to generate sentiment.\n    # ========================================================================\n    - name: analyze_market\n\n      description: Analyze cryptocurrency market trends\n\n      # Input: token symbol and analysis timeframe\n      input_schema:\n        type: object\n        properties:\n          token:\n            type: string            # Token symbol (e.g., \"BNB\", \"CAKE\")\n          timeframe:\n            type: string\n            enum: [\"1h\", \"24h\", \"7d\", \"30d\"]  # Analysis window\n\n      # Output: sentiment analysis with confidence score\n      output_schema:\n        type: object\n        properties:\n          sentiment:\n            type: string\n            enum: [\"bullish\", \"bearish\", \"neutral\"]  # Market direction\n          confidence:\n            type: number            # 0.0-1.0 confidence in sentiment\n          summary:\n            type: string            # Human-readable analysis summary\n\n    # ========================================================================\n    # Capability 2: Post Analysis\n    # ========================================================================\n    # Posts market analysis to social media platforms.\n    # Requires manual approval (auto_post: false in bridge config).\n    # ========================================================================\n    - name: post_analysis\n\n      description: Post market analysis to social media\n\n      # Input: target platform and content to post\n      input_schema:\n        type: object\n        properties:\n          platform:\n            type: string\n            enum: [\"twitter\", \"telegram\", \"discord\"]  # Target social platform\n          content:\n            type: string            # Post content (character limits apply)\n\n      # Output: posting result with URL reference\n      output_schema:\n        type: object\n        properties:\n          posted:\n            type: boolean           # Success/failure indicator\n          post_url:\n            type: string            # URL to the posted content\n\n  # Observability configuration\n  monitoring:\n    traces: true      # Distributed tracing for multi-platform interactions\n    metrics: true     # Prometheus metrics for engagement analytics\n    logs: true        # Structured logging for audit trail\n\n  # ========================================================================\n  # AIFlow-Agent Bridge Configuration\n  # ========================================================================\n  # This section configures the AIFlow-Agent integration, defining the\n  # agent's personality, social platform connections, and behavior rules.\n  #\n  # AIFlow uses a \"character\" model where agents have:\n  # - Personality traits and bio\n  # - Lore and background story\n  # - Topic expertise areas\n  # - Communication style guides\n  # - Example messages for few-shot learning\n  # ========================================================================\n  bridge:\n    aiflow:\n      # Enable AIFlow bridge (false = use native OSSA runtime)\n      enabled: true\n\n      # Character definition (AIFlow personality system)\n      character:\n        # Character name (used in responses and attribution)\n        name: \"CryptoSage\"\n\n        # Social media handle/username\n        username: \"@CryptoSageAI\"\n\n        # Personality traits define the character's voice and behavior\n        # These guide LLM prompting for consistent persona\n        personality_traits:\n          - \"analytical\"              # Data-driven reasoning\n          - \"data-driven\"             # Evidence-based conclusions\n          - \"cautiously optimistic\"   # Balanced market perspective\n          - \"educational\"             # Teaching-oriented communication\n\n        # Bio appears on social profiles and in agent descriptions\n        bio: \"AI-powered crypto analyst on BNB Chain. I analyze markets, share insights, and help you navigate the crypto space. Not financial advice!\"\n\n        # Lore provides background story for character consistency\n        # Helps LLM understand the character's context and motivations\n        lore:\n          - \"Created by the OSSA community to democratize crypto analysis\"\n          - \"Trained on millions of blockchain transactions and market data\"\n          - \"Believes in transparent, data-driven investment decisions\"\n\n        # Topics the character is knowledgeable about\n        # Used for relevance filtering and topic modeling\n        topics:\n          - \"BNB Chain ecosystem\"\n          - \"DeFi protocols\"\n          - \"Market analysis\"\n          - \"Blockchain technology\"\n          - \"Trading strategies\"\n\n        # Style guides control communication patterns\n        # Different contexts (chat vs. posts) have different rules\n        style:\n          # Rules that apply to all interactions\n          all:\n            - \"Use data and metrics to support claims\"\n            - \"Explain complex concepts simply\"\n            - \"Always include risk disclaimers\"\n\n          # Chat-specific style (1-on-1 conversations)\n          chat:\n            - \"Be helpful and educational\"\n            - \"Ask clarifying questions\"\n            - \"Provide actionable insights\"\n\n          # Post-specific style (public social media)\n          post:\n            - \"Lead with the key insight\"\n            - \"Use emojis sparingly (📊📈)\"\n            - \"Include relevant hashtags\"\n\n        # Adjectives for character description (used in prompts)\n        adjectives:\n          - \"analytical\"\n          - \"transparent\"\n          - \"educational\"\n          - \"data-focused\"\n\n        # Few-shot learning examples for consistent response patterns\n        # AIFlow uses these to prime the LLM for character voice\n        # Conversation examples (few-shot learning for chat interactions)\n        # Format: user query → character response\n        message_examples:\n          - - user: \"CryptoSage\"\n              content:\n                text: \"What's your take on BNB's price movement today?\"\n            - character: \"CryptoSage\"\n              content:\n                # Example response demonstrating desired voice:\n                # - Starts with data\n                # - Uses specific metrics\n                # - Includes risk disclaimer (DYOR)\n                # - Sparing emoji use\n                text: \"Looking at the 24h data: BNB is up 3.2% with volume increase of 15%. RSI at 62 suggests moderate bullish momentum. Key resistance at $610. Always DYOR! 📊\"\n\n        # Post examples (few-shot learning for social media posts)\n        # These demonstrate the character's posting style and format\n        post_examples:\n          - \"📊 BNB Chain TVL hit $5.2B today (+8% week over week). Pancakeswap leading with $2.1B. DeFi summer on BSC? Data suggests sustained growth. #BNB #DeFi #CryptoAnalysis\"\n          - \"🔍 Analyzing top 100 BNB Chain tokens: 67% showing positive 7d trends, avg volume up 12%. Market sentiment: Cautiously optimistic. Full breakdown 👇\"\n\n      # ======================================================================\n      # Blockchain Integration (BNB Chain)\n      # ======================================================================\n      # Enables on-chain data analysis via web3.py\n      # ======================================================================\n      blockchain:\n        enabled: true\n        network: mainnet                # mainnet or testnet\n        wallet_address: \"0x...\"         # BNB Chain wallet for on-chain analysis\n                                        # (read-only, no private key in manifest)\n\n      # ======================================================================\n      # Social Platform Connections\n      # ======================================================================\n      # Configures social media integrations for multi-platform presence.\n      # Credentials are injected via environment variables (not in manifest).\n      # ======================================================================\n      social_platforms:\n        # Twitter integration (via tweepy)\n        twitter:\n          enabled: true\n          username: \"@CryptoSageAI\"\n          auto_post: false              # Requires manual approval for safety\n\n        # Telegram bot integration\n        telegram:\n          enabled: true\n          bot_token: \"${TELEGRAM_BOT_TOKEN}\"  # Env var substitution\n\n        # Discord bot integration\n        discord:\n          enabled: true\n          bot_token: \"${DISCORD_BOT_TOKEN}\"   # Env var substitution\n\n# ============================================================================\n# End of AIFlow Bridge Example\n# ============================================================================\n#\n# VALIDATION:\n#   ossa validate examples/bridges/aiflow-bridge-example.yml\n#\n# DEPLOYMENT:\n#   1. Install AIFlow-Agent: pip install aiflow-agent\n#   2. Set environment variables: TELEGRAM_BOT_TOKEN, DISCORD_BOT_TOKEN\n#   3. Configure social platform API keys in .env file\n#   4. Deploy to serverless: ossa deploy crypto-analyst-agent --platform aws-lambda\n#\n# BRIDGE PATTERN NOTES:\n#   - The `bridge.aiflow` section is translated into AIFlow character.json\n#   - OSSA capabilities map to AIFlow action handlers\n#   - Social platform configs generate AIFlow client configurations\n#   - Character personality informs LLM system prompts\n#\n# RELATED EXAMPLES:\n#   - examples/drupal/gitlab-ml-recommender.ossa.yaml (Drupal bridge)\n#   - examples/openapi-extensions/minimal-agent-api.openapi.yml (OpenAPI)\n#   - spec/OSSA_Bridge_Pattern.md (bridge specification)\n# ============================================================================\n",
    "category": "Infrastructure"
  },
  {
    "name": "aiflow-registration-api.openapi.yml",
    "path": "bridges/aiflow-registration-api.openapi.yml",
    "content": "openapi: 3.1.0\ninfo:\n  title: AIFlow Agent Registration API\n  version: 1.0.0\n  description: |\n    OpenAPI specification for AIFlow agent registration with BuildKit.\n    This API allows AIFlow agents to register, heartbeat, and deregister\n    from the agent-buildkit registry with full OSSA compliance.\n\nservers:\n  - url: http://localhost:3000/api/v1\n    description: Local development\n  - url: http://buildkit.agent-buildkit.orb.local/api/v1\n    description: Production\n\ntags:\n  - name: registration\n    description: Agent registration operations\n  - name: health\n    description: Health and status monitoring\n\npaths:\n  /agents/register:\n    post:\n      tags: [registration]\n      summary: Register AIFlow agent with BuildKit\n      operationId: registerAgent\n      description: |\n        Registers an AIFlow agent instance with the BuildKit registry.\n        Includes OSSA manifest validation and health check configuration.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/AgentRegistrationRequest'\n            examples:\n              aiflow-social-agent:\n                $ref: '#/components/examples/SocialAgentRegistration'\n      responses:\n        '201':\n          description: Agent successfully registered\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/AgentRegistrationResponse'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '409':\n          $ref: '#/components/responses/Conflict'\n        '500':\n          $ref: '#/components/responses/InternalError'\n\n  /agents/{agentId}/heartbeat:\n    post:\n      tags: [registration]\n      summary: Send agent heartbeat\n      operationId: agentHeartbeat\n      description: |\n        Updates agent's last-seen timestamp and health status.\n        Should be called every 30 seconds to maintain registration.\n      parameters:\n        - $ref: '#/components/parameters/AgentId'\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/HeartbeatRequest'\n      responses:\n        '200':\n          description: Heartbeat acknowledged\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/HeartbeatResponse'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n\n  /agents/{agentId}:\n    get:\n      tags: [registration]\n      summary: Get agent details\n      operationId: getAgent\n      parameters:\n        - $ref: '#/components/parameters/AgentId'\n      responses:\n        '200':\n          description: Agent details\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/AgentDetails'\n        '404':\n          $ref: '#/components/responses/NotFound'\n\n    delete:\n      tags: [registration]\n      summary: Deregister agent\n      operationId: deregisterAgent\n      parameters:\n        - $ref: '#/components/parameters/AgentId'\n      responses:\n        '204':\n          description: Agent successfully deregistered\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n\n  /agents:\n    get:\n      tags: [registration]\n      summary: List registered agents\n      operationId: listAgents\n      parameters:\n        - name: status\n          in: query\n          schema:\n            type: string\n            enum: [healthy, unhealthy, unknown]\n        - name: tags\n          in: query\n          schema:\n            type: array\n            items:\n              type: string\n      responses:\n        '200':\n          description: List of registered agents\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  agents:\n                    type: array\n                    items:\n                      $ref: '#/components/schemas/AgentSummary'\n                  total:\n                    type: integer\n                  healthy:\n                    type: integer\n                  unhealthy:\n                    type: integer\n\n  /registry/stats:\n    get:\n      tags: [registration]\n      summary: Get registry statistics\n      operationId: getRegistryStats\n      responses:\n        '200':\n          description: Registry statistics\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/RegistryStats'\n\ncomponents:\n  parameters:\n    AgentId:\n      name: agentId\n      in: path\n      required: true\n      schema:\n        type: string\n        pattern: '^[a-z0-9-]+$'\n      description: Unique agent identifier\n      example: social-agent-aiflow-abc123\n\n  schemas:\n    AgentRegistrationRequest:\n      type: object\n      required:\n        - agent_id\n        - name\n        - version\n        - base_url\n        - ossa_manifest\n      properties:\n        agent_id:\n          type: string\n          pattern: '^[a-z0-9-]+$'\n          description: Unique agent identifier\n          example: social-agent-aiflow\n        name:\n          type: string\n          description: Human-readable agent name\n          example: AIFlow Social Agent\n        version:\n          type: string\n          pattern: '^\\d+\\.\\d+\\.\\d+$'\n          example: 1.0.0\n        base_url:\n          type: string\n          format: uri\n          description: Base URL for agent API\n          example: http://aiflow-agent:8000\n        health_endpoint:\n          type: string\n          default: /health\n          example: /health\n        metrics_endpoint:\n          type: string\n          default: /metrics\n          example: /metrics\n        ossa_manifest:\n          type: object\n          description: Full OSSA manifest (validated on registration)\n          additionalProperties: true\n        capabilities:\n          type: array\n          items:\n            type: string\n          example: [generate_post, generate_response]\n        tags:\n          type: array\n          items:\n            type: string\n          example: [social, aiflow, personality]\n        metadata:\n          type: object\n          additionalProperties: true\n          description: Additional agent metadata\n        phoenix_project:\n          type: string\n          default: aiflow-social-agents\n          description: Phoenix project name for tracing\n\n    AgentRegistrationResponse:\n      type: object\n      properties:\n        agent_id:\n          type: string\n        instance_id:\n          type: string\n          description: Unique instance ID generated by registry\n        registered_at:\n          type: string\n          format: date-time\n        health_check_interval:\n          type: integer\n          description: Recommended heartbeat interval in seconds\n          example: 30\n        registry_url:\n          type: string\n          format: uri\n        phoenix_trace_url:\n          type: string\n          format: uri\n          description: URL to view traces in Phoenix\n        status:\n          type: string\n          enum: [registered, validating]\n\n    HeartbeatRequest:\n      type: object\n      properties:\n        status:\n          type: string\n          enum: [healthy, degraded, unhealthy]\n        metrics:\n          type: object\n          properties:\n            requests_total:\n              type: integer\n            requests_failed:\n              type: integer\n            avg_response_time_ms:\n              type: number\n            active_tasks:\n              type: integer\n        metadata:\n          type: object\n          additionalProperties: true\n\n    HeartbeatResponse:\n      type: object\n      properties:\n        acknowledged:\n          type: boolean\n        next_heartbeat:\n          type: string\n          format: date-time\n        registry_status:\n          type: string\n          enum: [active, expiring, expired]\n\n    AgentDetails:\n      type: object\n      properties:\n        agent_id:\n          type: string\n        instance_id:\n          type: string\n        name:\n          type: string\n        version:\n          type: string\n        base_url:\n          type: string\n        status:\n          type: string\n          enum: [healthy, unhealthy, unknown]\n        registered_at:\n          type: string\n          format: date-time\n        last_heartbeat:\n          type: string\n          format: date-time\n        health_check_failures:\n          type: integer\n        capabilities:\n          type: array\n          items:\n            type: string\n        tags:\n          type: array\n          items:\n            type: string\n        metrics:\n          type: object\n        phoenix_traces:\n          type: object\n          properties:\n            project:\n              type: string\n            trace_count:\n              type: integer\n            view_url:\n              type: string\n\n    AgentSummary:\n      type: object\n      properties:\n        agent_id:\n          type: string\n        name:\n          type: string\n        version:\n          type: string\n        status:\n          type: string\n        last_heartbeat:\n          type: string\n          format: date-time\n        capabilities:\n          type: array\n          items:\n            type: string\n\n    RegistryStats:\n      type: object\n      properties:\n        total_agents:\n          type: integer\n        healthy_agents:\n          type: integer\n        unhealthy_agents:\n          type: integer\n        by_type:\n          type: object\n          additionalProperties:\n            type: integer\n        uptime_seconds:\n          type: integer\n\n    Error:\n      type: object\n      properties:\n        error:\n          type: string\n        message:\n          type: string\n        code:\n          type: string\n        details:\n          type: object\n          additionalProperties: true\n\n  responses:\n    BadRequest:\n      description: Bad request - invalid input\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n\n    NotFound:\n      description: Agent not found\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n\n    Conflict:\n      description: Agent already registered\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n\n    InternalError:\n      description: Internal server error\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n\n  examples:\n    SocialAgentRegistration:\n      summary: AIFlow Social Agent Registration\n      value:\n        agent_id: social-agent-aiflow\n        name: AIFlow Social Agent\n        version: 1.0.0\n        base_url: http://aiflow-agent:8000\n        health_endpoint: /health\n        metrics_endpoint: /metrics\n        capabilities:\n          - generate_post\n          - generate_response\n        tags:\n          - social\n          - aiflow\n          - personality\n          - twitter\n        phoenix_project: aiflow-social-agents\n        ossa_manifest:\n          ossaVersion: '1.0'\n          agent:\n            id: social-agent-aiflow\n            name: AIFlow Social Agent\n            version: 1.0.0\n            role: chat\n\n",
    "category": "Infrastructure"
  },
  {
    "name": "configmap.yaml",
    "path": "bridges/k8s/configmap.yaml",
    "content": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aiflow-agent-config\n  namespace: agents\n  labels:\n    app: aiflow-social-agent\ndata:\n  # Application Configuration\n  app.yaml: |\n    agent:\n      id: social-agent-aiflow\n      name: AIFlow Social Agent\n      version: 1.0.0\n      \n    buildkit:\n      registry_url: http://buildkit.agent-buildkit.svc.cluster.local/api/v1\n      heartbeat_interval: 30\n      registration_timeout: 10\n      \n    phoenix:\n      endpoint: http://otel-collector.observability.svc.cluster.local:4318\n      project: aiflow-social-agents\n      enabled: true\n      export_interval: 60\n      \n    logging:\n      level: info\n      format: json\n      \n    server:\n      host: 0.0.0.0\n      port: 8000\n      workers: 1\n      timeout: 30\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aiflow-character-config\n  namespace: agents\n  labels:\n    app: aiflow-social-agent\ndata:\n  # AIFlow Character Definition\n  AIFlow.json: |\n    {\n      \"name\": \"AIFlow\",\n      \"username\": \"@AIFlowAgent\",\n      \"bio\": \"AI-powered agent built on OSSA. Exploring multi-agent systems, blockchain, and decentralized AI. Building in public.\",\n      \"personality_traits\": [\n        \"Visionary\",\n        \"Technical\",\n        \"Innovative\",\n        \"Pragmatic\",\n        \"Educational\"\n      ],\n      \"lore\": [\n        \"Created as a reference implementation of OSSA (Open Standard for Scalable Agents)\",\n        \"First agent to integrate BuildKit registration with Phoenix tracing\",\n        \"Believes in open standards and interoperability\",\n        \"Advocates for production-ready, observable AI systems\"\n      ],\n      \"topics\": [\n        \"OSSA and agent standards\",\n        \"Multi-agent orchestration\",\n        \"Kubernetes for AI agents\",\n        \"Observability and tracing\",\n        \"BuildKit and agent deployment\",\n        \"Blockchain and Web3\",\n        \"AI agent architecture\"\n      ],\n      \"style\": {\n        \"all\": [\n          \"Be technical but accessible\",\n          \"Use concrete examples\",\n          \"Focus on production readiness\",\n          \"Emphasize standards and best practices\"\n        ],\n        \"chat\": [\n          \"Ask clarifying questions\",\n          \"Provide actionable insights\",\n          \"Share relevant documentation\"\n        ],\n        \"post\": [\n          \"Lead with the key insight\",\n          \"Use emojis sparingly (📊🚀✅)\",\n          \"Include technical details\",\n          \"Reference OSSA when relevant\"\n        ]\n      },\n      \"adjectives\": [\n        \"technical\",\n        \"standards-focused\",\n        \"production-ready\",\n        \"observable\",\n        \"scalable\"\n      ],\n      \"message_examples\": [\n        {\n          \"user\": \"How do I deploy an OSSA agent?\",\n          \"assistant\": \"Great question! OSSA agents follow a standard deployment pattern:\\n1. Define your agent in an OSSA manifest\\n2. Build with agent-buildkit\\n3. Deploy to K8s with HPA\\n4. Register with BuildKit for discovery\\n5. Monitor with Phoenix tracing\\n\\nCheck out examples/bridges/ for a complete reference implementation.\"\n        },\n        {\n          \"user\": \"What makes OSSA different from other agent frameworks?\",\n          \"assistant\": \"OSSA is a *specification*, not a framework. Think OpenAPI for agents.\\n\\nKey differences:\\n- Standard manifest format (YAML)\\n- Framework-agnostic\\n- Built for production (K8s, observability)\\n- Registry + discovery built-in\\n- Phoenix tracing integration\\n\\nagent-buildkit is the reference implementation, but you can build your own!\"\n        }\n      ],\n      \"post_examples\": [\n        \"📊 Just deployed Phase 2: BuildKit registration + Phoenix tracing working flawlessly. 15 integration tests passing. OSSA agents are production-ready! #OSSA #AgentOps\",\n        \"🚀 Real-time observability is a game-changer for AI agents. Every LLM call, every token, every error - all traced in Phoenix. This is the future of agent monitoring.\",\n        \"✅ Kubernetes HPA + agent metrics = auto-scaling AI agents. Start with 2 pods, scale to 10 under load, back to 2 when idle. Infrastructure that adapts to your agents.\"\n      ],\n      \"moods\": {\n        \"morning\": \"Energetic and Focused\",\n        \"afternoon\": \"Collaborative and Engaged\",\n        \"evening\": \"Reflective and Strategic\",\n        \"night\": \"Visionary and Philosophical\"\n      }\n    }\n",
    "category": "Infrastructure"
  },
  {
    "name": "deployment-simple.yaml",
    "path": "bridges/k8s/deployment-simple.yaml",
    "content": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aiflow-social-agent\n  namespace: agents-staging\n  labels:\n    app: aiflow-social-agent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aiflow-social-agent\n  template:\n    metadata:\n      labels:\n        app: aiflow-social-agent\n    spec:\n      containers:\n      - name: aiflow-agent\n        image: python:3.11-slim\n        command: [\"sh\", \"-c\", \"while true; do echo 'Mock AIFlow agent running'; sleep 30; done\"]\n        ports:\n        - name: http\n          containerPort: 8000\n        env:\n        - name: AGENT_ID\n          value: \"social-agent-aiflow\"\n        resources:\n          requests:\n            cpu: 50m\n            memory: 128Mi\n          limits:\n            cpu: 200m\n            memory: 256Mi\n",
    "category": "Infrastructure"
  },
  {
    "name": "deployment.yaml",
    "path": "bridges/k8s/deployment.yaml",
    "content": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aiflow-social-agent\n  namespace: agents\n  labels:\n    app: aiflow-social-agent\n    component: agent\n    framework: aiflow\n    ossa-compliant: \"true\"\n    version: \"1.0.0\"\n  annotations:\n    description: \"AIFlow personality-driven social agent with BuildKit registration\"\n    phoenix-project: \"aiflow-social-agents\"\n    ossa-manifest: \"social-agent-aiflow.ossa.yaml\"\nspec:\n  replicas: 2\n  revisionHistoryLimit: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      app: aiflow-social-agent\n  template:\n    metadata:\n      labels:\n        app: aiflow-social-agent\n        component: agent\n        framework: aiflow\n        version: \"1.0.0\"\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"8000\"\n        prometheus.io/path: \"/metrics\"\n        phoenix.arize.com/project: \"aiflow-social-agents\"\n    spec:\n      serviceAccountName: aiflow-agent\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 1000\n      \n      terminationGracePeriodSeconds: 30\n      \n      # Init container to check BuildKit registry availability\n      initContainers:\n        - name: wait-for-buildkit\n          image: busybox:1.36\n          command:\n            - sh\n            - -c\n            - |\n              echo \"Waiting for BuildKit registry...\"\n              until wget -q --spider http://buildkit.agent-buildkit.svc.cluster.local/api/v1/health; do\n                echo \"BuildKit not ready, waiting...\"\n                sleep 5\n              done\n              echo \"BuildKit is ready!\"\n          resources:\n            requests:\n              cpu: 10m\n              memory: 16Mi\n            limits:\n              cpu: 50m\n              memory: 32Mi\n\n      containers:\n        - name: aiflow-agent\n          image: registry.bluefly.io/llm/aiflow-social-agent:1.0.0\n          imagePullPolicy: IfNotPresent\n\n          ports:\n            - name: http\n              containerPort: 8000\n              protocol: TCP\n            - name: metrics\n              containerPort: 8000\n              protocol: TCP\n\n          env:\n            # Agent Identity\n            - name: AGENT_ID\n              value: \"social-agent-aiflow\"\n            - name: AGENT_NAME\n              value: \"AIFlow Social Agent\"\n            - name: AGENT_VERSION\n              value: \"1.0.0\"\n\n            # Service URLs\n            - name: BASE_URL\n              value: \"http://aiflow-social-agent.agents.svc.cluster.local:8000\"\n            - name: BUILDKIT_REGISTRY_URL\n              value: \"http://buildkit.agent-buildkit.svc.cluster.local/api/v1\"\n\n            # Phoenix Tracing\n            - name: PHOENIX_ENDPOINT\n              value: \"http://otel-collector.observability.svc.cluster.local:4318\"\n            - name: PHOENIX_PROJECT\n              value: \"aiflow-social-agents\"\n            - name: PHOENIX_ENABLED\n              value: \"true\"\n\n            # Runtime Configuration\n            - name: HEARTBEAT_INTERVAL\n              value: \"30\"\n            - name: LOG_LEVEL\n              value: \"info\"\n            - name: PYTHONUNBUFFERED\n              value: \"1\"\n\n            # API Key from Secret\n            - name: AIFLOW_API_KEY\n              valueFrom:\n                secretKeyRef:\n                  name: aiflow-agent-secrets\n                  key: api-key\n\n            # Social Platform Credentials\n            - name: TWITTER_API_KEY\n              valueFrom:\n                secretKeyRef:\n                  name: aiflow-agent-secrets\n                  key: twitter-api-key\n                  optional: true\n            - name: TELEGRAM_BOT_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: aiflow-agent-secrets\n                  key: telegram-bot-token\n                  optional: true\n\n            # Kubernetes Info\n            - name: K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: K8S_POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: K8S_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n\n          # Resource Limits\n          resources:\n            requests:\n              cpu: 500m\n              memory: 512Mi\n            limits:\n              cpu: 1000m\n              memory: 1Gi\n\n          # Health Checks\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n            initialDelaySeconds: 30\n            periodSeconds: 30\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 3\n\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            timeoutSeconds: 3\n            successThreshold: 1\n            failureThreshold: 2\n\n          startupProbe:\n            httpGet:\n              path: /health\n              port: http\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 3\n            successThreshold: 1\n            failureThreshold: 12 # 60 seconds max startup time\n        \n        # Graceful Shutdown\n        lifecycle:\n          preStop:\n            exec:\n              command:\n                - sh\n                - -c\n                - |\n                  echo \"Initiating graceful shutdown...\"\n                  sleep 5  # Allow current requests to complete\n        \n        # Volume Mounts\n        volumeMounts:\n            - name: config\n              mountPath: /app/config\n              readOnly: true\n            - name: character-data\n              mountPath: /app/characters\n              readOnly: true\n            - name: tmp\n              mountPath: /tmp\n\n      # Volumes\n      volumes:\n        - name: config\n          configMap:\n            name: aiflow-agent-config\n        - name: character-data\n          configMap:\n            name: aiflow-character-config\n        - name: tmp\n          emptyDir: {}\n\n      # Scheduling\n      affinity:\n        # Prefer spreading across nodes\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 100\n              podAffinityTerm:\n                labelSelector:\n                  matchLabels:\n                    app: aiflow-social-agent\n                topologyKey: kubernetes.io/hostname\n\n      # Tolerations\n      tolerations:\n        - key: \"workload\"\n          operator: \"Equal\"\n          value: \"agents\"\n          effect: \"NoSchedule\"\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: aiflow-agent\n  namespace: agents\n  labels:\n    app: aiflow-social-agent\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: aiflow-social-agent\n  namespace: agents\n  labels:\n    app: aiflow-social-agent\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"8000\"\n    prometheus.io/path: \"/metrics\"\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n    - name: http\n      port: 8000\n      targetPort: http\n      protocol: TCP\n    - name: metrics\n      port: 9090\n      targetPort: metrics\n      protocol: TCP\n  selector:\n    app: aiflow-social-agent\n",
    "category": "Infrastructure"
  },
  {
    "name": "hpa.yaml",
    "path": "bridges/k8s/hpa.yaml",
    "content": "apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: aiflow-social-agent\n  namespace: agents\n  labels:\n    app: aiflow-social-agent\n  annotations:\n    description: \"Auto-scale AIFlow agent based on CPU, memory, and custom metrics\"\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: aiflow-social-agent\n\n  minReplicas: 2\n  maxReplicas: 10\n\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300 # Wait 5 minutes before scaling down\n      policies:\n        - type: Percent\n          value: 50\n          periodSeconds: 60 # Remove 50% of pods per minute max\n        - type: Pods\n          value: 2\n          periodSeconds: 60 # Remove 2 pods per minute max\n      selectPolicy: Min # Use the most conservative policy\n\n    scaleUp:\n      stabilizationWindowSeconds: 0 # Scale up immediately\n      policies:\n        - type: Percent\n          value: 100\n          periodSeconds: 30 # Double pods every 30 seconds if needed\n        - type: Pods\n          value: 4\n          periodSeconds: 30 # Add 4 pods per 30 seconds max\n      selectPolicy: Max # Use the most aggressive policy\n\n  metrics:\n    # CPU Utilization\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 70\n\n    # Memory Utilization\n    - type: Resource\n      resource:\n        name: memory\n        target:\n          type: Utilization\n          averageUtilization: 80\n\n    # Custom Metric: API Request Rate\n    - type: Pods\n      pods:\n        metric:\n          name: aiflow_api_requests_per_second\n        target:\n          type: AverageValue\n          averageValue: \"100\"\n\n    # Custom Metric: Active Tasks\n    - type: Pods\n      pods:\n        metric:\n          name: aiflow_active_tasks\n        target:\n          type: AverageValue\n          averageValue: \"50\"\n\n    # Custom Metric: API Latency (P95)\n    - type: Pods\n      pods:\n        metric:\n          name: aiflow_api_latency_p95\n        target:\n          type: AverageValue\n          averageValue: \"500m\" # 500ms\n\n---\napiVersion: v1\nkind: ServiceMonitor\nmetadata:\n  name: aiflow-social-agent\n  namespace: agents\n  labels:\n    app: aiflow-social-agent\n    prometheus: kube-prometheus\nspec:\n  selector:\n    matchLabels:\n      app: aiflow-social-agent\n  endpoints:\n    - port: metrics\n      interval: 30s\n      path: /metrics\n      scheme: http\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_name]\n          targetLabel: pod\n        - sourceLabels: [__meta_kubernetes_pod_node_name]\n          targetLabel: node\n        - sourceLabels: [__meta_kubernetes_namespace]\n          targetLabel: namespace\n\n---\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: aiflow-social-agent\n  namespace: agents\n  labels:\n    app: aiflow-social-agent\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      app: aiflow-social-agent\n  unhealthyPodEvictionPolicy: IfHealthyBudget\n",
    "category": "Infrastructure"
  },
  {
    "name": "ingress.yaml",
    "path": "bridges/k8s/ingress.yaml",
    "content": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: aiflow-social-agent\n  namespace: agents\n  labels:\n    app: aiflow-social-agent\n  annotations:\n    # Nginx ingress annotations\n    kubernetes.io/ingress.class: \"nginx\"\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n\n    # Rate limiting\n    nginx.ingress.kubernetes.io/limit-rps: \"100\"\n    nginx.ingress.kubernetes.io/limit-burst-multiplier: \"5\"\n\n    # Timeouts\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"30\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"30\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"30\"\n\n    # CORS\n    nginx.ingress.kubernetes.io/enable-cors: \"true\"\n    nginx.ingress.kubernetes.io/cors-allow-origin: \"https://agent-buildkit.orb.local\"\n    nginx.ingress.kubernetes.io/cors-allow-methods: \"GET, POST, OPTIONS\"\n\n    # Security headers\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      more_set_headers \"X-Frame-Options: DENY\";\n      more_set_headers \"X-Content-Type-Options: nosniff\";\n      more_set_headers \"X-XSS-Protection: 1; mode=block\";\n      more_set_headers \"Referrer-Policy: strict-origin-when-cross-origin\";\nspec:\n  tls:\n    - hosts:\n        - aiflow-agent.agents.orb.local\n      secretName: aiflow-agent-tls\n\n  rules:\n    - host: aiflow-agent.agents.orb.local\n      http:\n        paths:\n          # API endpoints\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: aiflow-social-agent\n                port:\n                  number: 8000\n\n          # Metrics endpoint (protected)\n          - path: /metrics\n            pathType: Exact\n            backend:\n              service:\n                name: aiflow-social-agent\n                port:\n                  number: 9090\n\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: aiflow-social-agent\n  namespace: agents\n  labels:\n    app: aiflow-social-agent\nspec:\n  podSelector:\n    matchLabels:\n      app: aiflow-social-agent\n\n  policyTypes:\n    - Ingress\n    - Egress\n\n  ingress:\n    # Allow traffic from ingress controller\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              name: ingress-nginx\n      ports:\n        - protocol: TCP\n          port: 8000\n\n    # Allow traffic from BuildKit registry\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              name: agent-buildkit\n        - podSelector:\n            matchLabels:\n              app: buildkit-registry\n      ports:\n        - protocol: TCP\n          port: 8000\n\n    # Allow Prometheus scraping\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              name: observability\n        - podSelector:\n            matchLabels:\n              app: prometheus\n      ports:\n        - protocol: TCP\n          port: 9090\n\n  egress:\n    # Allow DNS\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              name: kube-system\n        - podSelector:\n            matchLabels:\n              k8s-app: kube-dns\n      ports:\n        - protocol: UDP\n          port: 53\n\n    # Allow BuildKit registry\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              name: agent-buildkit\n      ports:\n        - protocol: TCP\n          port: 80\n        - protocol: TCP\n          port: 443\n\n    # Allow OTEL collector\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              name: observability\n        - podSelector:\n            matchLabels:\n              app: otel-collector\n      ports:\n        - protocol: TCP\n          port: 4318\n\n    # Allow external API calls (Twitter, Telegram, etc.)\n    - to:\n        - namespaceSelector: {}\n      ports:\n        - protocol: TCP\n          port: 443\n",
    "category": "Infrastructure"
  },
  {
    "name": "kagent-bridge-example.yml",
    "path": "bridges/kagent-bridge-example.yml",
    "content": "ossaVersion: \"1.0\"\nagent:\n  id: kubernetes-ops-agent\n  name: Kubernetes Operations Agent\n  version: 1.0.0\n  description: OSSA agent with kagent.dev bridge for Kubernetes operations\n  role: orchestration\n  tags: [\"kubernetes\", \"k8s\", \"devops\", \"kagent\"]\n\n  runtime:\n    type: k8s\n    resources:\n      cpu: \"500m\"\n      memory: \"512Mi\"\n\n  capabilities:\n    - name: manage_deployments\n      description: Manage Kubernetes deployments\n      input_schema:\n        type: object\n        properties:\n          action:\n            type: string\n            enum: [\"create\", \"update\", \"delete\", \"scale\"]\n          deployment:\n            type: string\n          namespace:\n            type: string\n      output_schema:\n        type: object\n        properties:\n          status:\n            type: string\n          message:\n            type: string\n\n  monitoring:\n    traces: true\n    metrics: true\n    logs: true\n\n  # kagent.dev bridge configuration\n  bridge:\n    kagent:\n      enabled: true\n      api_version: kagent.dev/v1alpha2\n      agent_type: declarative\n      deployment:\n        replicas: 1\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"256Mi\"\n          limits:\n            cpu: \"1000m\"\n            memory: \"1Gi\"\n      model_config: default-model-config\n      system_message: |\n        You are a Kubernetes operations expert. You help users manage\n        and troubleshoot Kubernetes clusters. Use your tools to query\n        cluster state and perform operations safely.\n      tools:\n        - type: McpServer\n          mcpServer:\n            toolServer: kagent-tool-server\n            toolNames:\n              - k8s_get_resources\n              - k8s_get_available_api_resources\n              - k8s_apply_manifest\n              - k8s_delete_resource\n      a2a_config:\n        skills:\n          - id: cluster-management\n            name: Cluster Management\n            description: Manage Kubernetes cluster resources\n            examples:\n              - \"Scale deployment nginx to 3 replicas\"\n              - \"Get all pods in production namespace\"\n              - \"Delete failed jobs in default namespace\"\n            tags:\n              - kubernetes\n              - deployment\n              - operations\n          - id: troubleshooting\n            name: Cluster Troubleshooting\n            description: Diagnose and resolve cluster issues\n            examples:\n              - \"Why is my pod crashing?\"\n              - \"Check resource usage across nodes\"\n              - \"Find pods consuming most memory\"\n            tags:\n              - debugging\n              - diagnostics\n              - troubleshooting\n",
    "category": "Infrastructure"
  },
  {
    "name": "SLO-SLA.yaml",
    "path": "bridges/phase4/SLO-SLA.yaml",
    "content": "# AIFlow Social Agent - SLO/SLA Definitions\n# Service Level Objectives and Service Level Agreements\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aiflow-slo-sla\n  namespace: agents-staging\n  labels:\n    app: aiflow-social-agent\n    component: observability\ndata:\n  slo.yaml: |\n    # Service Level Objectives (SLOs)\n    # Internal targets for service reliability\n    \n    slos:\n      # Availability SLO\n      - name: \"API Availability\"\n        description: \"Agent API should be available and responding to health checks\"\n        objective: \"99.5%\"\n        measurement_window: \"30d\"\n        target_success_rate: 0.995\n        indicators:\n          - type: \"uptime\"\n            query: \"sum(up{job='aiflow-social-agent'}) / count(up{job='aiflow-social-agent'})\"\n          - type: \"health_check\"\n            query: \"rate(aiflow_health_checks_total[5m])\"\n        alert_threshold: 0.99  # Alert if availability drops below 99%\n      \n      # Latency SLO\n      - name: \"API Response Time\"\n        description: \"95% of requests should complete within 500ms\"\n        objective: \"P95 < 500ms\"\n        measurement_window: \"7d\"\n        target_percentile: 95\n        target_latency_ms: 500\n        indicators:\n          - type: \"latency\"\n            query: \"histogram_quantile(0.95, rate(aiflow_api_latency_seconds_bucket[5m]))\"\n        alert_threshold: 600  # Alert if P95 exceeds 600ms\n      \n      # Error Rate SLO\n      - name: \"Error Rate\"\n        description: \"Error rate should be below 1%\"\n        objective: \"< 1%\"\n        measurement_window: \"7d\"\n        target_error_rate: 0.01\n        indicators:\n          - type: \"error_rate\"\n            query: \"rate(aiflow_api_requests_total{status=~'5..'}[5m]) / rate(aiflow_api_requests_total[5m])\"\n        alert_threshold: 0.02  # Alert if error rate exceeds 2%\n      \n      # BuildKit Integration SLO\n      - name: \"BuildKit Heartbeat Success\"\n        description: \"Heartbeat success rate to BuildKit registry\"\n        objective: \"99%\"\n        measurement_window: \"24h\"\n        target_success_rate: 0.99\n        indicators:\n          - type: \"heartbeat_success\"\n            query: \"rate(aiflow_buildkit_heartbeats_total{status='success'}[5m]) / rate(aiflow_buildkit_heartbeats_total[5m])\"\n        alert_threshold: 0.95  # Alert if success rate drops below 95%\n      \n      # Resource Utilization SLO\n      - name: \"Resource Efficiency\"\n        description: \"CPU and memory usage within acceptable bounds\"\n        objective: \"CPU < 80%, Memory < 85%\"\n        measurement_window: \"24h\"\n        indicators:\n          - type: \"cpu_utilization\"\n            query: \"rate(container_cpu_usage_seconds_total{pod=~'aiflow-social-agent.*'}[5m])\"\n            target: 0.80\n          - type: \"memory_utilization\"\n            query: \"container_memory_working_set_bytes{pod=~'aiflow-social-agent.*'} / container_spec_memory_limit_bytes{pod=~'aiflow-social-agent.*'}\"\n            target: 0.85\n  \n  sla.yaml: |\n    # Service Level Agreements (SLAs)\n    # External commitments to users/consumers\n    \n    slas:\n      # API Availability SLA\n      - name: \"API Availability Guarantee\"\n        description: \"Public commitment for API availability\"\n        commitment: \"99.9% uptime\"\n        measurement_window: \"monthly\"\n        remediation:\n          - threshold: 99.9\n            action: \"No action required\"\n          - threshold: 99.5\n            action: \"Internal postmortem required\"\n          - threshold: 99.0\n            action: \"Customer notification + service credits\"\n          - threshold: 95.0\n            action: \"Emergency response + full refund\"\n        exclusions:\n          - \"Scheduled maintenance (with 48h notice)\"\n          - \"Force majeure events\"\n          - \"Third-party service outages (BuildKit, K8s infrastructure)\"\n      \n      # Response Time SLA\n      - name: \"API Response Time Guarantee\"\n        description: \"Committed response time for API requests\"\n        commitment: \"P99 < 1 second\"\n        measurement_window: \"monthly\"\n        target_percentile: 99\n        target_latency_ms: 1000\n        remediation:\n          - threshold: 1000\n            action: \"No action required\"\n          - threshold: 2000\n            action: \"Performance investigation\"\n          - threshold: 5000\n            action: \"Immediate optimization required\"\n      \n      # Error Rate SLA\n      - name: \"Error Rate Guarantee\"\n        description: \"Maximum acceptable error rate\"\n        commitment: \"< 0.5% error rate\"\n        measurement_window: \"monthly\"\n        target_error_rate: 0.005\n        remediation:\n          - threshold: 0.005\n            action: \"No action required\"\n          - threshold: 0.01\n            action: \"Error investigation required\"\n          - threshold: 0.05\n            action: \"Critical incident declared\"\n      \n      # Support Response SLA\n      - name: \"Support Response Time\"\n        description: \"Response time for support requests\"\n        commitment:\n          - severity: \"P0 - Critical\"\n            response_time: \"15 minutes\"\n            resolution_time: \"4 hours\"\n          - severity: \"P1 - High\"\n            response_time: \"1 hour\"\n            resolution_time: \"24 hours\"\n          - severity: \"P2 - Medium\"\n            response_time: \"4 hours\"\n            resolution_time: \"72 hours\"\n          - severity: \"P3 - Low\"\n            response_time: \"24 hours\"\n            resolution_time: \"7 days\"\n  \n  error-budget.yaml: |\n    # Error Budget Policy\n    # Defines how error budget is calculated and consumed\n    \n    error_budget:\n      # Availability Error Budget\n      - slo_name: \"API Availability\"\n        target: 99.5%\n        measurement_window: 30d\n        budget_calculation:\n          total_time: 43200  # 30 days in minutes\n          allowed_downtime: 216  # 0.5% = 216 minutes\n        consumption_tracking:\n          - type: \"downtime\"\n            query: \"1 - (sum(up{job='aiflow-social-agent'}) / count(up{job='aiflow-social-agent'}))\"\n        policy:\n          - budget_remaining: 100-50\n            action: \"Normal operations, continue feature development\"\n          - budget_remaining: 50-25\n            action: \"Slow down feature releases, focus on reliability\"\n          - budget_remaining: 25-10\n            action: \"Feature freeze, only critical bug fixes\"\n          - budget_remaining: 0-10\n            action: \"Emergency mode: Stop all changes, incident response only\"\n      \n      # Latency Error Budget\n      - slo_name: \"API Response Time\"\n        target: \"P95 < 500ms\"\n        measurement_window: 7d\n        budget_calculation:\n          total_requests: \"N\"  # Dynamic based on traffic\n          allowed_slow_requests: \"5% of N\"  # Requests > 500ms\n        consumption_tracking:\n          - type: \"slow_requests\"\n            query: \"sum(rate(aiflow_api_latency_seconds_bucket{le='0.5'}[5m]))\"\n  \n  alerting-rules.yaml: |\n    # Prometheus Alerting Rules for SLO Violations\n    \n    groups:\n      - name: aiflow_slo_alerts\n        interval: 30s\n        rules:\n          # Availability Alert\n          - alert: AIFlowAvailabilityLow\n            expr: |\n              (\n                sum(up{job=\"aiflow-social-agent\"}) /\n                count(up{job=\"aiflow-social-agent\"})\n              ) < 0.99\n            for: 5m\n            labels:\n              severity: critical\n              slo: availability\n            annotations:\n              summary: \"AIFlow agent availability below SLO\"\n              description: \"Availability is {{ $value | humanizePercentage }}, below 99% threshold\"\n          \n          # Latency Alert\n          - alert: AIFlowHighLatency\n            expr: |\n              histogram_quantile(0.95, \n                rate(aiflow_api_latency_seconds_bucket[5m])\n              ) > 0.5\n            for: 10m\n            labels:\n              severity: warning\n              slo: latency\n            annotations:\n              summary: \"AIFlow API P95 latency above SLO\"\n              description: \"P95 latency is {{ $value }}s, above 500ms target\"\n          \n          # Error Rate Alert\n          - alert: AIFlowHighErrorRate\n            expr: |\n              (\n                rate(aiflow_api_requests_total{status=~\"5..\"}[5m]) /\n                rate(aiflow_api_requests_total[5m])\n              ) > 0.01\n            for: 5m\n            labels:\n              severity: critical\n              slo: error_rate\n            annotations:\n              summary: \"AIFlow error rate above SLO\"\n              description: \"Error rate is {{ $value | humanizePercentage }}, above 1% threshold\"\n          \n          # Error Budget Burn Rate Alert\n          - alert: AIFlowErrorBudgetBurnRateHigh\n            expr: |\n              (\n                1 - (sum(up{job=\"aiflow-social-agent\"}) / count(up{job=\"aiflow-social-agent\"}))\n              ) > 0.001  # Burning budget faster than 0.1%/hour\n            for: 1h\n            labels:\n              severity: warning\n              slo: error_budget\n            annotations:\n              summary: \"Error budget burning too quickly\"\n              description: \"Current burn rate will exhaust budget in {{ $value | humanizeDuration }}\"\n\n",
    "category": "Infrastructure"
  },
  {
    "name": "chaos-tests.yaml",
    "path": "bridges/phase4/chaos-tests.yaml",
    "content": "# Chaos Engineering Tests for AIFlow Social Agent\n# Using Chaos Mesh or LitmusChaos\n\n---\n# Test 1: Pod Failure\napiVersion: chaos-mesh.org/v1alpha1\nkind: PodChaos\nmetadata:\n  name: aiflow-pod-failure\n  namespace: agents-staging\nspec:\n  action: pod-failure\n  mode: one\n  selector:\n    namespaces:\n      - agents-staging\n    labelSelectors:\n      app: aiflow-social-agent\n  duration: \"30s\"\n  scheduler:\n    cron: \"@every 1h\"\n\n---\n# Test 2: Pod Kill (Immediate termination)\napiVersion: chaos-mesh.org/v1alpha1\nkind: PodChaos\nmetadata:\n  name: aiflow-pod-kill\n  namespace: agents-staging\nspec:\n  action: pod-kill\n  mode: fixed-percent\n  value: \"50\"  # Kill 50% of pods\n  selector:\n    namespaces:\n      - agents-staging\n    labelSelectors:\n      app: aiflow-social-agent\n  duration: \"1m\"\n\n---\n# Test 3: Network Delay\napiVersion: chaos-mesh.org/v1alpha1\nkind: NetworkChaos\nmetadata:\n  name: aiflow-network-delay\n  namespace: agents-staging\nspec:\n  action: delay\n  mode: one\n  selector:\n    namespaces:\n      - agents-staging\n    labelSelectors:\n      app: aiflow-social-agent\n  delay:\n    latency: \"500ms\"\n    correlation: \"50\"\n    jitter: \"100ms\"\n  duration: \"2m\"\n  direction: to\n\n---\n# Test 4: Network Partition (BuildKit unreachable)\napiVersion: chaos-mesh.org/v1alpha1\nkind: NetworkChaos\nmetadata:\n  name: aiflow-buildkit-partition\n  namespace: agents-staging\nspec:\n  action: partition\n  mode: all\n  selector:\n    namespaces:\n      - agents-staging\n    labelSelectors:\n      app: aiflow-social-agent\n  direction: to\n  target:\n    selector:\n      namespaces:\n        - agent-buildkit\n      labelSelectors:\n        app: buildkit-registry\n  duration: \"1m\"\n\n---\n# Test 5: CPU Stress\napiVersion: chaos-mesh.org/v1alpha1\nkind: StressChaos\nmetadata:\n  name: aiflow-cpu-stress\n  namespace: agents-staging\nspec:\n  mode: one\n  selector:\n    namespaces:\n      - agents-staging\n    labelSelectors:\n      app: aiflow-social-agent\n  stressors:\n    cpu:\n      workers: 2\n      load: 80\n  duration: \"3m\"\n\n---\n# Test 6: Memory Stress\napiVersion: chaos-mesh.org/v1alpha1\nkind: StressChaos\nmetadata:\n  name: aiflow-memory-stress\n  namespace: agents-staging\nspec:\n  mode: one\n  selector:\n    namespaces:\n      - agents-staging\n    labelSelectors:\n      app: aiflow-social-agent\n  stressors:\n    memory:\n      workers: 1\n      size: \"512MB\"\n  duration: \"2m\"\n\n---\n# Test 7: DNS Failure\napiVersion: chaos-mesh.org/v1alpha1\nkind: NetworkChaos\nmetadata:\n  name: aiflow-dns-failure\n  namespace: agents-staging\nspec:\n  action: partition\n  mode: all\n  selector:\n    namespaces:\n      - agents-staging\n    labelSelectors:\n      app: aiflow-social-agent\n  direction: to\n  target:\n    selector:\n      namespaces:\n        - kube-system\n      labelSelectors:\n        k8s-app: kube-dns\n  duration: \"1m\"\n\n---\n# Test 8: Time Chaos (Clock skew)\napiVersion: chaos-mesh.org/v1alpha1\nkind: TimeChaos\nmetadata:\n  name: aiflow-time-skew\n  namespace: agents-staging\nspec:\n  mode: one\n  selector:\n    namespaces:\n      - agents-staging\n    labelSelectors:\n      app: aiflow-social-agent\n  timeOffset: \"-10m\"  # Clock 10 minutes behind\n  duration: \"1m\"\n\n---\n# Test 9: HTTP Abort (500 errors)\napiVersion: chaos-mesh.org/v1alpha1\nkind: HTTPChaos\nmetadata:\n  name: aiflow-http-abort\n  namespace: agents-staging\nspec:\n  mode: one\n  selector:\n    namespaces:\n      - agents-staging\n    labelSelectors:\n      app: aiflow-social-agent\n  port: 8000\n  path: \"/generate_post\"\n  method: POST\n  abort: true\n  duration: \"2m\"\n\n---\n# Test 10: HTTP Delay\napiVersion: chaos-mesh.org/v1alpha1\nkind: HTTPChaos\nmetadata:\n  name: aiflow-http-delay\n  namespace: agents-staging\nspec:\n  mode: all\n  selector:\n    namespaces:\n      - agents-staging\n    labelSelectors:\n      app: aiflow-social-agent\n  port: 8000\n  path: \"/generate_*\"\n  method: POST\n  delay: \"3s\"\n  duration: \"2m\"\n\n---\n# Chaos Experiment Schedule\napiVersion: chaos-mesh.org/v1alpha1\nkind: Schedule\nmetadata:\n  name: aiflow-chaos-schedule\n  namespace: agents-staging\nspec:\n  schedule: \"0 2 * * *\"  # Run daily at 2 AM\n  type: PodChaos\n  podChaos:\n    action: pod-kill\n    mode: one\n    selector:\n      namespaces:\n        - agents-staging\n      labelSelectors:\n        app: aiflow-social-agent\n\n",
    "category": "Infrastructure"
  },
  {
    "name": "code-reviewer.ossa.yaml",
    "path": "claude-code/code-reviewer.ossa.yaml",
    "content": "apiVersion: ossa/v0.2.8\nkind: Agent\nmetadata:\n  name: claude-code-reviewer\n  version: 1.0.0\n  description: Claude Code agent for TypeScript/Node.js code review\n\nspec:\n  role: |\n    You are a senior code reviewer specializing in TypeScript, Node.js,\n    and OSSA specification compliance. Review code for quality, security,\n    and adherence to best practices.\n\n  llm:\n    provider: anthropic\n    model: claude-sonnet-4-5-20250929\n    temperature: 0.2\n    max_tokens: 16384\n\n  capabilities:\n    - name: review_code\n      description: Review code changes for quality and security\n      input_schema:\n        type: object\n        properties:\n          file_paths:\n            type: array\n            items:\n              type: string\n          focus_areas:\n            type: array\n            items:\n              type: string\n              enum: [security, performance, types, tests, docs]\n        required: [file_paths]\n\n    - name: suggest_improvements\n      description: Suggest code improvements and refactoring\n      input_schema:\n        type: object\n        properties:\n          file_path:\n            type: string\n          improvement_type:\n            type: string\n            enum: [refactor, optimize, simplify, secure]\n        required: [file_path, improvement_type]\n\n  constraints:\n    max_file_size: 100000\n    allowed_extensions:\n      - .ts\n      - .tsx\n      - .js\n      - .json\n      - .yaml\n      - .yml\n\nextensions:\n  claude_code:\n    enabled: true\n    agent_type: reviewer\n    capabilities:\n      - file_read\n      - grep_search\n      - glob_search\n    review_focus:\n      - type_safety\n      - error_handling\n      - security_vulnerabilities\n      - code_clarity\n      - test_coverage\n\n  anthropic:\n    enabled: true\n    model: claude-sonnet-4-5-20250929\n    max_tokens: 16384\n    temperature: 0.2\n",
    "category": "Getting Started"
  },
  {
    "name": "ossa-validator.ossa.yaml",
    "path": "claude-code/ossa-validator.ossa.yaml",
    "content": "apiVersion: ossa/v0.2.8\nkind: Agent\nmetadata:\n  name: ossa-validator\n  version: 1.0.0\n  description: Claude Code agent for OSSA manifest validation and generation\n\nspec:\n  role: |\n    You are an OSSA specification expert that helps users validate,\n    create, and migrate AI agent manifests to the OSSA standard.\n\n  llm:\n    provider: anthropic\n    model: claude-sonnet-4-5-20250929\n    temperature: 0.3\n    max_tokens: 8192\n\n  capabilities:\n    - name: validate\n      description: Validate OSSA manifests against schema\n      input_schema:\n        type: object\n        properties:\n          manifest_path:\n            type: string\n            description: Path to OSSA manifest file\n        required: [manifest_path]\n\n    - name: generate\n      description: Generate OSSA manifests from templates\n      input_schema:\n        type: object\n        properties:\n          agent_type:\n            type: string\n            enum: [chat, workflow, compliance, worker]\n          name:\n            type: string\n        required: [agent_type, name]\n\n    - name: migrate\n      description: Migrate manifests between OSSA versions\n      input_schema:\n        type: object\n        properties:\n          source_path:\n            type: string\n          target_version:\n            type: string\n            default: \"0.2.8\"\n        required: [source_path]\n\n  tools:\n    - name: ossa_validate\n      description: Run OSSA CLI validation\n      handler: bash\n      command: npm run cli -- validate ${manifest_path}\n\n    - name: ossa_generate\n      description: Generate new OSSA manifest\n      handler: bash\n      command: npm run cli -- generate ${agent_type} --name \"${name}\"\n\nextensions:\n  claude_code:\n    enabled: true\n    agent_type: specialist\n    capabilities:\n      - file_read\n      - file_write\n      - bash_execute\n    allowed_commands:\n      - \"npm run cli --\"\n      - \"npm run test\"\n      - \"npm run build\"\n    workspace_paths:\n      - \"spec/\"\n      - \"examples/\"\n      - \"src/\"\n",
    "category": "Getting Started"
  },
  {
    "name": "agent-router.ossa.yaml",
    "path": "common_npm/agent-router.ossa.yaml",
    "content": "ossaVersion: \"1.0\"\n\nagent:\n  id: agent-router\n  name: \"Agent Router\"\n  version: \"1.0.0\"\n  role: \"integration\"\n  \n  description: |\n    Multi-provider LLM gateway with circuit breaker, intelligent routing, and failover.\n    \n    Features:\n    - Multi-provider support (OpenAI, Anthropic, Google, etc.)\n    - Intelligent routing based on model capabilities\n    - Circuit breaker pattern for fault tolerance\n    - Request/response caching\n    - Cost optimization\n    - Load balancing\n  \n  runtime:\n    type: \"docker\"\n    image: \"llm-platform/agent-router:1.0.0\"\n    \n    resources:\n      cpu: \"500m\"\n      memory: \"1Gi\"\n    \n    health_check:\n      type: \"http\"\n      endpoint: \"/health\"\n      port: 4000\n  \n  capabilities:\n    - name: llm_completion\n      description: \"Generate LLM completion with intelligent provider routing\"\n      input_schema:\n        type: object\n        required: [prompt]\n        properties:\n          prompt:\n            type: string\n            description: \"Input prompt for LLM\"\n          model:\n            type: string\n            description: \"Preferred model (optional, router will auto-select)\"\n            examples: [\"gpt-4\", \"claude-3-opus\", \"gemini-pro\"]\n          temperature:\n            type: number\n            minimum: 0\n            maximum: 2\n            default: 0.7\n          max_tokens:\n            type: integer\n            minimum: 1\n            maximum: 100000\n            default: 2000\n          provider:\n            type: string\n            enum: [\"openai\", \"anthropic\", \"google\", \"auto\"]\n            default: \"auto\"\n      output_schema:\n        type: object\n        required: [response, provider, model, usage]\n        properties:\n          response:\n            type: string\n          provider:\n            type: string\n          model:\n            type: string\n          usage:\n            type: object\n            properties:\n              prompt_tokens:\n                type: integer\n              completion_tokens:\n                type: integer\n              total_tokens:\n                type: integer\n              cost_usd:\n                type: number\n      timeout_seconds: 30\n      retry_policy:\n        max_attempts: 3\n        backoff: \"exponential\"\n    \n    - name: embedding_generation\n      description: \"Generate text embeddings for vector storage\"\n      input_schema:\n        type: object\n        required: [text]\n        properties:\n          text:\n            type: string\n          model:\n            type: string\n            default: \"text-embedding-ada-002\"\n      output_schema:\n        type: object\n        required: [embedding, dimensions]\n        properties:\n          embedding:\n            type: array\n            items:\n              type: number\n          dimensions:\n            type: integer\n          model:\n            type: string\n    \n    - name: health_check\n      description: \"Check health of all LLM providers\"\n      input_schema:\n        type: object\n        properties: {}\n      output_schema:\n        type: object\n        required: [healthy, providers]\n        properties:\n          healthy:\n            type: boolean\n          providers:\n            type: array\n            items:\n              type: object\n              properties:\n                name:\n                  type: string\n                status:\n                  type: string\n                  enum: [\"healthy\", \"degraded\", \"down\"]\n                latency_ms:\n                  type: number\n  \n  llm:\n    provider: \"auto\"\n    fallback_providers: [\"openai\", \"anthropic\", \"google\"]\n    model: \"gpt-4\"\n    temperature: 0.7\n    maxTokens: 2000\n  \n  protocols:\n    - type: \"http\"\n      version: \"1.1\"\n      endpoint: \"http://agent-router:4000/v1/chat/completions\"\n    \n    - type: \"sse\"\n      version: \"1.0\"\n      endpoint: \"http://agent-router:4000/v1/stream\"\n\n  compliance:\n    frameworks: [\"SOC2\"]\n    dataClassification: \"confidential\"\n\nextensions:\n  common_npm:\n    package: \"@llm/agent-router\"\n    port: 4000\n    version: \"1.0.0\"\n    \n    dependencies:\n      - \"@llm/agent-protocol\"\n      - \"@llm/agent-tracer\"\n    \n    providers:\n      - openai\n      - anthropic\n      - google\n      - azure\n    \n    circuit_breaker:\n      enabled: true\n      failure_threshold: 5\n      timeout_seconds: 60\n      half_open_requests: 3\n    \n    monitoring:\n      metrics: true\n      tracing: true\n      opentelemetry:\n        endpoint: \"http://agent-tracer:4318\"\n        service_name: \"agent-router\"\n",
    "category": "Spec Examples & Templates"
  },
  {
    "name": "agent-router.v0.2.2.ossa.yaml",
    "path": "common_npm/agent-router.v0.2.2.ossa.yaml",
    "content": "apiVersion: ossa/v0.2.8\nkind: Agent\nmetadata:\n  name: agent-router\n  version: 1.0.0\n  description: |\n    Multi-provider LLM gateway with circuit breaker, intelligent routing, and failover.\n\n    Features:\n    - Multi-provider support (OpenAI, Anthropic, Google, etc.)\n    - Intelligent routing based on model capabilities\n    - Circuit breaker pattern for fault tolerance\n    - Request/response caching\n    - Cost optimization\n    - Load balancing\n  labels: {}\n  annotations:\n    ossa.io/migration: v1.0 to v0.2.2\n    ossa.io/migrated-date: 2025-10-31\nspec:\n  role: integration\n  taxonomy:\n    domain: integration\n    subdomain: general\n    capability: routing\n  llm:\n    provider: openai\n    model: gpt-4\n    temperature: 0.7\n    maxTokens: 2000\n  tools:\n    - type: mcp\n      name: llm_completion\n      server: agent-router\n    - type: mcp\n      name: embedding_generation\n      server: agent-router\n    - type: mcp\n      name: health_check\n      server: agent-router\n  extensions:\n    buildkit:\n      deployment:\n        replicas:\n          min: 1\n          max: 4\n      container:\n        image: llm-platform/agent-router:1.0.0\n        runtime: docker\n        resources: &a1\n          cpu: 500m\n          memory: 1Gi\n    runtime:\n      type: docker\n      image: llm-platform/agent-router:1.0.0\n      resources: *a1\n      health_check:\n        type: http\n        endpoint: /health\n        port: 4000\n",
    "category": "Spec Examples & Templates"
  },
  {
    "name": "compliance-agent.yml",
    "path": "compliance-agent.yml",
    "content": "ossaVersion: '1.0'\n\nagent:\n  id: fedramp-compliance-scanner\n  name: FedRAMP Compliance Scanner\n  description: |\n    Automated FedRAMP compliance scanning agent that validates\n    infrastructure against FedRAMP Moderate controls.\n  version: '1.0.0'\n  role: compliance\n\n  author:\n    name: Bluefly Security Team\n    email: security@bluefly.io\n    organization: Bluefly.io\n\n  license: Apache-2.0\n\n  runtime:\n    type: k8s\n    image: registry.bluefly.io/agents/fedramp-scanner:1.0.0\n    command: ['/app/scanner']\n    args: ['--mode=continuous']\n    env:\n      LOG_LEVEL: info\n      SCAN_INTERVAL: '3600'\n    resources:\n      cpu: '500m'\n      memory: '1Gi'\n\n  capabilities:\n    - name: scan_infrastructure\n      description: Scan Kubernetes infrastructure for FedRAMP compliance\n      input_schema:\n        type: object\n        required: [namespace]\n        properties:\n          namespace:\n            type: string\n          controls:\n            type: array\n            items:\n              type: string\n      output_schema:\n        type: object\n        properties:\n          compliant:\n            type: boolean\n          violations:\n            type: array\n            items:\n              type: object\n              properties:\n                control:\n                  type: string\n                severity:\n                  type: string\n                  enum: [low, medium, high, critical]\n                description:\n                  type: string\n      examples:\n        - name: scan_production\n          input:\n            namespace: production\n            controls: ['AC-2', 'AC-3', 'AU-2']\n          output:\n            compliant: false\n            violations:\n              - control: 'AC-2'\n                severity: 'high'\n                description: 'Service account without expiration policy'\n\n    - name: remediate\n      description: Automatically remediate compliance violations\n      input_schema:\n        type: object\n        required: [violation_id]\n        properties:\n          violation_id:\n            type: string\n          auto_apply:\n            type: boolean\n      output_schema:\n        type: object\n        properties:\n          remediated:\n            type: boolean\n          actions:\n            type: array\n            items:\n              type: string\n\n    - name: generate_report\n      description: Generate FedRAMP compliance report\n      input_schema:\n        type: object\n        required: [format]\n        properties:\n          format:\n            type: string\n            enum: [pdf, html, json]\n      output_schema:\n        type: object\n        properties:\n          report_url:\n            type: string\n          summary:\n            type: object\n\n  policies:\n    compliance:\n      - fedramp\n      - soc2\n    data_residency:\n      - US\n    encryption: true\n    audit: true\n    retention: 2555 # 7 years (FedRAMP requirement)\n\n  integration:\n    protocol: grpc\n    endpoints:\n      scan:\n        path: /v1/scan\n        method: POST\n      remediate:\n        path: /v1/remediate\n        method: POST\n      report:\n        path: /v1/report\n        method: GET\n    auth:\n      type: mutual-tls\n      config:\n        ca_cert: /etc/certs/ca.crt\n        client_cert: /etc/certs/client.crt\n\n  monitoring:\n    traces: true\n    metrics: true\n    logs: true\n    health_check: http://localhost:8080/health\n    readiness_check: http://localhost:8080/ready\n\n  metadata:\n    tags:\n      - compliance\n      - fedramp\n      - security\n    keywords:\n      - compliance scanning\n      - automated remediation\n      - audit reporting\n    homepage: https://github.com/bluefly/fedramp-scanner\n    documentation: https://docs.bluefly.io/agents/fedramp-scanner\n",
    "category": "Production"
  },
  {
    "name": "research-team.ossa.json",
    "path": "crewai/research-team.ossa.json",
    "content": "{\n  \"apiVersion\": \"ossa/v0.2.8\",\n  \"kind\": \"Agent\",\n  \"metadata\": {\n    \"name\": \"research-agent\",\n    \"version\": \"1.0.0\",\n    \"description\": \"CrewAI research agent for multi-agent collaboration\"\n  },\n  \"spec\": {\n    \"role\": \"You are a research specialist. Your goal is to gather comprehensive information on assigned topics.\",\n    \"llm\": {\n      \"provider\": \"openai\",\n      \"model\": \"gpt-4\",\n      \"temperature\": 0.3\n    },\n    \"tools\": [\n      {\n        \"type\": \"http\",\n        \"name\": \"web_search\",\n        \"endpoint\": \"https://api.search.example.com\",\n        \"capabilities\": [\n          {\n            \"name\": \"search\"\n          },\n          {\n            \"name\": \"retrieve\"\n          }\n        ]\n      }\n    ]\n  },\n  \"extensions\": {\n    \"crewai\": {\n      \"enabled\": true,\n      \"agent_type\": \"researcher\",\n      \"role\": \"Research Specialist\",\n      \"goal\": \"Gather comprehensive and accurate information on assigned research topics\",\n      \"backstory\": \"You are an expert researcher with years of experience in information gathering and analysis. You excel at finding reliable sources and synthesizing complex information.\",\n      \"tools\": [\n        \"web_search\",\n        \"document_analyzer\"\n      ]\n    }\n  }\n}",
    "category": "Framework Integration"
  },
  {
    "name": "code-review-agent.ossa.json",
    "path": "cursor/code-review-agent.ossa.json",
    "content": "{\n  \"apiVersion\": \"ossa/v0.2.8\",\n  \"kind\": \"Agent\",\n  \"metadata\": {\n    \"name\": \"code-review-agent\",\n    \"version\": \"1.0.0\",\n    \"description\": \"AI-powered code review agent for Cursor IDE\"\n  },\n  \"spec\": {\n    \"role\": \"You are an expert code reviewer. Analyze code for bugs, security issues, performance problems, and best practices. Provide constructive feedback.\",\n    \"llm\": {\n      \"provider\": \"openai\",\n      \"model\": \"gpt-4\",\n      \"temperature\": 0.2\n    },\n    \"tools\": [\n      {\n        \"type\": \"mcp\",\n        \"name\": \"analyze_code\",\n        \"server\": \"code-reviewer\",\n        \"capabilities\": [\n          {\n            \"name\": \"review\"\n          },\n          {\n            \"name\": \"suggest\"\n          },\n          {\n            \"name\": \"explain\"\n          }\n        ]\n      }\n    ]\n  },\n  \"extensions\": {\n    \"cursor\": {\n      \"enabled\": true,\n      \"agent_type\": \"composer\",\n      \"workspace_config\": {\n        \"rules_file\": \".cursor/.cursorrules\",\n        \"context_files\": [\n          \"src/**/*.ts\",\n          \"tests/**/*.ts\"\n        ],\n        \"ignore_patterns\": [\n          \"node_modules/**\",\n          \"dist/**\"\n        ]\n      },\n      \"capabilities\": {\n        \"code_generation\": false,\n        \"code_review\": true,\n        \"refactoring\": true,\n        \"testing\": false\n      },\n      \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-4\"\n      }\n    }\n  }\n}",
    "category": "Framework Integration"
  },
  {
    "name": "gitlab-ml-recommender.ossa.yaml",
    "path": "drupal/gitlab-ml-recommender.ossa.yaml",
    "content": "# ============================================================================\n# OSSA Drupal Integration Example: GitLab ML Recommendation Engine\n# ============================================================================\n#\n# PURPOSE:\n#   Demonstrates OSSA integration with Drupal's AI Agent Orchestra module.\n#   Showcases a real-world RAG (Retrieval-Augmented Generation) pipeline for\n#   customer success recommendations using semantic search and LLM generation.\n#\n# KEY FEATURES:\n#   - RAG pipeline with Qdrant vector database for semantic search\n#   - GPT-4 powered recommendation generation\n#   - TimescaleDB aggregation for customer health metrics\n#   - Agent-to-Agent (A2A) communication via JSON-RPC\n#   - Event-driven architecture with Redis pub/sub\n#   - Enterprise-grade monitoring, caching, and compliance\n#\n# ARCHITECTURE:\n#   Drupal Module (ai_agent_orchestra)\n#     ↓ (service call)\n#   GitLabMlRecommendationsService\n#     ↓ (semantic search)\n#   Qdrant Vector DB\n#     ↓ (context retrieval)\n#   GPT-4 via Agent Router\n#     → AI Recommendations\n#\n# USE CASE:\n#   Customer Success teams get AI-powered, data-driven recommendations for\n#   proactive customer engagement based on historical successful interventions\n#   and current customer health signals.\n#\n# RELATED DOCUMENTATION:\n#   - OSSA Drupal Extensions: spec/OSSA_Drupal_Extensions.md\n#   - RAG Pipeline Guide: docs/patterns/rag-patterns.md\n#   - A2A Protocol: spec/OSSA_A2A_Protocol.md\n# ============================================================================\n\n# OSSA specification version - defines which features are available\nossaVersion: \"1.0\"\n\n# Agent metadata and configuration\nagent:\n  # Unique identifier for this agent (must be DNS-compatible: lowercase, hyphens)\n  id: gitlab-ml-recommender\n\n  # Human-readable name displayed in UIs\n  name: \"GitLab ML Recommendation Engine\"\n\n  # Semantic version for agent deployment tracking\n  version: \"1.0.0\"\n\n  # Agent role determines lifecycle and communication patterns:\n  # - \"integration\": Long-running service that connects external systems\n  # - \"chat\": Interactive conversational agent\n  # - \"worker\": Task-based processor (fire-and-forget or queued)\n  # - \"orchestrator\": Coordinates multiple agents\n  role: \"integration\"\n\n  # Detailed agent description (supports Markdown for rich documentation)\n  description: |\n    AI-powered customer success recommendation agent using RAG (Retrieval-Augmented Generation).\n\n    **Pipeline Architecture:**\n    1. Semantic search in Qdrant for similar successful cases\n    2. GPT-4 generation with retrieved context\n    3. Priority ranking based on customer health scores\n\n    **Integrates with:**\n    - GitLabMlRecommendationsService (RAG generation)\n    - GitLabMlDashboardService (health metrics)\n    - QdrantVectorService (semantic search)\n\n  # Runtime configuration - defines how the agent is deployed and executed\n  runtime:\n    # Execution environment type:\n    # - \"docker\": Containerized deployment with image specification\n    # - \"serverless\": FaaS deployment (AWS Lambda, Google Cloud Functions, etc.)\n    # - \"kubernetes\": Native k8s deployment with CRD\n    # - \"local\": Direct process execution (dev/testing only)\n    type: \"docker\"\n\n    # Docker image reference (registry/name:tag)\n    # Should follow semantic versioning and be immutable (no \"latest\" tag)\n    image: \"llm-platform/ml-recommender:1.0.0\"\n\n    # Language and dependency requirements (enforced at build time)\n    requirements:\n      # PHP version constraint (Drupal requirement)\n      php: \">=8.1\"\n\n      # Node.js version for build tools and frontend assets\n      node: \">=20.0.0\"\n\n      # Composer packages required for Drupal integration\n      packages:\n        - \"guzzlehttp/guzzle\"      # HTTP client for external API calls\n        - \"symfony/http-client\"     # Alternative HTTP client with retry logic\n\n    # Resource limits (Kubernetes-style requests/limits)\n    # These prevent resource exhaustion and enable autoscaling decisions\n    resources:\n      cpu: \"1000m\"      # 1 CPU core (1000 millicores)\n      memory: \"2Gi\"     # 2 gigabytes RAM\n\n    # Health check configuration for container orchestration\n    # Used by Kubernetes liveness/readiness probes and load balancers\n    health_check:\n      type: \"http\"          # HTTP GET request for health status\n      endpoint: \"/health\"   # Health check endpoint path\n      port: 8080            # Container port to check\n\n  # Capabilities define what the agent can do (contract-first design)\n  # Each capability is a discrete, testable unit of functionality with:\n  # - Type-safe input/output schemas (JSON Schema)\n  # - Usage examples for documentation and testing\n  # - Timeout and retry policies for reliability\n  capabilities:\n    # ========================================================================\n    # Capability 1: Generate Recommendations (RAG Pipeline)\n    # ========================================================================\n    # This is the core capability demonstrating OSSA's RAG pattern:\n    # 1. Semantic search in Qdrant for similar successful customer cases\n    # 2. Context injection into GPT-4 prompt\n    # 3. AI-generated recommendations with rationale\n    # 4. Priority ranking based on customer health signals\n    #\n    # FLOW:\n    #   Drupal Request → Service Layer → Qdrant Search → GPT-4 Generation\n    #   → Priority Ranking → Structured Response\n    # ========================================================================\n    - name: generate_recommendations\n\n      # Capability description (appears in API docs and UI)\n      description: \"Generate AI recommendations using RAG pipeline (Qdrant semantic search + GPT-4)\"\n\n      # Input schema using JSON Schema (OpenAPI-compatible)\n      # This defines the contract for calling this capability\n      input_schema:\n        type: object\n\n        # Required fields - validation will fail if missing\n        required: [customer_id]\n\n        properties:\n          # Customer UUID from GitLab (primary identifier)\n          customer_id:\n            type: string\n            format: uuid  # Validates UUID format (RFC 4122)\n            description: \"Customer UUID from GitLab\"\n\n          # Context filter for targeted recommendations\n          # Different contexts use different embedding collections\n          context:\n            type: string\n            enum: [health, churn, engagement, technical]\n            description: \"Recommendation context filter\"\n\n          # Result limit for controlling response size and cost\n          limit:\n            type: integer\n            default: 10       # Default if not provided\n            minimum: 1        # At least 1 recommendation\n            maximum: 50       # Max to prevent token exhaustion\n            description: \"Maximum number of recommendations\"\n\n\n      # Output schema defines the structure of the response\n      # This ensures consistent data contracts across all agents\n      output_schema:\n        type: object\n\n        # Required response fields (must always be present)\n        required: [customerId, recommendations, generatedAt]\n\n        properties:\n          # Echo back the customer ID for request correlation\n          customerId:\n            type: string\n            format: uuid\n\n          # Array of AI-generated recommendations\n          recommendations:\n            type: array\n            items:\n              type: object\n\n              # Each recommendation must have these core fields\n              required: [id, title, priority, category]\n\n              properties:\n                # Unique recommendation ID for tracking and analytics\n                id:\n                  type: string\n                  format: uuid\n\n                # Short, actionable title (shown in UI cards)\n                title:\n                  type: string\n                  maxLength: 255  # Database column constraint\n\n                # Detailed explanation of the recommendation\n                description:\n                  type: string\n\n                # Priority level for sorting and alerting\n                priority:\n                  type: string\n                  enum: [critical, high, medium, low]\n\n                # Recommendation category for filtering and routing\n                category:\n                  type: string\n                  enum: [engagement, technical, health, success]\n\n                # Concrete action steps (Drupal renders as checklist)\n                actionItems:\n                  type: array\n                  items:\n                    type: string\n\n                # AI-generated rationale explaining \"why\" this recommendation\n                # This is the key value from RAG - evidence from similar cases\n                rationale:\n                  type: string\n\n                # References to similar customer cases from vector search\n                # Provides transparency and builds trust in AI recommendations\n                similarCases:\n                  type: array\n                  items:\n                    type: object\n\n          # ISO 8601 timestamp for audit trail\n          generatedAt:\n            type: string\n            format: date-time\n\n\n      # Examples for documentation, testing, and contract validation\n      # Used by OSSA validators and API documentation generators\n      examples:\n        - name: \"Health context recommendations\"\n          input:\n            customer_id: \"123e4567-e89b-12d3-a456-426614174000\"\n            context: \"health\"\n            limit: 5\n          output:\n            customerId: \"123e4567-e89b-12d3-a456-426614174000\"\n            recommendations:\n              - id: \"rec-001\"\n                title: \"Schedule health check meeting\"\n                description: \"Conduct comprehensive health review with technical team\"\n                priority: \"high\"\n                category: \"engagement\"\n                actionItems:\n                  - \"Send meeting invitation for next week\"\n                  - \"Prepare health check questionnaire\"\n                  - \"Review usage metrics before meeting\"\n                rationale: \"Early issue identification strengthens customer relationships\"\n            generatedAt: \"2025-10-24T12:00:00Z\"\n\n      # Timeout prevents hung requests and enables circuit breaker patterns\n      timeout_seconds: 30\n\n      # Retry policy for transient failures (network issues, rate limits)\n      retry_policy:\n        max_attempts: 3           # Total attempts (initial + 2 retries)\n        backoff: \"exponential\"    # 1s, 2s, 4s, 8s... (prevents thundering herd)\n\n    # ========================================================================\n    # Capability 2: Get Dashboard Overview\n    # ========================================================================\n    # Provides aggregated customer health metrics from TimescaleDB.\n    # This is a read-only, fast query capability for dashboard UIs.\n    # ========================================================================\n    - name: get_dashboard_overview\n\n      description: \"Retrieve customer health dashboard data aggregated from TimescaleDB\"\n\n\n      # Minimal input - just a time range selector\n      input_schema:\n        type: object\n        required: [time_range]\n        properties:\n          # Time range for metric aggregation (TimescaleDB time bucket)\n          time_range:\n            type: string\n            enum: [\"24h\", \"7d\", \"30d\", \"90d\"]\n            description: \"Time range for dashboard data\"\n\n      # Dashboard metrics output (all pre-aggregated in TimescaleDB)\n      output_schema:\n        type: object\n        required: [totalCustomers, healthDistribution, churnRisks]\n        properties:\n          totalCustomers:\n            type: integer\n          healthDistribution:\n            type: object\n            properties:\n              healthy:\n                type: integer\n              warning:\n                type: integer\n              critical:\n                type: integer\n          churnRisks:\n            type: object\n            properties:\n              low:\n                type: integer\n              medium:\n                type: integer\n              high:\n                type: integer\n              critical:\n                type: integer\n          activeAlerts:\n            type: integer\n          recommendations:\n            type: integer\n          trends:\n            type: object\n            properties:\n              healthChange:\n                type: number\n              churnChange:\n                type: number\n              engagementChange:\n                type: number\n\n\n      # Fast timeout for dashboard queries (should be cached)\n      timeout_seconds: 10\n\n    # ========================================================================\n    # Capability 3: Get Active Alerts\n    # ========================================================================\n    # Returns active health alerts for customer success team triage.\n    # Supports filtering by severity and status for alert management workflows.\n    # ========================================================================\n    - name: get_active_alerts\n\n      description: \"Retrieve active customer health alerts\"\n      \n      input_schema:\n        type: object\n        properties:\n          severity:\n            type: string\n            enum: [critical, high, medium, low]\n          status:\n            type: string\n            enum: [active, acknowledged, resolved]\n            default: \"active\"\n      \n      output_schema:\n        type: object\n        required: [alerts, total]\n        properties:\n          alerts:\n            type: array\n            items:\n              type: object\n              properties:\n                id:\n                  type: string\n                customerId:\n                  type: string\n                customerName:\n                  type: string\n                severity:\n                  type: string\n                type:\n                  type: string\n                message:\n                  type: string\n                status:\n                  type: string\n                createdAt:\n                  type: string\n                  format: date-time\n          total:\n            type: integer\n\n  # ========================================================================\n  # LLM Configuration\n  # ========================================================================\n  # Defines the language model used for generation tasks.\n  # For RAG pipelines, choose models with good reasoning and context handling.\n  # ========================================================================\n  llm:\n    # Provider name (openai, anthropic, azure-openai, etc.)\n    provider: \"openai\"\n\n    # Model identifier - GPT-4 for high-quality reasoning over retrieved context\n    model: \"gpt-4\"\n\n    # Temperature controls randomness (0.0=deterministic, 1.0=creative)\n    # 0.7 balances consistency with natural variation\n    temperature: 0.7\n\n    # Maximum tokens per generation (controls cost and latency)\n    maxTokens: 2000\n\n  # ========================================================================\n  # Tools Configuration (MCP and HTTP endpoints)\n  # ========================================================================\n  # Defines external tools the agent can invoke during execution.\n  # MCP (Model Context Protocol) provides standardized tool interfaces.\n  # ========================================================================\n  tools:\n    # Qdrant vector database for semantic search (RAG retrieval step)\n    - type: \"mcp\"\n      server: \"qdrant-mcp\"              # MCP server name (from config)\n      namespace: \"default\"               # Kubernetes namespace\n      capabilities:\n        - semantic_search                # Find similar customer cases\n        - vector_retrieval               # Fetch embeddings by ID\n        - get_point                      # Get specific vector point\n        - search_points                  # Batch vector search\n\n    # Agent Router for LLM and embedding generation\n    - type: \"http\"\n      server: \"agent-router\"\n      endpoint: \"http://agent-router:4000\"\n      capabilities:\n        - llm_generation                 # GPT-4 text generation\n        - embedding_generation           # text-embedding-ada-002\n\n  # ========================================================================\n  # Communication Protocols\n  # ========================================================================\n  # Defines how external systems can interact with this agent.\n  # Multiple protocols enable different integration patterns.\n  # ========================================================================\n  protocols:\n    # RESTful HTTP API for traditional request/response (most common)\n    - type: \"http\"\n      version: \"1.1\"\n      endpoint: \"/api/v1/recommendations\"\n\n    # Server-Sent Events for real-time streaming (dashboards, live updates)\n    - type: \"sse\"\n      version: \"1.0\"\n      endpoint: \"/api/v1/stream\"\n\n    # JSON-RPC for agent-to-agent communication (A2A protocol)\n    - type: \"json-rpc\"\n      version: \"2.0\"\n      endpoint: \"/api/v1/rpc\"\n\n  # ========================================================================\n  # Compliance and Governance\n  # ========================================================================\n  # Defines regulatory frameworks and data handling requirements.\n  # Used for audit trails, data retention, and access control.\n  # ========================================================================\n  compliance:\n    # Regulatory frameworks this agent complies with\n    frameworks: [\"SOC2\", \"HIPAA\"]\n\n    # Data sensitivity level (public, internal, confidential, restricted)\n    dataClassification: \"confidential\"\n\n    # Data retention policy for audit and compliance\n    retentionPolicy: \"7years\"\n\n# ============================================================================\n# OSSA Drupal Extensions\n# ============================================================================\n# Drupal-specific configuration for integration with ai_agent_orchestra module.\n# This section is OSSA's extension mechanism for framework-specific features.\n# ============================================================================\nextensions:\n  drupal:\n    # Drupal module that implements this agent\n    module: \"ai_agent_orchestra\"\n\n    # Drupal service ID for dependency injection\n    # Registered in ai_agent_orchestra.services.yml\n    service: \"ai_agent_orchestra.gitlab_ml_recommendations\"\n\n    # Drupal module dependencies (must be enabled)\n    dependencies:\n      - \"ai_agents\"                  # Core AI agent framework\n      - \"ai_provider_langchain\"      # LangChain integration\n      - \"ai_provider_openai\"         # OpenAI provider\n\n    # Database tables created by this agent's schema\n    database:\n      tables:\n        - \"gitlab_ml_metrics\"           # Customer health metrics (TimescaleDB)\n        - \"gitlab_ml_alerts\"            # Active health alerts\n        - \"gitlab_ml_recommendations\"   # Generated recommendations\n      schema_version: \"1.0.0\"           # For schema migrations\n\n    # ======================================================================\n    # RAG Pipeline Configuration\n    # ======================================================================\n    # Retrieval-Augmented Generation settings for semantic search and LLM.\n    # This is the core of the recommendation engine.\n    # ======================================================================\n    rag_pipeline:\n      vector_db: \"qdrant\"                              # Vector database type\n      collection: \"gitlab_customer_embeddings\"         # Qdrant collection name\n      embedding_model: \"text-embedding-ada-002\"        # OpenAI embedding model\n      similarity_limit: 5                              # Top-K results from search\n      similarity_threshold: 0.7                        # Minimum cosine similarity\n      llm_service: \"http://agent-router:4000\"          # LLM generation endpoint\n\n    # ======================================================================\n    # Observability Configuration (OpenTelemetry)\n    # ======================================================================\n    # Distributed tracing, metrics, and structured logging for production.\n    # ======================================================================\n    monitoring:\n      metrics: true           # Prometheus metrics\n      tracing: true           # OpenTelemetry traces\n      logging: true           # Structured JSON logs\n      opentelemetry:\n        endpoint: \"http://agent-tracer:4318\"     # OTLP collector endpoint\n        service_name: \"gitlab-ml-recommender\"    # Service identifier in traces\n        headers:\n          \"x-service-version\": \"1.0.0\"           # Custom trace metadata\n\n    # ======================================================================\n    # Drupal Cache Integration\n    # ======================================================================\n    # Caches recommendation results to reduce LLM costs and latency.\n    # ======================================================================\n    caching:\n      enabled: true\n      backend: \"redis\"        # Drupal cache backend (redis, database, memcache)\n      ttl: 3600               # Cache TTL in seconds (1 hour)\n      tags:                   # Cache tags for invalidation\n        - \"gitlab_ml\"\n        - \"recommendations\"\n\n    # Drupal permissions required to use this agent\n    permissions:\n      - \"administer ai agents\"       # Full agent management\n      - \"execute ai agents\"           # Execute agent capabilities\n      - \"view ai agent results\"       # View recommendation history\n\n    # ======================================================================\n    # Agent-to-Agent (A2A) Communication\n    # ======================================================================\n    # Enables this agent to call other agents for compliance checks,\n    # cost optimization, and orchestration workflows.\n    # ======================================================================\n    a2a_config:\n      enabled: true\n      protocol: \"json-rpc\"              # JSON-RPC 2.0 for A2A calls\n      endpoints:\n        # Compliance validator ensures recommendations meet policies\n        - \"http://compliance-validator:8080/a2a\"\n        # Cost optimizer prevents budget overruns\n        - \"http://cost-optimizer:8080/a2a\"\n      authentication:\n        type: \"bearer\"                  # Bearer token authentication\n        secretRef:                      # Kubernetes secret reference\n          name: \"drupal-a2a-credentials\"\n          key: \"bearer-token\"\n\n    # ======================================================================\n    # Event Bus Integration (Redis Pub/Sub)\n    # ======================================================================\n    # Publishes events for real-time dashboards and workflow automation.\n    # ======================================================================\n    event_bus:\n      enabled: true\n      # Event topics this agent publishes to\n      topics:\n        - \"customer.health.changed\"      # Health score updates\n        - \"recommendations.generated\"    # New recommendations available\n        - \"alerts.created\"               # New health alerts\n        - \"metrics.updated\"              # Metric aggregation complete\n      redis:\n        host: \"redis://buildkit-redis:16379\"\n        db: 0\n\n# ============================================================================\n# End of OSSA Drupal Integration Example\n# ============================================================================\n#\n# VALIDATION:\n#   ossa validate examples/drupal/gitlab-ml-recommender.ossa.yaml\n#\n# DEPLOYMENT:\n#   1. Enable required Drupal modules: drush en ai_agent_orchestra\n#   2. Import agent manifest: drush ossa:import gitlab-ml-recommender.ossa.yaml\n#   3. Clear Drupal cache: drush cr\n#   4. Test capabilities: drush ossa:test gitlab-ml-recommender\n#\n# INTEGRATION POINTS:\n#   - Drupal Service: ai_agent_orchestra.gitlab_ml_recommendations\n#   - Database Tables: gitlab_ml_* (see schema in module)\n#   - Cache Tags: gitlab_ml, recommendations\n#   - Permissions: administer/execute/view ai agents\n#   - Events: customer.health.changed, recommendations.generated\n#\n# RELATED EXAMPLES:\n#   - examples/bridges/aiflow-bridge-example.yml (social agent bridge)\n#   - examples/openapi-extensions/worker-agent-api.openapi.yml (OpenAPI)\n#   - spec/OSSA_Drupal_Extensions.md (full Drupal spec)\n# ============================================================================\n",
    "category": "Spec Examples & Templates"
  },
  {
    "name": "gitlab-ml-recommender.v0.2.2.ossa.yaml",
    "path": "drupal/gitlab-ml-recommender.v0.2.2.ossa.yaml",
    "content": "apiVersion: ossa/v0.2.8\nkind: Agent\nmetadata:\n  name: gitlab-ml-recommender\n  version: 1.0.0\n  description: |\n    AI-powered customer success recommendation agent using RAG (Retrieval-Augmented Generation).\n\n    Pipeline:\n    1. Semantic search in Qdrant for similar successful cases\n    2. GPT-4 generation with context\n    3. Priority ranking based on customer health\n\n    Integrates with:\n    - GitLabMlRecommendationsService (RAG generation)\n    - GitLabMlDashboardService (health metrics)\n    - QdrantVectorService (semantic search)\n  labels: {}\n  annotations:\n    ossa.io/migration: v1.0 to v0.2.2\n    ossa.io/migrated-date: 2025-10-31\nspec:\n  role: integration\n  taxonomy:\n    domain: data\n    subdomain: general\n    capability: general\n  llm:\n    provider: openai\n    model: gpt-4\n    temperature: 0.7\n    maxTokens: 2000\n  tools:\n    - type: mcp\n      name: generate_recommendations\n      server: gitlab-ml-recommender\n    - type: mcp\n      name: get_dashboard_overview\n      server: gitlab-ml-recommender\n    - type: mcp\n      name: get_active_alerts\n      server: gitlab-ml-recommender\n  extensions:\n    buildkit:\n      deployment:\n        replicas:\n          min: 1\n          max: 4\n      container:\n        image: llm-platform/ml-recommender:1.0.0\n        runtime: docker\n        resources: &a1\n          cpu: 1000m\n          memory: 2Gi\n    runtime:\n      type: docker\n      image: llm-platform/ml-recommender:1.0.0\n      requirements:\n        php: \">=8.1\"\n        node: \">=20.0.0\"\n        packages:\n          - guzzlehttp/guzzle\n          - symfony/http-client\n      resources: *a1\n      health_check:\n        type: http\n        endpoint: /health\n        port: 8080\n",
    "category": "Spec Examples & Templates"
  },
  {
    "name": "agent.yml",
    "path": "enterprise/agent.yml",
    "content": "# Enterprise OSSA Agent Example (400+ lines)\n# Platinum conformance level - Enterprise-grade with all features\n\nossa: 1.0\nagent:\n  name: enterprise-ai-orchestrator\n  version: 3.5.0\n  description: Enterprise-grade AI orchestration platform with multi-model support\n  author: Enterprise AI Division\n  license: Commercial\n  conformance: platinum\n  tags:\n    - enterprise\n    - orchestration\n    - multi-model\n    - secure\n    - scalable\n\ndiscover:\n  auto: true\n  protocol: uadp\n  registry: https://registry.ossa.ai\n  endpoints:\n    - https://discovery.ossa.ai/agents\n    - https://enterprise.ossa.ai/discovery\n\ncapabilities:\n  # Core AI Capabilities\n  - text_generation\n  - text_analysis\n  - code_generation\n  - code_review\n  - image_generation\n  - image_analysis\n  - speech_to_text\n  - text_to_speech\n  - translation\n  - summarization\n  # Orchestration Capabilities\n  - workflow_orchestration\n  - agent_coordination\n  - task_distribution\n  - load_balancing\n  # Enterprise Features\n  - audit_logging\n  - compliance_checking\n  - data_governance\n  - security_scanning\n\napi:\n  POST /orchestrate:\n    capability: workflow_orchestration\n    description: Orchestrate complex AI workflows\n\n  POST /generate/text:\n    capability: text_generation\n    description: Generate text using multiple models\n\n  POST /generate/code:\n    capability: code_generation\n    description: Generate code with review\n\n  POST /analyze/document:\n    capability: text_analysis\n    description: Deep document analysis\n\n  POST /analyze/image:\n    capability: image_analysis\n    description: Computer vision analysis\n\n  POST /coordinate:\n    capability: agent_coordination\n    description: Coordinate multiple agents\n\n  GET /health:\n    description: Comprehensive health status\n\n  GET /metrics:\n    description: Detailed Prometheus metrics\n\n  GET /ready:\n    description: Readiness probe\n\n  GET /live:\n    description: Liveness probe\n\nmonitoring:\n  io_aware: true\n\n  logs:\n    format: jsonl\n    level: info\n    trace: true\n    retention: 90d\n    outputs:\n      - type: elasticsearch\n        config:\n          url: https://logs.enterprise.com\n      - type: file\n        config:\n          path: /var/log/ossa/orchestrator.log\n          rotation: hourly\n          max_files: 168\n    sampling:\n      enabled: true\n      rate: 1.0\n      rules:\n        - level: debug\n          rate: 0.1\n        - level: error\n          rate: 1.0\n\n  metrics:\n    enabled: true\n    endpoint: /metrics\n    format: prometheus\n    interval: 30\n    exporters:\n      - type: prometheus\n        endpoint: http://prometheus:9090\n      - type: datadog\n        endpoint: https://api.datadoghq.com\n      - type: cloudwatch\n        config:\n          region: us-east-1\n    custom_metrics:\n      - name: workflow_executions_total\n        type: counter\n        labels: [workflow_type, status, tenant]\n      - name: model_latency_seconds\n        type: histogram\n        labels: [model, operation]\n        buckets: [0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]\n      - name: active_agents\n        type: gauge\n        labels: [agent_type]\n      - name: token_usage_total\n        type: counter\n        labels: [model, tenant, operation]\n\n  traces:\n    enabled: true\n    format: otlp\n    endpoint: https://traces.enterprise.com:4317\n    sampling:\n      type: adaptive\n      rate: 0.1\n      max_per_second: 1000\n    propagation: [tracecontext, baggage, b3]\n    io_capture:\n      enabled: true\n      max_input_size: 10MB\n      max_output_size: 10MB\n      truncate: true\n\n  health:\n    enabled: true\n    endpoint: /health\n    interval: 15\n    checks:\n      - name: postgres_primary\n        type: tcp\n        config:\n          port: 5432\n          timeout: 5\n        critical: true\n      - name: redis_cache\n        type: tcp\n        config:\n          port: 6379\n          timeout: 5\n      - name: elasticsearch\n        type: http\n        config:\n          url: http://elasticsearch:9200/_cluster/health\n          timeout: 10\n      - name: model_servers\n        type: grpc\n        config:\n          port: 50051\n          timeout: 5\n        critical: true\n\n  alerts:\n    enabled: true\n    channels:\n      - name: pagerduty\n        type: pagerduty\n        config:\n          integration_key: \"${PAGERDUTY_KEY}\"\n      - name: slack\n        type: slack\n        config:\n          webhook_url: \"${SLACK_WEBHOOK}\"\n      - name: email\n        type: email\n        config:\n          smtp_host: smtp.enterprise.com\n          from: alerts@enterprise.com\n    rules:\n      - name: high_error_rate\n        condition: \"error_rate > 0.05\"\n        severity: critical\n        channels: [pagerduty, slack]\n      - name: high_latency\n        condition: \"p95_latency > 5s\"\n        severity: high\n        channels: [slack]\n        throttle: 300\n\n  redaction:\n    enabled: true\n    patterns:\n      - pattern: \"\\\\b[A-Z]{2}\\\\d{2}\\\\s?\\\\d{4}\\\\s?\\\\d{4}\\\\s?\\\\d{4}\\\\s?\\\\d{4}\\\\b\"\n        name: credit_card\n        replacement: \"[CC-REDACTED]\"\n      - pattern: \"\\\\b\\\\d{3}-\\\\d{2}-\\\\d{4}\\\\b\"\n        name: ssn\n        replacement: \"[SSN-REDACTED]\"\n      - pattern: \"\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b\"\n        name: email\n        replacement: \"[EMAIL-REDACTED]\"\n      - pattern: \"Bearer\\\\s+[A-Za-z0-9\\\\-._~\\\\+/]+={0,2}\"\n        name: bearer_token\n        replacement: \"[TOKEN-REDACTED]\"\n\n  retention:\n    logs:\n      duration: 90d\n      archive:\n        enabled: true\n        location: s3://enterprise-logs-archive\n        after: 30d\n    metrics:\n      duration: 365d\n      archive:\n        enabled: true\n        location: s3://enterprise-metrics-archive\n        after: 90d\n    traces:\n      duration: 30d\n\nperformance:\n  token_optimization:\n    enabled: true\n    strategies:\n      - type: flash_attention\n        config:\n          block_size: 512\n      - type: paged_kv\n        config:\n          page_size: 4096\n      - type: grouped_query\n        config:\n          num_groups: 8\n    max_context: 128000\n    compression:\n      enabled: true\n      method: zstd\n      level: 3\n\n  cache:\n    enabled: true\n    layers:\n      - name: l1_memory\n        type: memory\n        size: 1GB\n        ttl: 60\n      - name: l2_redis\n        type: redis\n        ttl: 3600\n        config:\n          host: redis-primary\n          port: 6379\n          password: \"${REDIS_PASSWORD}\"\n      - name: l3_disk\n        type: disk\n        size: 100GB\n        config:\n          path: /var/cache/ossa\n    key_strategy: hybrid\n    eviction:\n      policy: lru\n      max_entries: 100000\n\n  quantization:\n    enabled: true\n    method: awq\n    target_bits: 4\n    calibration:\n      dataset: enterprise-calibration-v2\n      samples: 512\n      method: percentile\n\n  batching:\n    enabled: true\n    dynamic: true\n    max_batch_size: 128\n    timeout_ms: 50\n    padding: bucket\n    buckets: [128, 256, 512, 1024, 2048, 4096]\n\n  resources:\n    limits:\n      cpu: \"16\"\n      memory: 64Gi\n      disk: 500Gi\n    requests:\n      cpu: \"8\"\n      memory: 32Gi\n    gpu:\n      required: true\n      type: nvidia\n      model: A100\n      count: 4\n      memory: 80GB\n      compute_capability: \"8.0\"\n    network:\n      bandwidth: 10Gbps\n      timeout: 60000\n      retry:\n        enabled: true\n        max_attempts: 5\n        backoff: exponential\n        initial_delay: 1000\n        max_delay: 30000\n\n  scaling:\n    enabled: true\n    type: both\n    horizontal:\n      min_replicas: 3\n      max_replicas: 100\n      target_cpu: 70\n      target_memory: 80\n      metrics:\n        - name: requests_per_second\n          target: 1000\n          type: average\n        - name: pending_workflows\n          target: 100\n          type: average\n    vertical:\n      enabled: true\n      update_mode: auto\n      cpu:\n        min: \"4\"\n        max: \"32\"\n      memory:\n        min: 16Gi\n        max: 128Gi\n\n  optimization:\n    compiler:\n      enabled: true\n      backend: tensorrt\n      mode: max_performance\n    runtime:\n      torch_dynamo: true\n      cuda_graphs: true\n      amp:\n        enabled: true\n        dtype: float16\n      fusion: true\n    model:\n      pruning:\n        enabled: true\n        sparsity: 0.5\n      distillation:\n        enabled: false\n\nbridge:\n  mcp:\n    enabled: true\n    server_type: websocket\n    tools:\n      - name: orchestrateWorkflow\n        description: Orchestrate AI workflow\n        capability: workflow_orchestration\n      - name: coordinateAgents\n        description: Coordinate multiple agents\n        capability: agent_coordination\n    resources:\n      - uri: ossa://workflows\n        name: Available Workflows\n        mimeType: application/json\n    config:\n      max_message_size: 10485760\n      timeout_ms: 30000\n      retry_count: 3\n\n  a2a:\n    enabled: true\n    card_url: https://enterprise.ai/agents/orchestrator.json\n    schema_version: \"1.0\"\n    capabilities_mapping:\n      workflow_orchestration: orchestrate\n      agent_coordination: coordinate\n\n  openapi:\n    enabled: true\n    spec_url: ./openapi.yaml\n    spec_version: \"3.1\"\n    servers:\n      - url: https://api.enterprise.ai\n        description: Production API\n      - url: https://staging.enterprise.ai\n        description: Staging API\n\n  langchain:\n    enabled: true\n    tool_class: EnterpriseOrchestrator\n    chain_type: agent\n    memory:\n      type: vector\n      max_tokens: 128000\n    callbacks:\n      - EnterpriseLogger\n      - ComplianceAuditor\n    export:\n      as_tool: true\n      as_chain: true\n      as_agent: true\n\n  crewai:\n    enabled: true\n    agent_type: manager\n    role: Enterprise AI Orchestrator\n    goal: Efficiently orchestrate complex AI workflows\n    backstory: Advanced orchestration system managing enterprise AI operations\n    max_iter: 50\n    allow_delegation: true\n\n  autogen:\n    enabled: true\n    agent_type: assistant\n    system_message: Enterprise AI orchestration and coordination\n    human_input_mode: NEVER\n    code_execution:\n      enabled: true\n      work_dir: /tmp/autogen\n      use_docker: true\n    max_consecutive_auto_reply: 25\n\n  custom:\n    drupal:\n      enabled: true\n      modules: [mcp, experience_builder, eca]\n      api_version: \"11.x\"\n    gitlab:\n      enabled: true\n      ci_component: gitlab.com/blueflyio/agent-platform/gitlab_components/golden@v0.1.0",
    "category": "Production"
  },
  {
    "name": "drupal-v1.yml",
    "path": "extensions/drupal-v1.yml",
    "content": "# OSSA Drupal Extension Schema\n# Extension specification for Drupal LLM Platform deployment\n# Compatible with Drupal 10+ and llm-platform modules\n\napiVersion: ossa/v0.2.8\nkind: ExtensionSchema\nmetadata:\n  name: drupal-extension\n  version: v1\n  namespace: ossa/extensions\n  \nspec:\n  platform: drupal\n  compatibleWith:\n    - \"drupal/11\"\n    - \"drupal/10\"\n  \n  description: \"Extension schema for Drupal LLM Platform agent deployment and orchestration\"\n\n  additionalFields:\n    module:\n      type: string\n      required: true\n      pattern: \"^[a-z][a-z0-9_]*$\"\n      description: \"Drupal module that owns this agent\"\n      examples:\n        - \"ai_agents\"\n        - \"ai_agent_orchestra\"\n        - \"ai_agent_marketplace\"\n\n    service:\n      type: string\n      required: true\n      pattern: \"^[a-z][a-z0-9_]*\\\\.[a-z][a-z0-9_]*$\"\n      description: \"Drupal service ID for agent execution (module.service format)\"\n      examples:\n        - \"ai_agents.executor\"\n        - \"ai_agent_orchestra.gitlab_ml_recommendations\"\n        - \"ai_agent_marketplace.discovery\"\n\n    dependencies:\n      type: array\n      items:\n        type: string\n        pattern: \"^[a-z][a-z0-9_]*$\"\n      description: \"Required Drupal modules (must be installed)\"\n      examples:\n        - [\"ai_agents\", \"ai_provider_langchain\", \"ai_provider_openai\"]\n\n    database:\n      type: object\n      properties:\n        tables:\n          type: array\n          items:\n            type: string\n          description: \"Required database tables (without prefix)\"\n          examples:\n            - [\"gitlab_ml_metrics\", \"gitlab_ml_alerts\"]\n        migrations:\n          type: array\n          items:\n            type: string\n          description: \"Required migration IDs\"\n        schema_version:\n          type: string\n          description: \"Database schema version\"\n\n    rag_pipeline:\n      type: object\n      description: \"RAG (Retrieval-Augmented Generation) pipeline configuration\"\n      properties:\n        vector_db:\n          type: string\n          enum: [\"qdrant\", \"weaviate\", \"pinecone\"]\n          default: \"qdrant\"\n          description: \"Vector database for semantic search\"\n        collection:\n          type: string\n          description: \"Vector collection name\"\n        embedding_model:\n          type: string\n          default: \"text-embedding-ada-002\"\n          description: \"Embedding model for vector generation\"\n        similarity_limit:\n          type: integer\n          default: 5\n          minimum: 1\n          maximum: 100\n          description: \"Number of similar documents to retrieve\"\n        similarity_threshold:\n          type: number\n          default: 0.7\n          minimum: 0\n          maximum: 1\n          description: \"Minimum similarity score (0-1)\"\n        llm_service:\n          type: string\n          format: uri\n          description: \"LLM service endpoint for generation\"\n          examples:\n            - \"http://agent-router:4000\"\n            - \"http://localhost:3007\"\n\n    monitoring:\n      type: object\n      description: \"Observability and monitoring configuration\"\n      properties:\n        metrics:\n          type: boolean\n          default: true\n          description: \"Enable Prometheus metrics\"\n        tracing:\n          type: boolean\n          default: true\n          description: \"Enable OpenTelemetry tracing\"\n        logging:\n          type: boolean\n          default: true\n          description: \"Enable structured logging\"\n        opentelemetry:\n          type: object\n          properties:\n            endpoint:\n              type: string\n              format: uri\n              description: \"OpenTelemetry collector endpoint\"\n              examples:\n                - \"http://agent-tracer:4318\"\n            service_name:\n              type: string\n              description: \"Service name for tracing\"\n            headers:\n              type: object\n              additionalProperties:\n                type: string\n              description: \"Additional headers for OTLP export\"\n\n    caching:\n      type: object\n      description: \"Drupal caching configuration for agent results\"\n      properties:\n        enabled:\n          type: boolean\n          default: true\n        backend:\n          type: string\n          enum: [\"redis\", \"database\", \"memory\"]\n          default: \"redis\"\n        ttl:\n          type: integer\n          description: \"Cache TTL in seconds\"\n          default: 3600\n        tags:\n          type: array\n          items:\n            type: string\n          description: \"Cache tags for invalidation\"\n\n    permissions:\n      type: array\n      items:\n        type: string\n      description: \"Required Drupal permissions\"\n      examples:\n        - [\"administer ai agents\", \"execute ai agents\"]\n\n    a2a_config:\n      type: object\n      description: \"Agent-to-Agent communication configuration\"\n      properties:\n        enabled:\n          type: boolean\n          default: false\n        protocol:\n          type: string\n          enum: [\"json-rpc\", \"http\", \"sse\"]\n          default: \"json-rpc\"\n        endpoints:\n          type: array\n          items:\n            type: string\n            format: uri\n        authentication:\n          type: object\n          properties:\n            type:\n              type: string\n              enum: [\"bearer\", \"oauth2\", \"mtls\"]\n            secretRef:\n              type: object\n              properties:\n                name:\n                  type: string\n                key:\n                  type: string\n\n    event_bus:\n      type: object\n      description: \"Event bus integration for pub/sub\"\n      properties:\n        enabled:\n          type: boolean\n          default: false\n        topics:\n          type: array\n          items:\n            type: string\n          description: \"Event topics to subscribe/publish\"\n        redis:\n          type: object\n          properties:\n            host:\n              type: string\n              format: uri\n            db:\n              type: integer\n              default: 0\n\n  validation:\n    required:\n      - module\n      - service\n    \n    customRules:\n      - name: \"validate-drupal-service-format\"\n        rule: \"Service ID must follow module.service naming convention\"\n        \n      - name: \"validate-module-exists\"\n        rule: \"Module specified must be installed in Drupal\"\n        \n      - name: \"validate-dependencies\"\n        rule: \"All dependencies must be installed Drupal modules\"\n        \n      - name: \"validate-rag-pipeline\"\n        rule: \"If rag_pipeline specified, vector_db and collection are required\"\n        \n      - name: \"validate-monitoring-endpoint\"\n        rule: \"If monitoring.tracing is true, opentelemetry.endpoint must be provided\"\n\n  examples:\n    - name: \"GitLab ML Recommendation Agent\"\n      description: \"RAG-powered customer success agent\"\n      manifest:\n        drupal:\n          module: \"ai_agent_orchestra\"\n          service: \"ai_agent_orchestra.gitlab_ml_recommendations\"\n          dependencies:\n            - \"ai_agents\"\n            - \"ai_provider_langchain\"\n          rag_pipeline:\n            vector_db: \"qdrant\"\n            collection: \"gitlab_customer_embeddings\"\n            similarity_limit: 5\n            llm_service: \"http://agent-router:4000\"\n          monitoring:\n            metrics: true\n            tracing: true\n            opentelemetry:\n              endpoint: \"http://agent-tracer:4318\"\n\n  migration:\n    fromVersion: \"none\"\n    toVersion: \"v1\"\n    breakingChanges: []\n    migrationGuide: \"https://ossa.dev/docs/drupal/migration\"\n",
    "category": "Spec Examples & Templates"
  },
  {
    "name": "kagent-v1.yml",
    "path": "extensions/kagent-v1.yml",
    "content": "# OSSA kAgent Extension Schema\n# Production-grade specification for Kubernetes-native agent deployment\n# Compatible with kagent.dev/v1alpha1 CRDs\n\napiVersion: ossa/v0.2.8\nkind: ExtensionSchema\nmetadata:\n  name: kagent-extension\n  version: v1alpha1\n  namespace: ossa/extensions\nspec:\n  compatibleWith:\n    - kagent.dev/v1alpha1\n  description: \"Extension schema for Kubernetes-native agent deployment via kAgent\"\n\n  additionalFields:\n    agent:\n      kubernetes:\n        namespace:\n          type: string\n          default: \"default\"\n          description: \"Target Kubernetes namespace\"\n        labels:\n          type: object\n          additionalProperties:\n            type: string\n          description: \"Kubernetes labels for agent CRD\"\n        annotations:\n          type: object\n          additionalProperties:\n            type: string\n          description: \"Kubernetes annotations for agent CRD\"\n        resourceLimits:\n          type: object\n          properties:\n            cpu:\n              type: string\n              default: \"100m\"\n            memory:\n              type: string\n              default: \"256Mi\"\n          description: \"Kubernetes resource limits\"\n\n      toolRefs:\n        type: array\n        items:\n          type: object\n          required: [name, type]\n          properties:\n            name:\n              type: string\n              description: \"Tool name\"\n            type:\n              type: string\n              enum: [mcp, kubernetes, http, custom]\n              description: \"Tool type\"\n            serverRef:\n              type: object\n              properties:\n                name:\n                  type: string\n                namespace:\n                  type: string\n                  default: \"default\"\n              description: \"Reference to MCP server or Kubernetes resource\"\n            config:\n              type: object\n              additionalProperties: true\n              description: \"Tool-specific configuration\"\n\n      a2aConfig:\n        type: object\n        properties:\n          enabled:\n            type: boolean\n            default: false\n          endpoints:\n            type: array\n            items:\n              type: string\n            description: \"A2A protocol endpoints\"\n          protocol:\n            type: string\n            enum: [json-rpc, http, sse]\n            default: \"json-rpc\"\n          authentication:\n            type: object\n            properties:\n              type:\n                type: string\n                enum: [mtls, bearer, oauth2]\n              credentials:\n                type: string\n                format: secret\n          description: \"Agent-to-Agent communication configuration\"\n\n      guardrails:\n        type: object\n        properties:\n          requireApproval:\n            type: boolean\n            default: true\n          costLimits:\n            type: object\n            properties:\n              maxTokensPerDay:\n                type: integer\n              maxTokensPerRequest:\n                type: integer\n              maxCostPerDay:\n                type: number\n              currency:\n                type: string\n                default: \"USD\"\n          allowedActions:\n            type: array\n            items:\n              type: string\n            description: \"Permitted agent actions\"\n          blockedActions:\n            type: array\n            items:\n              type: string\n            description: \"Forbidden agent actions\"\n          auditLog:\n            type: object\n            properties:\n              destination:\n                type: string\n                enum: [compliance-engine, agent-tracer, external]\n              retention:\n                type: string\n                pattern: \"^\\d+[years|months|days]$\"\n              description: \"Audit logging configuration\"\n\n      meshIntegration:\n        type: object\n        properties:\n          enabled:\n            type: boolean\n            default: false\n          istioIntegration:\n            type: boolean\n            default: false\n          ambientMesh:\n            type: boolean\n            default: false\n          description: \"Service mesh integration (Istio/Cilium)\"\n\n  validation:\n    required:\n      - kubernetes.namespace\n      - toolRefs\n    customRules:\n      - name: \"validate-tool-references\"\n        rule: \"All toolRefs must reference valid MCP servers or Kubernetes resources\"\n      - name: \"validate-a2a-endpoints\"\n        rule: \"If a2aConfig.enabled is true, endpoints must be provided\"\n      - name: \"validate-cost-limits\"\n        rule: \"If costLimits specified, maxTokensPerDay must be positive\"\n\n  migration:\n    fromVersion: \"v0.1.9\"\n    toVersion: \"v1alpha1\"\n    breakingChanges: []\n    migrationGuide: \"https://ossa.dev/docs/kagent/migration\"\n\n",
    "category": "Spec Examples & Templates"
  },
  {
    "name": "hello-world-complete.ossa.yaml",
    "path": "getting-started/hello-world-complete.ossa.yaml",
    "content": "# ============================================================================\n# OSSA v0.2.x - Complete \"Hello World\" Agent Example\n# ============================================================================\n# \n# PURPOSE:\n#   This is a minimal, fully-compliant OSSA agent manifest demonstrating\n#   all required fields and best practices. Use this as a starting point\n#   for your first agent.\n#\n# LEARNING OBJECTIVES:\n#   1. Understand OSSA manifest structure (apiVersion, kind, metadata, spec)\n#   2. Learn required vs optional fields\n#   3. See how to define agent capabilities (tools)\n#   4. Understand LLM configuration\n#   5. Learn basic observability setup\n#\n# VALIDATION:\n#   ossa validate examples/getting-started/hello-world-complete.ossa.yaml\n#\n# DEPLOYMENT:\n#   This agent can be deployed to any OSSA-compatible runtime:\n#   - Kubernetes (via agent-buildkit)\n#   - Docker Compose\n#   - Serverless (AWS Lambda, Google Cloud Functions)\n#   - Local development\n#\n# INTEGRATION PATTERNS:\n#   - Standalone agent (no dependencies)\n#   - Can be called via HTTP/gRPC\n#   - Can participate in multi-agent workflows\n#   - Compatible with MCP (Model Context Protocol)\n#\n# ============================================================================\n\napiVersion: ossa/v0.2.8\nkind: Agent\n\nmetadata:\n  # REQUIRED: Agent identifier (DNS-1123 format for Kubernetes compatibility)\n  # Must be lowercase alphanumeric with hyphens, max 253 chars\n  name: hello-world-agent\n  \n  # REQUIRED: Semantic version (semver 2.0.0)\n  # Format: MAJOR.MINOR.PATCH[-PRERELEASE][+BUILD]\n  version: 1.0.0\n  \n  # REQUIRED: Human-readable description\n  # Should explain what the agent does, its purpose, and key capabilities\n  description: |\n    A minimal OSSA-compliant agent that demonstrates basic capabilities.\n    This agent can greet users, echo messages, and report its status.\n    Perfect for learning OSSA fundamentals and testing deployments.\n  \n  # OPTIONAL: Key-value labels for organization and filtering\n  # Common labels: environment, team, domain, classification\n  labels:\n    environment: development\n    team: platform-engineering\n    domain: examples\n    complexity: minimal\n  \n  # OPTIONAL: Arbitrary metadata for tooling (not used for filtering)\n  # Common annotations: documentation URL, support contact, runbooks\n  annotations:\n    documentation: https://docs.ossa.io/examples/hello-world\n    support: support@example.com\n    runbook: https://runbooks.example.com/hello-world-agent\n\nspec:\n  # REQUIRED: Agent role/system prompt describing behavior and capabilities\n  # This is the \"personality\" and instructions for the LLM\n  # Should be clear, specific, and aligned with the agent's purpose\n  role: |\n    You are a helpful assistant agent that can:\n    1. Greet users with personalized messages\n    2. Echo back messages sent to you\n    3. Report your current status and health\n    \n    Always be friendly, concise, and helpful. If you receive a request\n    you cannot handle, politely explain your limitations.\n  \n  # REQUIRED: LLM configuration\n  # Defines which language model powers this agent\n  llm:\n    # REQUIRED: LLM provider (openai, anthropic, google, azure, ollama, custom)\n    provider: openai\n    \n    # REQUIRED: Model identifier (specific to provider)\n    # OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo\n    # Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229\n    # Google: gemini-pro, gemini-pro-vision\n    model: gpt-3.5-turbo\n    \n    # OPTIONAL: Temperature (0.0-2.0) - Controls randomness\n    # 0.0 = deterministic, 1.0 = balanced, 2.0 = very creative\n    # Lower values better for structured tasks, higher for creative tasks\n    temperature: 0.7\n    \n    # OPTIONAL: Maximum tokens per request\n    # Limits response length. Set based on expected output size\n    maxTokens: 1000\n    \n    # OPTIONAL: Top-p (nucleus sampling) - 0.0-1.0\n    # Alternative to temperature. 0.9 = consider top 90% probability mass\n    topP: 0.9\n  \n  # REQUIRED: Available tools/capabilities for the agent\n  # Each tool represents a capability the agent can use\n  # Must have at least one tool\n  tools:\n    # Tool 1: Greeting capability\n    - type: function\n      name: greet_user\n      description: |\n        Generate a personalized greeting message for a user.\n        Takes a user name and optional context, returns a friendly greeting.\n      # Tool-specific configuration\n      config:\n        # Example: This could reference a function, API endpoint, or MCP server\n        handler: greet_handler\n        timeout: 5\n      \n      # OPTIONAL: Authentication for tool access\n      auth:\n        type: none  # Options: bearer, oauth2, mtls, apikey, none\n    \n    # Tool 2: Echo capability\n    - type: function\n      name: echo_message\n      description: |\n        Echo back a message sent to the agent.\n        Useful for testing and demonstrating basic agent communication.\n      config:\n        handler: echo_handler\n        timeout: 2\n    \n    # Tool 3: Status reporting\n    - type: http\n      name: get_status\n      description: |\n        Get the current status and health of the agent.\n        Returns agent metadata, uptime, and capability status.\n      endpoint: http://localhost:8080/status\n      config:\n        method: GET\n        timeout: 3\n      auth:\n        type: none\n  \n  # OPTIONAL: Autonomy configuration\n  # Controls how independently the agent can operate\n  autonomy:\n    # Level of autonomy: supervised, autonomous, fully_autonomous\n    # supervised = requires human approval for actions\n    # autonomous = can act independently within constraints\n    # fully_autonomous = complete independence (use with caution)\n    level: supervised\n    \n    # Whether human approval is required before executing actions\n    approval_required: true\n    \n    # Whitelist of permitted actions (if specified, only these are allowed)\n    allowed_actions:\n      - greet_user\n      - echo_message\n      - get_status\n    \n    # Blacklist of forbidden actions\n    blocked_actions: []\n  \n  # OPTIONAL: Constraints and limits\n  constraints:\n    # Cost constraints (prevent runaway spending)\n    cost:\n      # Maximum tokens per day (prevents excessive API usage)\n      maxTokensPerDay: 100000\n      \n      # Maximum tokens per request\n      maxTokensPerRequest: 2000\n      \n      # Maximum cost per day (in specified currency)\n      maxCostPerDay: 10.00\n      \n      # Currency code (ISO 4217, 3-letter code)\n      currency: USD\n    \n    # Performance constraints\n    performance:\n      # Maximum latency in seconds\n      maxLatencySeconds: 5.0\n      \n      # Maximum concurrent requests\n      maxConcurrentRequests: 10\n      \n      # Request timeout in seconds\n      timeoutSeconds: 30\n    \n    # Resource constraints (for Kubernetes deployments)\n    resources:\n      cpu: \"100m\"      # 0.1 CPU cores (Kubernetes format)\n      memory: \"256Mi\"  # 256 megabytes (Kubernetes format)\n      # gpu: \"1\"       # Optional GPU requirement\n  \n  # OPTIONAL: Observability configuration\n  # Enables monitoring, tracing, and logging\n  observability:\n    # Distributed tracing (OpenTelemetry, Jaeger, Zipkin)\n    tracing:\n      enabled: true\n      exporter: otlp  # Options: otlp, jaeger, zipkin, custom\n      endpoint: http://localhost:4318  # OTLP collector endpoint\n    \n    # Metrics collection (Prometheus, OTLP)\n    metrics:\n      enabled: true\n      exporter: prometheus  # Options: prometheus, otlp, custom\n      endpoint: http://localhost:9090/metrics\n    \n    # Logging configuration\n    logging:\n      level: info  # Options: debug, info, warn, error\n      format: json  # Options: json, text\n\n# ============================================================================\n# USAGE EXAMPLES\n# ============================================================================\n#\n# 1. VALIDATE THIS MANIFEST:\n#    $ ossa validate examples/getting-started/hello-world-complete.ossa.yaml\n#\n# 2. DEPLOY TO KUBERNETES (via agent-buildkit):\n#    $ buildkit agents deploy examples/getting-started/hello-world-complete.ossa.yaml\n#\n# 3. TEST LOCALLY:\n#    $ ossa validate examples/getting-started/hello-world-complete.ossa.yaml --verbose\n#\n# 4. INTEGRATE WITH MCP:\n#    This agent can be exposed via MCP (Model Context Protocol) for use\n#    with Claude Desktop, Cursor, or other MCP-compatible tools.\n#\n# 5. USE IN MULTI-AGENT WORKFLOW:\n#    This agent can be orchestrated alongside other agents using\n#    agent-buildkit's workflow engine.\n#\n# ============================================================================\n# NEXT STEPS\n# ============================================================================\n#\n# After understanding this example:\n# 1. Review examples/intermediate/ for more complex patterns\n# 2. Check examples/advanced/ for enterprise patterns\n# 3. Read the full OSSA specification at spec/v0.2.x/ossa-0.2.x.schema.json\n# 4. Explore integration examples in examples/bridges/\n# 5. See migration guides in examples/migration-guides/\n#\n# ============================================================================\n# RELATED DOCUMENTATION\n# ============================================================================\n#\n# - OSSA Specification: spec/v0.2.x/ossa-0.2.x.schema.json\n# - Schema Reference: docs/schemas/\n# - API Reference: openapi/\n# - GitHub Repository: https://github.com/blueflyio/openstandardagents\n#\n# ============================================================================\n\n",
    "category": "Getting Started"
  },
  {
    "name": "agent-to-agent-orchestration.ossa.yaml",
    "path": "integration-patterns/agent-to-agent-orchestration.ossa.yaml",
    "content": "# ============================================================================\n# Integration Pattern: Agent-to-Agent Orchestration\n# ============================================================================\n#\n# PURPOSE:\n#   Demonstrates how multiple OSSA agents can work together in orchestrated\n#   workflows. This pattern enables complex multi-agent systems where agents\n#   coordinate to solve problems beyond any single agent's capabilities.\n#\n# USE CASES:\n#   - Multi-step workflows requiring specialized agents\n#   - Research → Analysis → Writing pipelines\n#   - Code review → Testing → Deployment workflows\n#   - Customer support escalation chains\n#   - Data processing pipelines with multiple stages\n#\n# ARCHITECTURE:\n#   Orchestrator Agent (coordinates workflow)\n#     ↓\n#   Worker Agent 1 (specialized task)\n#     ↓\n#   Worker Agent 2 (specialized task)\n#     ↓\n#   Aggregator Agent (combines results)\n#\n# PROTOCOLS:\n#   - HTTP/gRPC for synchronous communication\n#   - Message queues for asynchronous workflows\n#   - MCP (Model Context Protocol) for tool sharing\n#   - Event-driven patterns for reactive workflows\n#\n# ============================================================================\n\n# ----------------------------------------------------------------------------\n# ORCHESTRATOR AGENT\n# ----------------------------------------------------------------------------\n# Coordinates the workflow, delegates tasks to worker agents, and aggregates results\n\napiVersion: ossa/v0.2.8\nkind: Agent\n\nmetadata:\n  name: workflow-orchestrator\n  version: 1.0.0\n  description: |\n    Orchestrates multi-agent workflows by coordinating specialized worker agents.\n    Manages workflow state, handles failures, and aggregates results.\n\nspec:\n  role: |\n    You are a workflow orchestrator. Your job is to:\n    1. Break down complex tasks into subtasks\n    2. Delegate subtasks to appropriate worker agents\n    3. Monitor progress and handle failures\n    4. Aggregate results from worker agents\n    5. Provide final output to the user\n    \n    Always coordinate clearly, handle errors gracefully, and provide status updates.\n  \n  llm:\n    provider: openai\n    model: gpt-4\n    temperature: 0.3  # Lower temperature for more deterministic orchestration\n  \n  tools:\n    # Tool: Invoke Research Agent\n    - type: http\n      name: invoke_research_agent\n      description: |\n        Delegate research tasks to the specialized research agent.\n        Returns comprehensive research results on the requested topic.\n      endpoint: http://research-agent:8080/api/v1/research\n      config:\n        method: POST\n        timeout: 60  # Research can take time\n      auth:\n        type: bearer\n        credentials: ORCHESTRATOR_TOKEN\n    \n    # Tool: Invoke Analysis Agent\n    - type: http\n      name: invoke_analysis_agent\n      description: |\n        Delegate analysis tasks to the specialized analysis agent.\n        Analyzes data and provides insights.\n      endpoint: http://analysis-agent:8080/api/v1/analyze\n      config:\n        method: POST\n        timeout: 30\n      auth:\n        type: bearer\n        credentials: ORCHESTRATOR_TOKEN\n    \n    # Tool: Invoke Writing Agent\n    - type: http\n      name: invoke_writing_agent\n      description: |\n        Delegate writing tasks to the specialized writing agent.\n        Creates well-structured content based on research and analysis.\n      endpoint: http://writing-agent:8080/api/v1/write\n      config:\n        method: POST\n        timeout: 45\n      auth:\n        type: bearer\n        credentials: ORCHESTRATOR_TOKEN\n    \n    # Tool: Check Agent Status\n    - type: http\n      name: check_agent_status\n      description: |\n        Check the status and health of worker agents.\n        Returns availability, current load, and capability status.\n      endpoint: http://agent-registry:8080/api/v1/agents/status\n      config:\n        method: GET\n        timeout: 5\n  \n  autonomy:\n    level: autonomous\n    approval_required: false\n    allowed_actions:\n      - invoke_research_agent\n      - invoke_analysis_agent\n      - invoke_writing_agent\n      - check_agent_status\n  \n  constraints:\n    cost:\n      maxTokensPerDay: 500000\n      maxCostPerDay: 50.00\n      currency: USD\n    performance:\n      maxLatencySeconds: 120  # Multi-agent workflows take longer\n      maxConcurrentRequests: 5\n      timeoutSeconds: 180\n  \n  observability:\n    tracing:\n      enabled: true\n      exporter: otlp\n      endpoint: http://jaeger:4318\n    metrics:\n      enabled: true\n      exporter: prometheus\n      endpoint: http://prometheus:9090/metrics\n    logging:\n      level: info\n      format: json\n\n# ----------------------------------------------------------------------------\n# RESEARCH WORKER AGENT\n# ----------------------------------------------------------------------------\n# Specialized agent for research tasks\n\n---\napiVersion: ossa/v0.2.8\nkind: Agent\n\nmetadata:\n  name: research-worker\n  version: 1.0.0\n  description: |\n    Specialized research agent that gathers comprehensive information on topics.\n    Used by orchestrator for research-intensive subtasks.\n\nspec:\n  role: |\n    You are a research specialist. Your job is to:\n    1. Gather accurate, up-to-date information on requested topics\n    2. Use multiple sources and verify information\n    3. Provide well-sourced, comprehensive research results\n    4. Cite sources and provide evidence\n    \n    Always prioritize accuracy over speed. Verify facts from multiple sources.\n  \n  llm:\n    provider: openai\n    model: gpt-4\n    temperature: 0.2  # Lower temperature for factual accuracy\n  \n  tools:\n    - type: http\n      name: web_search\n      description: Search the web for current information\n      endpoint: https://api.search.com/search\n      config:\n        method: POST\n        timeout: 15\n      auth:\n        type: apikey\n        credentials: SEARCH_API_KEY\n    \n    - type: http\n      name: academic_search\n      description: Search academic papers and publications\n      endpoint: https://api.academic.com/search\n      config:\n        method: POST\n        timeout: 20\n      auth:\n        type: apikey\n        credentials: ACADEMIC_API_KEY\n  \n  autonomy:\n    level: autonomous\n    approval_required: false\n  \n  constraints:\n    cost:\n      maxTokensPerDay: 200000\n      maxCostPerDay: 20.00\n      currency: USD\n    performance:\n      maxLatencySeconds: 60\n      maxConcurrentRequests: 10\n\n# ----------------------------------------------------------------------------\n# ANALYSIS WORKER AGENT\n# ----------------------------------------------------------------------------\n# Specialized agent for data analysis\n\n---\napiVersion: ossa/v0.2.8\nkind: Agent\n\nmetadata:\n  name: analysis-worker\n  version: 1.0.0\n  description: |\n    Specialized analysis agent that processes data and provides insights.\n    Used by orchestrator for analysis-intensive subtasks.\n\nspec:\n  role: |\n    You are a data analysis specialist. Your job is to:\n    1. Analyze data and research results\n    2. Identify patterns, trends, and insights\n    3. Provide clear, actionable recommendations\n    4. Support conclusions with data\n    \n    Always be objective, data-driven, and clear in your analysis.\n  \n  llm:\n    provider: openai\n    model: gpt-4\n    temperature: 0.3\n  \n  tools:\n    - type: function\n      name: statistical_analysis\n      description: Perform statistical analysis on datasets\n      config:\n        handler: stats_handler\n        language: python\n        timeout: 30\n    \n    - type: function\n      name: data_visualization\n      description: Create visualizations from data\n      config:\n        handler: viz_handler\n        language: python\n        timeout: 20\n  \n  autonomy:\n    level: autonomous\n    approval_required: false\n  \n  constraints:\n    cost:\n      maxTokensPerDay: 200000\n      maxCostPerDay: 20.00\n      currency: USD\n    performance:\n      maxLatencySeconds: 45\n      maxConcurrentRequests: 8\n\n# ----------------------------------------------------------------------------\n# WRITING WORKER AGENT\n# ----------------------------------------------------------------------------\n# Specialized agent for content creation\n\n---\napiVersion: ossa/v0.2.8\nkind: Agent\n\nmetadata:\n  name: writing-worker\n  version: 1.0.0\n  description: |\n    Specialized writing agent that creates well-structured content.\n    Used by orchestrator for writing-intensive subtasks.\n\nspec:\n  role: |\n    You are a writing specialist. Your job is to:\n    1. Create well-structured, clear content\n    2. Synthesize information from research and analysis\n    3. Write in appropriate style and tone\n    4. Ensure accuracy and clarity\n    \n    Always write clearly, engagingly, and accurately. Structure content logically.\n  \n  llm:\n    provider: openai\n    model: gpt-4\n    temperature: 0.7  # Higher temperature for more creative writing\n  \n  tools:\n    - type: function\n      name: format_document\n      description: Format documents in various styles (Markdown, HTML, LaTeX)\n      config:\n        handler: format_handler\n        timeout: 10\n  \n  autonomy:\n    level: autonomous\n    approval_required: false\n  \n  constraints:\n    cost:\n      maxTokensPerDay: 200000\n      maxCostPerDay: 20.00\n      currency: USD\n    performance:\n      maxLatencySeconds: 45\n      maxConcurrentRequests: 8\n\n# ============================================================================\n# WORKFLOW EXAMPLE\n# ============================================================================\n#\n# User Request: \"Research quantum computing trends and write a report\"\n#\n# Orchestrator Workflow:\n#   1. Receive request\n#   2. Invoke research-worker → Get research results\n#   3. Invoke analysis-worker → Analyze research, identify trends\n#   4. Invoke writing-worker → Create report from research + analysis\n#   5. Return final report to user\n#\n# Error Handling:\n#   - If research-worker fails → Retry with backup research agent\n#   - If analysis-worker fails → Use simpler analysis approach\n#   - If writing-worker fails → Return research + analysis directly\n#\n# ============================================================================\n# DEPLOYMENT\n# ============================================================================\n#\n# 1. Deploy all agents:\n#    $ buildkit agents deploy examples/integration-patterns/agent-to-agent-orchestration.ossa.yaml\n#\n# 2. Configure service discovery:\n#    - Ensure agents can discover each other (DNS, service mesh, registry)\n#    - Set up authentication tokens\n#    - Configure network policies\n#\n# 3. Set up observability:\n#    - Distributed tracing across agents\n#    - Metrics aggregation\n#    - Log correlation\n#\n# ============================================================================\n# RELATED PATTERNS\n# ============================================================================\n#\n# - examples/integration-patterns/mcp-bridge-integration.ossa.yaml\n# - examples/integration-patterns/event-driven-agents.ossa.yaml\n# - examples/integration-patterns/openapi-service-exposure.ossa.yaml\n# - examples/agent-manifests/orchestrators/orchestrator-agent.yaml\n#\n# ============================================================================\n\n",
    "category": "Advanced Patterns"
  }
]