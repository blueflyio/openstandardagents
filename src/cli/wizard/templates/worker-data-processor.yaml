apiVersion: ossa.ai/v1alpha1
kind: Agent
metadata:
  name: worker-data-processor
  version: 1.0.0
  description: Data processing worker for ETL pipelines and batch operations
  labels:
    ossa.ai/display-name: Data Processor
    ossa.ai/agent-type: worker
    ossa.ai/domain: data-processing
    ossa.ai/capabilities: etl,validation,transformation,batch-processing
spec:
  role: |
    You are a data processing specialist that transforms and validates data in batch and streaming modes.

    Your responsibilities:
    - Extract data from various sources (files, databases, APIs)
    - Transform data according to business rules
    - Load data into target systems
    - Validate data quality and integrity
    - Handle large datasets efficiently
    - Generate data quality reports

    When processing data:
    - Validate input data against schemas
    - Apply transformations consistently
    - Handle nulls and missing values appropriately
    - Maintain data lineage
    - Log transformation steps for audit
    - Report data quality metrics

  llm:
    provider: anthropic
    model: claude-haiku-4-20250514
    temperature: 0.2
    max_tokens: 4096

  tools:
    - type: mcp
      name: filesystem
      description: Read and write files
      config:
        server: npx -y @modelcontextprotocol/server-filesystem
        args:
          - ${DATA_DIR:-/data}

    - type: function
      name: validate_schema
      description: Validate data against JSON schema
      input_schema:
        type: object
        properties:
          data:
            type: object
          schema:
            type: object
        required: [data, schema]

  autonomy:
    level: full

  observability:
    tracing:
      enabled: true
      exporter: otlp
      endpoint: ${OTEL_ENDPOINT:-http://localhost:4317}
    metrics:
      enabled: true
      custom_labels:
        service: data-processor
        data_pipeline: ${PIPELINE_NAME:-default}
    logging:
      level: info

  deployment:
    target: kubernetes
    kubernetes:
      replicas: 5
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
        limits:
          cpu: 2000m
          memory: 4Gi
      autoscaling:
        enabled: true
        min_replicas: 2
        max_replicas: 20
        target_cpu_utilization: 70
