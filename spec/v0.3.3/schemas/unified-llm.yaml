# OSSA v0.3.3 - Unified LLM Configuration Schema
# Compatible with: OSSA, GitLab Duo, Google A2A, MCP
# Zero hardcoded models, runtime-configurable, multi-provider

$schema: http://json-schema.org/draft-07/schema#
$id: https://openstandardagents.org/schemas/v0.3.3/unified-llm.yaml

title: Unified LLM Configuration
description: |
  Universal LLM configuration that works across OSSA, GitLab Duo, Google A2A, and MCP.
  Eliminates hardcoded model names and enables runtime model selection.

type: object
required:
  - provider
  - model
properties:
  provider:
    type: string
    description: LLM provider name (runtime-configurable via env var)
    default: ${LLM_PROVIDER:-anthropic}
    examples:
      - anthropic
      - openai
      - google
      - groq
      - ollama
      - bedrock

  model:
    type: string
    description: Model identifier (runtime-configurable via env var)
    default: ${LLM_MODEL:-claude-sonnet}
    examples:
      - claude-sonnet-4
      - gpt-4o
      - gemini-2.0-flash
      - llama-3.3-70b
      - mixtral-8x7b

  profile:
    type: string
    description: Execution profile for task-specific optimization (A2A compatible)
    default: ${LLM_PROFILE:-balanced}
    enum:
      - fast
      - balanced
      - deep
      - safe

  temperature:
    type: number
    description: Sampling temperature
    default: ${LLM_TEMPERATURE:-0.1}
    minimum: 0.0
    maximum: 2.0

  maxTokens:
    type: integer
    description: Maximum tokens in response
    default: ${LLM_MAX_TOKENS:-16000}

  topP:
    type: number
    description: Nucleus sampling threshold
    default: ${LLM_TOP_P:-0.9}

  fallback_models:
    type: array
    items:
      type: object
      required: [provider, model]
      properties:
        provider:
          type: string
        model:
          type: string
        condition:
          type: string
          enum: [on_error, on_timeout, on_rate_limit, always]

  retry_config:
    type: object
    properties:
      max_attempts:
        type: integer
        default: ${LLM_RETRY_ATTEMPTS:-3}
      backoff_strategy:
        type: string
        enum: [exponential, linear, constant]
        default: ${LLM_BACKOFF_STRATEGY:-exponential}
