apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: monitoring
    app.kubernetes.io/part-of: ossa-platform
    ossa.bluefly.io/version: "0.1.8"
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus
  template:
    metadata:
      labels:
        app.kubernetes.io/name: prometheus
        app.kubernetes.io/component: monitoring
        app.kubernetes.io/part-of: ossa-platform
    spec:
      serviceAccountName: prometheus
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
      containers:
      - name: prometheus
        image: prom/prometheus:v2.45.0
        args:
          - '--config.file=/etc/prometheus/prometheus.yml'
          - '--storage.tsdb.path=/prometheus'
          - '--storage.tsdb.retention.time=30d'
          - '--storage.tsdb.retention.size=50GB'
          - '--web.console.libraries=/etc/prometheus/console_libraries'
          - '--web.console.templates=/etc/prometheus/consoles'
          - '--web.enable-lifecycle'
          - '--web.enable-admin-api'
          - '--query.max-concurrency=50'
          - '--query.max-samples=50000000'
        ports:
        - containerPort: 9090
          name: web
        resources:
          requests:
            memory: "4Gi"
            cpu: "1"
          limits:
            memory: "8Gi"
            cpu: "2"
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
        - name: storage
          mountPath: /prometheus
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
      volumes:
      - name: config
        configMap:
          name: prometheus-config
      - name: storage
        persistentVolumeClaim:
          claimName: prometheus-storage

---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: monitoring
    app.kubernetes.io/part-of: ossa-platform
spec:
  selector:
    app.kubernetes.io/name: prometheus
  ports:
  - port: 9090
    targetPort: 9090
    name: web

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: prometheus-storage
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: storage
    app.kubernetes.io/part-of: ossa-platform
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ossa-fast-ssd
  resources:
    requests:
      storage: 100Gi

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/component: monitoring
    app.kubernetes.io/part-of: ossa-platform
    ossa.bluefly.io/version: "0.1.8"
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/component: monitoring
        app.kubernetes.io/part-of: ossa-platform
    spec:
      serviceAccountName: grafana
      securityContext:
        runAsNonRoot: true
        runAsUser: 472
        fsGroup: 472
      containers:
      - name: grafana
        image: grafana/grafana:10.0.0
        ports:
        - containerPort: 3000
          name: grafana
        env:
        - name: GF_SECURITY_ADMIN_USER
          value: admin
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: ossa-platform-secrets
              key: GRAFANA_ADMIN_PASSWORD
        - name: GF_USERS_ALLOW_SIGN_UP
          value: "false"
        - name: GF_INSTALL_PLUGINS
          value: "grafana-clock-panel,grafana-simple-json-datasource,grafana-piechart-panel,grafana-worldmap-panel"
        - name: GF_FEATURE_TOGGLES_ENABLE
          value: "publicDashboards"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1"
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
        - name: grafana-config
          mountPath: /etc/grafana/provisioning
        livenessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 60
          timeoutSeconds: 30
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 30
          timeoutSeconds: 30
      volumes:
      - name: grafana-storage
        persistentVolumeClaim:
          claimName: grafana-storage
      - name: grafana-config
        configMap:
          name: grafana-config

---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/component: monitoring
    app.kubernetes.io/part-of: ossa-platform
spec:
  selector:
    app.kubernetes.io/name: grafana
  ports:
  - port: 3000
    targetPort: 3000
    name: grafana

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana-storage
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/component: storage
    app.kubernetes.io/part-of: ossa-platform
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ossa-fast-ssd
  resources:
    requests:
      storage: 10Gi

---
# Jaeger for distributed tracing
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/component: tracing
    app.kubernetes.io/part-of: ossa-platform
    ossa.bluefly.io/version: "0.1.8"
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: jaeger
  template:
    metadata:
      labels:
        app.kubernetes.io/name: jaeger
        app.kubernetes.io/component: tracing
        app.kubernetes.io/part-of: ossa-platform
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.46
        ports:
        - containerPort: 16686
          name: ui
        - containerPort: 14268
          name: collector
        - containerPort: 6831
          name: agent-compact
        - containerPort: 6832
          name: agent-binary
        - containerPort: 14250
          name: grpc
        env:
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        - name: QUERY_BASE_PATH
          value: "/jaeger"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1"
        volumeMounts:
        - name: jaeger-storage
          mountPath: /badger
      volumes:
      - name: jaeger-storage
        persistentVolumeClaim:
          claimName: jaeger-storage

---
apiVersion: v1
kind: Service
metadata:
  name: jaeger
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/component: tracing
    app.kubernetes.io/part-of: ossa-platform
spec:
  selector:
    app.kubernetes.io/name: jaeger
  ports:
  - port: 16686
    targetPort: 16686
    name: ui
  - port: 14268
    targetPort: 14268
    name: collector
  - port: 6831
    targetPort: 6831
    name: agent-compact
    protocol: UDP
  - port: 6832
    targetPort: 6832
    name: agent-binary
    protocol: UDP
  - port: 14250
    targetPort: 14250
    name: grpc

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jaeger-storage
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/component: storage
    app.kubernetes.io/part-of: ossa-platform
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ossa-fast-ssd
  resources:
    requests:
      storage: 50Gi

---
# AlertManager for alert handling
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/component: alerting
    app.kubernetes.io/part-of: ossa-platform
    ossa.bluefly.io/version: "0.1.8"
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: alertmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: alertmanager
        app.kubernetes.io/component: alerting
        app.kubernetes.io/part-of: ossa-platform
    spec:
      serviceAccountName: alertmanager
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.25.0
        args:
          - '--config.file=/etc/alertmanager/alertmanager.yml'
          - '--storage.path=/alertmanager'
          - '--web.external-url=https://ossa.llm.bluefly.io/alertmanager'
          - '--cluster.listen-address=0.0.0.0:9094'
          - '--cluster.peer=alertmanager-0.alertmanager:9094'
          - '--cluster.peer=alertmanager-1.alertmanager:9094'
          - '--cluster.peer=alertmanager-2.alertmanager:9094'
        ports:
        - containerPort: 9093
          name: web
        - containerPort: 9094
          name: cluster
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        volumeMounts:
        - name: config
          mountPath: /etc/alertmanager
        - name: storage
          mountPath: /alertmanager
      volumes:
      - name: config
        configMap:
          name: alertmanager-config
      - name: storage
        persistentVolumeClaim:
          claimName: alertmanager-storage

---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/component: alerting
    app.kubernetes.io/part-of: ossa-platform
spec:
  selector:
    app.kubernetes.io/name: alertmanager
  ports:
  - port: 9093
    targetPort: 9093
    name: web
  - port: 9094
    targetPort: 9094
    name: cluster

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: alertmanager-storage
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/component: storage
    app.kubernetes.io/part-of: ossa-platform
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ossa-fast-ssd
  resources:
    requests:
      storage: 10Gi

---
# ServiceMonitor for Prometheus Operator
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: ossa-platform-monitor
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: ossa-platform
    app.kubernetes.io/component: servicemonitor
    app.kubernetes.io/part-of: ossa-platform
    ossa.bluefly.io/version: "0.1.8"
spec:
  selector:
    matchLabels:
      prometheus.io/scrape: "true"
  namespaceSelector:
    matchNames:
    - ossa-platform
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s

---
# PrometheusRule for OSSA-specific alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ossa-platform-rules
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: ossa-platform
    app.kubernetes.io/component: prometheus-rules
    app.kubernetes.io/part-of: ossa-platform
    ossa.bluefly.io/version: "0.1.8"
spec:
  groups:
  - name: ossa-platform.rules
    rules:
    - alert: OSSAGatewayDown
      expr: up{job="ossa-gateway"} == 0
      for: 1m
      labels:
        severity: critical
        service: ossa-gateway
      annotations:
        summary: "OSSA Gateway is down"
        description: "OSSA Gateway {{ $labels.instance }} has been down for more than 1 minute."
    
    - alert: OSSAHighResponseTime
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=~"ossa-.*"}[5m])) > 2
      for: 5m
      labels:
        severity: warning
        service: "{{ $labels.job }}"
      annotations:
        summary: "High response time detected"
        description: "95th percentile response time for {{ $labels.job }} is above 2 seconds."
    
    - alert: OSSAAgentQueueDepth
      expr: ossa_agent_queue_depth > 50
      for: 2m
      labels:
        severity: warning
        service: "{{ $labels.job }}"
      annotations:
        summary: "Agent queue depth is high"
        description: "Agent queue depth for {{ $labels.job }} is {{ $value }}."
    
    - alert: OSSAMemoryUsageHigh
      expr: (container_memory_working_set_bytes{pod=~"ossa-.*"} / container_spec_memory_limit_bytes) > 0.9
      for: 5m
      labels:
        severity: warning
        pod: "{{ $labels.pod }}"
      annotations:
        summary: "High memory usage"
        description: "Memory usage for pod {{ $labels.pod }} is above 90%."

---
# Service Accounts for monitoring components
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: serviceaccount
    app.kubernetes.io/part-of: ossa-platform

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: grafana
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/component: serviceaccount
    app.kubernetes.io/part-of: ossa-platform

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: alertmanager
  namespace: ossa-monitoring
  labels:
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/component: serviceaccount
    app.kubernetes.io/part-of: ossa-platform

---
# ClusterRole for Prometheus
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: rbac
    app.kubernetes.io/part-of: ossa-platform
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]

---
# ClusterRoleBinding for Prometheus
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: rbac
    app.kubernetes.io/part-of: ossa-platform
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: ossa-monitoring